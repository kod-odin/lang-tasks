<!doctype html><html lang="en"><head><title data-rh="true">How to fine-tune a NLLB-200 model for translating a new language | by David Dale | Medium</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" name="title" content="How to fine-tune a NLLB-200 model for translating a new language | by David Dale | Medium"/><meta data-rh="true" property="og:title" content="How to fine-tune a NLLB-200 model for translating a new language"/><meta data-rh="true" property="al:android:url" content="medium://p/a37fc706b865"/><meta data-rh="true" property="al:ios:url" content="medium://p/a37fc706b865"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="“NLLB” (which stands for “no language left behind”) is a family of machine translation models published by Meta AI in 2022. These models can translate a sentence between any of the 202 language…"/><meta data-rh="true" property="og:description" content="“NLLB” (which stands for “no language left behind”) is a family of machine translation models published by Meta AI in 2022. These models…"/><meta data-rh="true" property="og:url" content="https://cointegrated.medium.com/a37fc706b865"/><meta data-rh="true" property="al:web:url" content="https://cointegrated.medium.com/a37fc706b865"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fit:704/1*qNRkzVmsA4GkrsTQwWKJfw.png"/><meta data-rh="true" property="article:author" content="https://cointegrated.medium.com"/><meta data-rh="true" name="author" content="David Dale"/><meta data-rh="true" name="robots" content="noindex,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="origin"/><meta data-rh="true" property="twitter:title" content="How to fine-tune a NLLB-200 model for translating a new language"/><meta data-rh="true" name="twitter:site" content="@Medium"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/a37fc706b865"/><meta data-rh="true" property="twitter:description" content="“NLLB” (which stands for “no language left behind”) is a family of machine translation models published by Meta AI in 2022. These models…"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fit:704/1*qNRkzVmsA4GkrsTQwWKJfw.png"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="24 min read"/><meta data-rh="true" name="twitter:tile:template:testing" content="2"/><meta data-rh="true" name="twitter:tile:image" content="https://miro.medium.com/v2/resize:fit:704/1*qNRkzVmsA4GkrsTQwWKJfw.png"/><meta data-rh="true" name="twitter:tile:info1:icon" content="Person"/><meta data-rh="true" name="twitter:tile:info1:text" content="David Dale"/><meta data-rh="true" name="twitter:tile:info2:icon" content="Calendar"/><meta data-rh="true" name="twitter:cta" content="Read on Medium"/><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://cointegrated.medium.com"/><link data-rh="true" rel="canonical" href="https://cointegrated.medium.com/a37fc706b865"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/a37fc706b865"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:1200\u002F1*qNRkzVmsA4GkrsTQwWKJfw.png"],"url":"https:\u002F\u002Fcointegrated.medium.com\u002Fa37fc706b865","dateCreated":0,"datePublished":0,"dateModified":"2023-10-14T21:42:49.373Z","headline":"How to fine-tune a NLLB-200 model for translating a new language","name":"How to fine-tune a NLLB-200 model for translating a new language","description":"“NLLB” (which stands for “no language left behind”) is a family of machine translation models published by Meta AI in 2022. These models can translate a sentence between any of the 202 language…","identifier":"a37fc706b865","author":{"@type":"Person","name":"David Dale","url":"https:\u002F\u002Fcointegrated.medium.com"},"creator":["David Dale"],"publisher":{"@type":"Organization","name":"Medium","url":"https:\u002F\u002Fcointegrated.medium.com\u002F","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:616\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https:\u002F\u002Fcointegrated.medium.com\u002Fa37fc706b865"}</script><style type="text/css" data-fela-rehydration="696" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="696" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{fill:rgba(0, 0, 0, 1)}.av{height:22px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.dv{margin-left:8px}.dw{color:#6B6B6B}.dx{font-size:13px}.dy{height:100%}.dz{height:25px}.ea{fill:rgba(41, 41, 41, 1)}.ed{margin-right:32px}.ee{position:relative}.ef{fill:#6B6B6B}.ei{background:transparent}.ej svg{margin-left:4px}.ek svg{fill:#6B6B6B}.em{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.eo{position:absolute}.eq{box-sizing:border-box}.ew{margin:0 24px}.fa{background:rgba(255, 255, 255, 1)}.fb{border:1px solid #F2F2F2}.fc{box-shadow:0 1px 4px #F2F2F2}.fd{max-height:100vh}.fe{overflow-y:auto}.ff{left:0}.fg{top:calc(100vh + 100px)}.fh{bottom:calc(100vh + 100px)}.fi{width:10px}.fj{pointer-events:none}.fk{word-break:break-word}.fl{word-wrap:break-word}.fm:after{display:block}.fn:after{content:""}.fo:after{clear:both}.fp{line-height:1.23}.fq{letter-spacing:0}.fr{font-style:normal}.fs{font-weight:700}.gs{@media all and (max-width: 551.98px):8px}.gt{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.gu{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.gv{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.gw{@media all and (min-width: 1080px):16px}.hc{align-items:baseline}.hd{width:48px}.he{height:48px}.hf{border:2px solid rgba(255, 255, 255, 1)}.hg{z-index:0}.hh{box-shadow:none}.hi{border:1px solid rgba(0, 0, 0, 0.05)}.hj{margin-bottom:2px}.hk{flex-wrap:nowrap}.hl{font-size:16px}.hm{line-height:24px}.ho{margin:0 8px}.hp{display:inline}.hq{color:#1A8917}.hr{fill:#1A8917}.hs:disabled{opacity:0.3}.hv{flex:0 0 auto}.hy{flex-wrap:wrap}.hz{padding-left:8px}.ia{padding-right:8px}.jb> *{flex-shrink:0}.jc{overflow-x:scroll}.jd::-webkit-scrollbar{display:none}.je{scrollbar-width:none}.jf{-ms-overflow-style:none}.jg{width:74px}.jh{flex-direction:row}.ji{margin-right:4px}.jl{-webkit-user-select:none}.jm{border:0}.jn{cursor:progress}.jo{fill:rgba(117, 117, 117, 1)}.jr{opacity:0.25}.js{outline:0}.jt{user-select:none}.ju> svg{pointer-events:none}.kd{cursor:not-allowed}.ke{opacity:1}.kf{padding:4px 0}.kg{width:16px}.kh{padding:8px 2px}.kk{opacity:0.3}.kl svg path{fill:#6B6B6B}.km path{fill:#242424}.kn svg{color:#6B6B6B}.lf{line-height:1.58}.lg{letter-spacing:-0.004em}.lh{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.mc{margin-bottom:-0.46em}.md{text-decoration:underline}.mh{list-style-type:decimal}.mi{margin-left:30px}.mj{padding-left:0px}.mp{line-height:1.18}.mq{letter-spacing:-0.022em}.mr{font-weight:600}.nj{margin-bottom:-0.31em}.np{list-style-type:disc}.nq{font-style:italic}.nr{padding:2px 4px}.ns{font-size:75%}.nt> strong{font-family:inherit}.nu{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.oa{overflow-x:auto}.ob{padding:32px}.oc{border:1px solid #E5E5E5}.od{line-height:1.4}.oe{margin-top:-0.2em}.of{margin-bottom:-0.2em}.og{white-space:pre}.oh{min-width:fit-content}.oi{margin-left:auto}.oj{margin-right:auto}.ok{max-width:704px}.ol{clear:both}.on{cursor:zoom-in}.oo{z-index:auto}.oq{max-width:100%}.or{height:auto}.os{margin-top:10px}.ot{text-align:center}.ou{max-width:728px}.ox{line-height:1.12}.pq{margin-bottom:-0.28em}.pr{max-width:866px}.ps{max-width:1351px}.pt{max-width:527px}.pu{max-width:1146px}.pv{margin-bottom:26px}.pw{margin-top:6px}.px{margin-top:8px}.py{margin-right:8px}.pz{padding:8px 16px}.qa{border-radius:100px}.qb{transition:background 300ms ease}.qd{white-space:nowrap}.qe{border-top:none}.qk{height:52px}.ql{max-height:52px}.qm{box-sizing:content-box}.qn{position:static}.qo{z-index:1}.qq{max-width:155px}.qw{margin-right:20px}.rc{align-items:flex-end}.rd{width:76px}.re{height:76px}.rf{border:2px solid #F9F9F9}.rg{height:72px}.rh{width:72px}.ri{color:#F2F2F2}.rj{fill:#F2F2F2}.rk{background:#F2F2F2}.rl{border-color:#F2F2F2}.rr:disabled{cursor:inherit !important}.rs:disabled:hover{background:#1A8917}.rt:disabled:hover{border-color:#1A8917}.ru{border-radius:99em}.rv{width:auto}.rw{border-width:1px}.rx{border-style:solid}.ry{text-decoration:none}.rz{stroke:#F2F2F2}.sa{height:36px}.sb{width:36px}.sc{font-weight:500}.sd{font-size:24px}.se{line-height:30px}.sf{letter-spacing:-0.016em}.sg{margin-top:16px}.sh{height:0px}.si{border-bottom:solid 1px #E5E5E5}.so{margin-top:72px}.sp{padding:24px 0}.sq{margin-bottom:0px}.sr{margin-right:16px}.ss{display:inline-flex}.sy{margin-bottom:32px}.sz{margin-top:40px}.ta{align-items:stretch}.uk{flex-grow:0}.uq{display:grid}.ur{grid-template-columns:repeat(12, 1fr)}.us{grid-template-rows:auto 1fr}.vd{grid-area:image}.ve{grid-area:content}.vk{border-radius:2px}.vl{aspect-ratio:2}.vm{object-fit:cover}.vn{object-position:50% 50%}.vt{height:20px}.vu{width:20px}.vv{padding-right:4px}.vw{overflow:hidden}.vx{max-height:20px}.vy{text-overflow:ellipsis}.vz{display:-webkit-box}.wa{-webkit-line-clamp:1}.wb{-webkit-box-orient:vertical}.wc{word-break:break-all}.wy{padding-top:8px}.wz{max-height:40px}.xa{-webkit-line-clamp:2}.xq{margin-left:20px}.xr{margin-left:4px}.xs{margin-top:0px}.xu{justify-content:flex-end}.xv{flex:0 0 0}.xy{padding-left:24px}.xz{margin-top:32px}.yg{fill:#242424}.yh{background:0}.yi{border-color:#242424}.ym:disabled:hover{color:#242424}.yn:disabled:hover{fill:#242424}.yo:disabled:hover{border-color:#242424}.yz{padding-bottom:40px}.za{padding-top:88px}.zb{margin-bottom:40px}.zc{margin-top:4px}.zd{border-right:3px solid #F9F9F9}.ze{z-index:3}.zf{z-index:2}.zg{margin-left:-24px}.zh{margin-left:-36px}.zi{border-radius:0 3px 3px 0}.zj{width:93px}.zs{margin-left:2px}.zt{margin-top:2px}.zu{cursor:initial}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.eg:hover{color:#242424}.eh:hover{fill:#242424}.el:hover svg{fill:#242424}.ep:hover{background-color:rgba(0, 0, 0, 0.1)}.hn:hover{text-decoration:underline}.ht:hover:not(:disabled){color:#156D12}.hu:hover:not(:disabled){fill:#156D12}.jq:hover{fill:rgba(117, 117, 117, 1)}.ki:hover:not(:disabled) svg path{fill:#6B6B6B}.ko:hover svg{color:#000000}.kp:hover p{color:#000000}.qc:hover{background-color:#F2F2F2}.rm:hover{background:#F2F2F2}.rn:hover{border-color:#F2F2F2}.ro:hover{cursor:wait}.rp:hover{color:#F2F2F2}.rq:hover{fill:#F2F2F2}.xt:hover{fill:#000000}.xw:hover:not(:disabled) svg path{fill:#000000}.yj:hover{color:#000000}.yk:hover{border-color:#242424}.yl:hover{cursor:pointer}.bc:focus-within path{fill:#242424}.jp:focus{fill:rgba(117, 117, 117, 1)}.kj:focus svg path{fill:#6B6B6B}.kq:focus svg{color:#000000}.op:focus{transform:scale(1.01)}.xx:focus svg path{fill:#000000}.jv:active{border-style:none}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ec{display:flex}.ev{margin-bottom:68px}.ez{max-width:680px}.gn{font-size:42px}.go{margin-top:1.19em}.gp{margin-bottom:32px}.gq{line-height:52px}.gr{letter-spacing:-0.011em}.hb{align-items:center}.in{border-top:solid 1px #F2F2F2}.io{border-bottom:solid 1px #F2F2F2}.ip{margin:32px 0 0}.iq{padding:3px 8px}.iz> *{margin-right:24px}.ja> :last-child{margin-right:0}.kc{margin-top:0px}.ly{font-size:20px}.lz{margin-top:2em}.ma{line-height:32px}.mb{letter-spacing:-0.003em}.mg{margin-top:2.14em}.mo{margin-top:1.14em}.ng{margin-top:1.72em}.nh{line-height:24px}.ni{letter-spacing:0}.no{margin-top:0.86em}.nz{margin-top:56px}.pm{font-size:24px}.pn{margin-top:1.95em}.po{line-height:30px}.pp{letter-spacing:-0.016em}.qj{margin-bottom:88px}.qv{display:inline-block}.rb{padding-top:72px}.sn{margin-top:40px}.sx{margin:0}.tn{width:calc(100% + 32px)}.to{margin-left:-16px}.tp{margin-right:-16px}.ug{padding-left:16px}.uh{padding-right:16px}.ui{flex-basis:50%}.uj{max-width:50%}.up{padding-bottom:56px}.vb{gap:24px 0}.vc{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.vj{display:block}.vs{margin-bottom:16px}.wm{padding-bottom:16px}.wn{flex:1 0 auto}.ww{max-height:48px}.wx{-webkit-line-clamp:2}.xf{padding-top:16px}.xo{max-width:56%}.xp{flex:1 0 0}.yc{margin-bottom:24px}.yf{flex-direction:row}.yt{width:min-width}.yy{margin-top:96px}.zq{margin-top:24px}.zr{margin-bottom:56px}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.kb{margin-top:0px}.ov{margin-left:auto}.ow{text-align:center}.qu{display:inline-block}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.ka{margin-top:0px}.qt{display:inline-block}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.jy{margin-top:0px}.jz{margin-right:0px}.qs{display:inline-block}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.ds{justify-content:center}.er{margin-bottom:4px}.ft{font-size:32px}.fu{margin-top:1.01em}.fv{margin-bottom:24px}.fw{line-height:38px}.fx{letter-spacing:-0.014em}.gx{align-items:flex-start}.hw{flex-direction:column}.ib{margin:24px -24px 0}.ic{padding:0}.ir> *{margin-right:8px}.is> :last-child{margin-right:24px}.jj{margin-left:0px}.jw{margin-top:0px}.jx{margin-right:0px}.kr{border:1px solid #F2F2F2}.ks{border-radius:99em}.kt{padding:0px 16px 0px 12px}.ku{height:38px}.kv{align-items:center}.kx svg{margin-right:8px}.li{font-size:18px}.lj{margin-top:1.56em}.lk{line-height:28px}.ll{letter-spacing:-0.003em}.mk{margin-top:1.34em}.ms{font-size:16px}.mt{margin-top:1.23em}.mu{line-height:20px}.mv{letter-spacing:0}.nk{margin-top:0.67em}.nv{margin-top:40px}.oy{font-size:20px}.oz{margin-top:1.2em}.pa{line-height:24px}.qf{margin-bottom:80px}.qr{display:inline-block}.qx{padding-top:48px}.sj{margin-top:32px}.st{margin:0}.tb{width:calc(100% + 24px)}.tc{margin-left:-12px}.td{margin-right:-12px}.tq{padding-left:12px}.tr{padding-right:12px}.ts{flex-basis:100%}.tt{max-width:100%}.ul{padding-bottom:32px}.ut{gap:24px 0}.uu{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.vf{display:block}.vo{margin-bottom:16px}.we{padding-bottom:16px}.wf{flex:1 0 auto}.wo{max-height:48px}.wp{-webkit-line-clamp:2}.xb{padding-top:16px}.xg{max-width:56%}.xh{flex:1 0 0}.yp{width:100%}.yu{margin-top:72px}.zk{margin-top:16px}.kw:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.eb{display:flex}.eu{margin-bottom:68px}.ey{max-width:680px}.gi{font-size:42px}.gj{margin-top:1.19em}.gk{margin-bottom:32px}.gl{line-height:52px}.gm{letter-spacing:-0.011em}.ha{align-items:center}.ij{border-top:solid 1px #F2F2F2}.ik{border-bottom:solid 1px #F2F2F2}.il{margin:32px 0 0}.im{padding:3px 8px}.ix> *{margin-right:24px}.iy> :last-child{margin-right:0}.lu{font-size:20px}.lv{margin-top:2em}.lw{line-height:32px}.lx{letter-spacing:-0.003em}.mf{margin-top:2.14em}.mn{margin-top:1.14em}.nd{margin-top:1.72em}.ne{line-height:24px}.nf{letter-spacing:0}.nn{margin-top:0.86em}.ny{margin-top:56px}.pi{font-size:24px}.pj{margin-top:1.95em}.pk{line-height:30px}.pl{letter-spacing:-0.016em}.qi{margin-bottom:88px}.ra{padding-top:72px}.sm{margin-top:40px}.sw{margin:0}.tk{width:calc(100% + 32px)}.tl{margin-left:-16px}.tm{margin-right:-16px}.uc{padding-left:16px}.ud{padding-right:16px}.ue{flex-basis:50%}.uf{max-width:50%}.uo{padding-bottom:56px}.uz{gap:24px 0}.va{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.vi{display:block}.vr{margin-bottom:16px}.wk{padding-bottom:16px}.wl{flex:1 0 auto}.wu{max-height:48px}.wv{-webkit-line-clamp:2}.xe{padding-top:16px}.xm{max-width:56%}.xn{flex:1 0 0}.yb{margin-bottom:24px}.ye{flex-direction:row}.ys{width:min-width}.yx{margin-top:96px}.zo{margin-top:24px}.zp{margin-bottom:56px}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.du{justify-content:center}.et{margin-bottom:68px}.ex{max-width:680px}.gd{font-size:42px}.ge{margin-top:1.19em}.gf{margin-bottom:32px}.gg{line-height:52px}.gh{letter-spacing:-0.011em}.gz{align-items:center}.if{border-top:solid 1px #F2F2F2}.ig{border-bottom:solid 1px #F2F2F2}.ih{margin:32px 0 0}.ii{padding:3px 8px}.iv> *{margin-right:24px}.iw> :last-child{margin-right:0}.lq{font-size:20px}.lr{margin-top:2em}.ls{line-height:32px}.lt{letter-spacing:-0.003em}.me{margin-top:2.14em}.mm{margin-top:1.14em}.na{margin-top:1.72em}.nb{line-height:24px}.nc{letter-spacing:0}.nm{margin-top:0.86em}.nx{margin-top:56px}.pe{font-size:24px}.pf{margin-top:1.95em}.pg{line-height:30px}.ph{letter-spacing:-0.016em}.qh{margin-bottom:88px}.qz{padding-top:72px}.sl{margin-top:40px}.sv{margin:0}.th{width:calc(100% + 28px)}.ti{margin-left:-14px}.tj{margin-right:-14px}.ty{padding-left:14px}.tz{padding-right:14px}.ua{flex-basis:50%}.ub{max-width:50%}.un{padding-bottom:56px}.ux{gap:24px 0}.uy{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.vh{display:block}.vq{margin-bottom:16px}.wi{padding-bottom:16px}.wj{flex:1 0 auto}.ws{max-height:48px}.wt{-webkit-line-clamp:2}.xd{padding-top:16px}.xk{max-width:56%}.xl{flex:1 0 0}.ya{margin-bottom:24px}.yd{flex-direction:row}.yr{width:min-width}.yw{margin-top:96px}.zm{margin-top:24px}.zn{margin-bottom:56px}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dt{justify-content:center}.es{margin-bottom:4px}.fy{font-size:32px}.fz{margin-top:1.01em}.ga{margin-bottom:24px}.gb{line-height:38px}.gc{letter-spacing:-0.014em}.gy{align-items:flex-start}.hx{flex-direction:column}.id{margin:24px 0 0}.ie{padding:0}.it> *{margin-right:8px}.iu> :last-child{margin-right:8px}.jk{margin-left:0px}.ky{border:1px solid #F2F2F2}.kz{border-radius:99em}.la{padding:0px 16px 0px 12px}.lb{height:38px}.lc{align-items:center}.le svg{margin-right:8px}.lm{font-size:18px}.ln{margin-top:1.56em}.lo{line-height:28px}.lp{letter-spacing:-0.003em}.ml{margin-top:1.34em}.mw{font-size:16px}.mx{margin-top:1.23em}.my{line-height:20px}.mz{letter-spacing:0}.nl{margin-top:0.67em}.nw{margin-top:40px}.pb{font-size:20px}.pc{margin-top:1.2em}.pd{line-height:24px}.qg{margin-bottom:80px}.qy{padding-top:48px}.sk{margin-top:32px}.su{margin:0}.te{width:calc(100% + 24px)}.tf{margin-left:-12px}.tg{margin-right:-12px}.tu{padding-left:12px}.tv{padding-right:12px}.tw{flex-basis:100%}.tx{max-width:100%}.um{padding-bottom:32px}.uv{gap:24px 0}.uw{grid-template-areas:"image image image image image image image image image image image image" "content content content content content content content content content content content content"}.vg{display:block}.vp{margin-bottom:16px}.wg{padding-bottom:16px}.wh{flex:1 0 auto}.wq{max-height:48px}.wr{-webkit-line-clamp:2}.xc{padding-top:16px}.xi{max-width:56%}.xj{flex:1 0 0}.yq{width:100%}.yv{margin-top:72px}.zl{margin-top:16px}.ld:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="print">.qp{display:none}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.om{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style><style type="text/css" data-fela-rehydration="696" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.wd{max-height:none}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="l m n o c"><div class="am q r s ds u dt w du i d y z"><a class="dw ag dx be ak b am an ao ap aq ar as at s u w i d q dy z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa37fc706b865&amp;%7Efeature=LiOpenInAppButton&amp;%7Echannel=ShowPostUnderUser&amp;source=---two_column_layout_nav----------------------------------" rel="noopener follow">Open in app<svg width="10" height="10" viewBox="0 0 10 10" fill="none" class="dv"><path d="M.98 8.48a.37.37 0 1 0 .54.54l-.54-.54zm7.77-7.23h.38c0-.2-.17-.38-.38-.38v.38zM8.37 6.5a.37.37 0 1 0 .76 0h-.76zM3.5.87a.37.37 0 1 0 0 .76V.88zM1.52 9.03l7.5-7.5-.54-.54-7.5 7.5.54.54zm6.86-7.77V6.5h.74V1.25h-.74zm-4.88.38h5.25V.88H3.5v.74z" fill="currentColor"></path></svg></a></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" href="https://medium.com/?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="dz ea"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a><div class="aw h"><div class="ab ax ay az ba q bb bc"><div class="bl" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bm bn ab"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ax bd be bf z bg bh bi bj bk" placeholder="Search" value=""/></div></div></div><div class="h k w eb ec"><div class="ed ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" href="https://medium.com/new-story?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ee ef ab q eg eh"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Write"><path d="M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z" fill="currentColor"></path><path d="M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2" stroke="currentColor"></path></svg><div class="dv l">Write</div></div></a></div></div><div class="k j i d"><div class="ed ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" href="https://medium.com/search?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ee ef ab q eg eh"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Search"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div></a></div></div><div class="ed ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerNotificationButton" href="https://medium.com/me/notifications?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ee ef ab q eg eh"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Notifications"><path d="M15 18.5a3 3 0 1 1-6 0" stroke="currentColor" stroke-linecap="round"></path><path d="M5.5 10.53V9a6.5 6.5 0 0 1 13 0v1.53c0 1.42.56 2.78 1.57 3.79l.03.03c.26.26.4.6.4.97v2.93c0 .14-.11.25-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.93c0-.37.14-.71.4-.97l.03-.03c1-1 1.57-2.37 1.57-3.79z" stroke="currentColor" stroke-linejoin="round"></path></svg></div></a></div><div class="l" aria-hidden="false"><button class="ax ei am ab q ao ej ek el" aria-label="user options menu" data-testid="headerUserIcon"><div class="l ee"><div class="l ee"><img alt="Aygiz Kunafin" class="l eq bx by bz cw" src="https://miro.medium.com/v2/resize:fill:64:64/0*W4kEaTY72ng3L8ng.jpg" width="32" height="32" loading="lazy"/><div class="em bx l by bz eo n ax ep"></div></div></div></button></div></div></div><div class="l"><div class="er es et eu ev l"><div class="ab ca"><div class="ch bg ew ex ey ez"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="eo ff fg fh fi fj"></div><div class="fk fl fm fn fo"><div class="ab ca"><div class="ch bg ew ex ey ez"><div><h1 id="729f" class="pw-post-title fp fq fr be fs ft fu fv fw fx fy fz ga gb gc gd ge gf gg gh gi gj gk gl gm gn go gp gq gr bj" data-testid="storyTitle">How to fine-tune a NLLB-200 model for translating a new language</h1><div class="gs gt gu gv gw"><div class="speechify-ignore ab co"><div class="speechify-ignore bg l"><div class="gx gy gz ha hb ab"><div><div class="ab hc"><a rel="noopener follow" href="/?source=post_page-----a37fc706b865--------------------------------"><div><div class="bl" aria-hidden="false"><div class="l hd he bx hf hg"><div class="l ee"><img alt="David Dale" class="l eq bx dc dd cw" src="https://miro.medium.com/v2/resize:fill:88:88/0*Lnxo9LylIaenwvDF." width="44" height="44" loading="lazy" data-testid="authorPhoto"/><div class="hh bx l dc dd eo n hi ep"></div></div></div></div></div></a></div></div><div class="bm bg l"><div class="ab"><div style="flex:1"><span class="be b bf z bj"><div class="hj ab q"><div class="ab q hk"><div class="ab q"><div><div class="bl" aria-hidden="false"><p class="be b hl hm bj"><a class="af ag ah ai aj ak al am an ao ap aq ar hn" data-testid="authorName" rel="noopener follow" href="/?source=post_page-----a37fc706b865--------------------------------">David Dale</a></p></div></div></div><span class="ho hp" aria-hidden="true"><span class="be b bf z dw">·</span></span><p class="be b hl hm dw"><button class="hq hr ah ai aj ak al am an ao ap aq ar hs ht hu" disabled="">Follow</button></p></div></div></span></div></div><div class="l hv"><span class="be b bf z dw"><div class="ab cm hw hx hy"><span class="be b bf z dw"><div class="ab ae"><span data-testid="storyReadTime">24 min read</span><div class="hz ia l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="be b bf z dw">·</span></span></div>Draft</div></span></div></span></div></div></div><div class="ab co ib ic id ie if ig ih ii ij ik il im in io ip iq"><div class="h k w eb ec q"><div class="jg l"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div></div><div><div class="bl" aria-hidden="false"><button class="kd jm ke kf ab q ef" aria-label="responses" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" class="kd ef jr"><path fill-rule="evenodd" clip-rule="evenodd" d="M18.47 20.27a6.08 6.08 0 0 1-4.06-1.55c-.74.2-1.51.3-2.29.3-4.48 0-8.12-3.35-8.12-7.48 0-4.15 3.64-7.5 8.12-7.5 4.48 0 8.12 3.35 8.12 7.48 0 1.98-.81 3.83-2.3 5.23.02.17.05.34.1.53.2.66.52 1.33 1 1.96a.66.66 0 0 1-.53 1.04h-.04z"></path></svg></button></div></div></div><div class="ab q ir is it iu iv iw ix iy iz ja jb jc jd je jf"><div class="kg k j i d"></div><div class="h k"><div><div class="bl" aria-hidden="false"><div class="kh ki kj kd kk kl" data-testid="headerBookmarkButton"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="km" aria-label="Add to list bookmark button" disabled=""><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></div></div></div></div><div class="eq ss cm"><div class="l ae"><div class="ab ca"><div class="st su sv sw sx oq ch bg"><div class="ab"><div class="bl bg" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq kr ks kt ku s kv kw kx ky kz la lb u lc ld le"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0zm9-10a10 10 0 1 0 0 20 10 10 0 0 0 0-20zm3.38 10.42l-4.6 3.06a.5.5 0 0 1-.78-.41V8.93c0-.4.45-.63.78-.41l4.6 3.06c.3.2.3.64 0 .84z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq kr ks kt ku s kv kw kx ky kz la lb u lc ld le"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">Share</p></div></button></div></div></div><div class="bl" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq kr ks kt ku s kv kw kx ky kz la lb u lc ld le"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="7c01" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">“NLLB” (which stands for “no language left behind”) is a family of machine translation models <a class="af md" href="https://ai.meta.com/blog/nllb-200-high-quality-machine-translation/" rel="noopener ugc nofollow" target="_blank">published by Meta AI in 2022</a>. These models can translate a sentence between any of the <a class="af md" href="https://ai.meta.com/research/no-language-left-behind/#200-languages-accordion" rel="noopener ugc nofollow" target="_blank">202 language varieties</a>, which is a huge research breakthrough. But there are about 7000 languages in the world, so most of them are still left behind so far. </p><p id="d009" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">How can we teach this model one more language? In this tutorial, I show it. As a demonstration, we will teach the model to translate between Russian and a low-resource <a class="af md" href="https://en.wikipedia.org/wiki/Tuvan_language" rel="noopener ugc nofollow" target="_blank">Tyvan language</a>, which is new to this model. The Russian language is Slavic, while Tyvan is a Turkic language with a lot of Mongolian influence. Therefore, while they are written in similar Cyrillic scripts, translating between them is not trivial. Nevertheless, thanks to the pretraining with 202 languages, NLLB can do it pretty well. </p><p id="ec1a" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The full code of the tutorial can be found in <a class="af md" href="https://colab.research.google.com/drive/1bayEaw2fz_9Mhg9jFFZhrmDlQlBj1YZf?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lh fs">this Colab notebook</strong></a>. The tutorial includes 8 steps:</p><ol class=""><li id="911c" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc mh mi mj bj">Looking at the training data</li><li id="c4cb" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">Examining tokenization of the new language</li><li id="7188" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">(Optionally) Updating the vocabulary</li><li id="b2cf" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">(Optionally) Adding a new language token</li><li id="4765" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">Training the neural model</li><li id="0ee7" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">Evaluating the model</li><li id="f159" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">Publishing</li><li id="e47d" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">(Optionally) Serving the model with a Docker app</li></ol><h2 id="9cbf" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Why NLLB?</h2><p id="9bba" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">It is one of the most popular publicly available translation models (according e.g. to <a class="af md" href="https://huggingface.co/models?pipeline_tag=translation&amp;sort=likes" rel="noopener ugc nofollow" target="_blank">the number of likes on HF</a>), and probably the most multilingual one. </p><p id="5a32" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Alternatively, you may want to use open LLMs, such as LLaMA. In the last year, LLMs became popular for multiple tasks, including translation, and they <a class="af md" href="https://arxiv.org/abs/2309.11674" rel="noopener ugc nofollow" target="_blank">are reported</a> to translate into English better than NLLB. However, these models are at least x10 larger than the <a class="af md" href="https://huggingface.co/facebook/nllb-200-distilled-600M" rel="noopener ugc nofollow" target="_blank">NLLB-200-600M</a> checkpoint which we will fine-tune, and this makes their training and deployment more difficult and expensive. Also, the pretraining data for most LLMs is usually 90% or more English, so they may not generate texts so well in other languages, especially lower-resourced ones. Thus, we stick to NLLB.</p><h2 id="8fbf" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Key concepts</h2><p id="f506" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">In this section, I briefly list the terms that we will be using. If too many of them are unfamiliar to you, please consider doing some introductory reading (e.g. <a class="af md" href="https://lena-voita.github.io/nlp_course.html" rel="noopener ugc nofollow" target="_blank">NLP course by Lena Voita</a>) before proceeding with this article.</p><ul class=""><li id="5ac3" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc np mi mj bj"><em class="nq">Corpus</em>: a diverse collection of texts (or, if it is a <em class="nq">parallel corpus</em>, of text pairs in two different languages).</li><li id="0fc9" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj"><em class="nq">Normalization</em>: conversion of something to its “normal” form, where “normal” is defined arbitrarily. For example, if we decide that quotation marks should look like <code class="cw nr ns nt nu b">&quot;</code>, then replacing <code class="cw nr ns nt nu b">«</code> with <code class="cw nr ns nt nu b">&quot;</code> would be a normalization.</li><li id="4850" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj"><em class="nq">Token</em>: a minimal unit of text with which a neural network operates. Their size is usually between a single character and a whole word with a space before it. Each model usually support a fixed set of tokens, called its <em class="nq">vocabulary</em>. <br/>For example, the vocabulary of NLLB by default has 256204 tokens. The word <code class="cw nr ns nt nu b">&quot;preprocess&quot;</code> is not in this vocabulary, but can be represented as 3 tokens, <code class="cw nr ns nt nu b">&quot;▁pre&quot; + &quot;pro&quot; + &quot;cess&quot;</code>, where <code class="cw nr ns nt nu b">&quot;▁&quot;</code> is a substitute for space and it signals the start of a new word. These tokens occupy the positions 951, 4573, and 8786 in the vocabulary.</li><li id="3faa" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj"><em class="nq">Tokenizer</em>: a tool for converting texts into token ids (their positions in the vocabulary) and back.</li><li id="07c3" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj"><em class="nq">Embedding</em>: a numerical vector that represents some object. In our case, embeddings are parts of the translation model and they represent tokens. For example, NLLB-200–600M model has 256204 embeddings (one per each token in its vocabulary), and they are 1024-dimensional vectors. Embeddings are trained together with the rest of the neural network.</li></ul><h2 id="e615" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Prerequisites</h2><p id="d95c" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">I assume that you are familiar with the <a class="af md" href="https://www.python.org" rel="noopener ugc nofollow" target="_blank">Python </a>programming language and the <a class="af md" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank">Google Colab</a> environment. Ideally, you should also understand the Huggingface ecosystem (they have <a class="af md" href="https://huggingface.co/learn/nlp-course/chapter1/1" rel="noopener ugc nofollow" target="_blank">a good course about it</a>).</p><p id="9ddb" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To reproduce this tutorial, you will need a Tyvan-Russian parallel corpus, that is, a collection of translated sentences, phrases or words. You can download a version of it from <a class="af md" href="https://tyvan.ru/" rel="noopener ugc nofollow" target="_blank">https://tyvan.ru</a>. This version contains 50K translation pairs, whereas the one I used in the notebook is larger, almost 120K pairs, so my results won’t be reproduced exactly. Nevertheless, even 50K training pairs can be enough for decent translation.</p><p id="2261" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">If you want to use another language pair, you will have to find the data for it (at least a few thousand pairs; a few hundred thousand for really good translation). Two good spots to start the search are <a class="af md" href="https://opus.nlpl.eu/" rel="noopener ugc nofollow" target="_blank">OPUS</a> and <a class="af md" href="https://huggingface.co/datasets?task_categories=task_categories%3Atranslation&amp;sort=likes" rel="noopener ugc nofollow" target="_blank">HF datasets</a>.</p><p id="a4ae" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Originally, I ran the tutorial code in a Google Colab notebook with a Tesla T4 GPU (15Gb of memory). I fine-tuned the model for about 20 hours (50K training steps); running the notebook for this duration without interruption required from me a paid Colab subscription. Fine-tuning for a few hours can usually be done on a free plan; fine-tuning with less GPU memory may require adjusting the batch size.</p><p id="f65f" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To load the dataset and save the model, I use my Google Drive, which I mount to the <code class="cw nr ns nt nu b">/gd</code> directory in Colab:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="38bb" class="od mq fr nu b bf oe of l og oh">from google.colab import drive<br/>import os<br/>if not os.path.exists(&#x27;/gd&#x27;):<br/>    drive.mount(&#x27;/gd&#x27;)</span></pre><p id="6191" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">For uploading the dataset, instead of Google Drive, you can just use the “upload” graphical interface of Colab (and change accordingly the path to the file in the code in the next section):</p><figure class="nv nw nx ny nz ol oi oj paragraph-image"><div role="button" tabindex="0" class="om on ee oo bg op"><div class="oi oj ok"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*qNRkzVmsA4GkrsTQwWKJfw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*qNRkzVmsA4GkrsTQwWKJfw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*qNRkzVmsA4GkrsTQwWKJfw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*qNRkzVmsA4GkrsTQwWKJfw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*qNRkzVmsA4GkrsTQwWKJfw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*qNRkzVmsA4GkrsTQwWKJfw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNRkzVmsA4GkrsTQwWKJfw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*qNRkzVmsA4GkrsTQwWKJfw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*qNRkzVmsA4GkrsTQwWKJfw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*qNRkzVmsA4GkrsTQwWKJfw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*qNRkzVmsA4GkrsTQwWKJfw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*qNRkzVmsA4GkrsTQwWKJfw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*qNRkzVmsA4GkrsTQwWKJfw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*qNRkzVmsA4GkrsTQwWKJfw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg oq or c" width="700" height="385" loading="lazy" role="presentation"/></picture></div></div><figcaption class="os ot ou oi oj ov ow be b bf z dw">File upload interface. Colab screenshot by author.</figcaption></figure><p id="94d5" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Before running my notebook, I install a few Python modules:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="3c70" class="od mq fr nu b bf oe of l og oh">!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu -q</span></pre><p id="1980" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The specific version <code class="cw nr ns nt nu b">transformers==4.33</code> is important, because the way I mingle with the tokenizer at the Step 4 depends on it. In the version <code class="cw nr ns nt nu b">4.34</code>, the package started introducing breaking changes to the tokenizer, and when these changes stabilize, the recommended code for updating the tokenizer will be different.</p><h1 id="32bf" class="ox mq fr be mr oy oz pa mv pb pc pd mz pe pf pg ph pi pj pk pl pm pn po pp pq bj">The steps for adding a new language to NLLB</h1><h2 id="0425" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Step 1: looking at the data</h2><p id="4a29" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">I start by reading the training dataset and taking a look at it:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="cea5" class="od mq fr nu b bf oe of l og oh">import pandas as pd<br/>trans_df = pd.read_csv(&#x27;/gd/MyDrive/datasets/nlp/tyvan/rus_tyv_parallel_50k.tsv&#x27;, sep=&quot;\t&quot;)<br/>print(trans_df.shape) # (50000, 5)<br/>print(trans_df.columns) # [&#x27;row_id&#x27;, &#x27;ind&#x27;, &#x27;tyv&#x27;, &#x27;ru&#x27;, &#x27;split&#x27;]<br/>trans_df.sample(10)</span></pre><p id="c6f8" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Here is the output; <code class="cw nr ns nt nu b">tyv</code> and <code class="cw nr ns nt nu b">ru</code> are the columns with the translation pairs. </p><figure class="nv nw nx ny nz ol oi oj paragraph-image"><div role="button" tabindex="0" class="om on ee oo bg op"><div class="oi oj pr"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*55P9jCCcCvl89cp0bJ4beQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*55P9jCCcCvl89cp0bJ4beQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*55P9jCCcCvl89cp0bJ4beQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*55P9jCCcCvl89cp0bJ4beQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*55P9jCCcCvl89cp0bJ4beQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*55P9jCCcCvl89cp0bJ4beQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55P9jCCcCvl89cp0bJ4beQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*55P9jCCcCvl89cp0bJ4beQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*55P9jCCcCvl89cp0bJ4beQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*55P9jCCcCvl89cp0bJ4beQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*55P9jCCcCvl89cp0bJ4beQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*55P9jCCcCvl89cp0bJ4beQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*55P9jCCcCvl89cp0bJ4beQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*55P9jCCcCvl89cp0bJ4beQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg oq or c" width="700" height="323" loading="lazy" role="presentation"/></picture></div></div><figcaption class="os ot ou oi oj ov ow be b bf z dw">A sample of the source dataset</figcaption></figure><p id="1d03" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">In this dataset, most of the texts are individual words or short phrases, and only a few of them are complete sentences. This is not typical for the machine translation data; normally it is collected on the sentence level. However, with low-resource languages, dictionaries may be a dominant data source, and they are constructed mostly on the word or phrase level, so such distribution of texts is not totally unusual. In any case, you need to make sure that the training texts do contain many diverse sentences, and that they look more or less clean and consistent. If there are longer training texts (e.g. paragraphs), it is recommended to split them in sentences (e.g. with <a class="af md" href="https://pypi.org/project/sentence-splitter/" rel="noopener ugc nofollow" target="_blank">this tool</a>), and re-align them into translation pairs on the sentence level (e.g. with <a class="af md" href="https://pypi.org/project/lingtrain-aligner/" rel="noopener ugc nofollow" target="_blank">lingtrain-aligner</a> or <a class="af md" href="https://github.com/thompsonb/vecalign" rel="noopener ugc nofollow" target="_blank">vecalign</a>).</p><p id="fe4e" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">This dataset is pre-split into train, dev and test subsets. I did it previously with <code class="cw nr ns nt nu b">sklearn.model_selection.train_test_split</code> ; in the training notebook, I load the splits into separate dataframes:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="d7a9" class="od mq fr nu b bf oe of l og oh">df_train = trans_df[trans_df.split==&#x27;train&#x27;].copy() # 49000 items<br/>df_dev = trans_df[trans_df.split==&#x27;dev&#x27;].copy()     # 500 items<br/>df_test = trans_df[trans_df.split==&#x27;test&#x27;].copy()   # 500 items</span></pre><h2 id="8745" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj"><strong class="al">Step 2: </strong>How well does the data fit into a NLLB tokenizer?</h2><p id="4b59" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">The NLLB models (as most of other modern NLP neural model) consist of 2 components: </p><ol class=""><li id="7d3e" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc mh mi mj bj">the <em class="nq">tokenizer</em> (a thing that splits the text into chunks and maps each chunk into a number, according to a pre-defined vocabulary);</li><li id="fe07" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">the neural network itself (it performs the translation based on these numbers and outputs some new numbers; then the tokenizer converts them back to texts).</li></ol><p id="96c0" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">So the translation always goes the way “input text -&gt; input tokens -&gt; translated tokens -&gt; translated text”, like in this example:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="6324" class="od mq fr nu b bf oe of l og oh">from transformers import AutoModelForSeq2SeqLM, AutoTokenizer<br/>model_name = &quot;facebook/nllb-200-distilled-600M&quot;<br/>tokenizer = AutoTokenizer.from_pretrained(model_name)<br/>model = AutoModelForSeq2SeqLM.from_pretrained(model_name)<br/><br/>tokenizer.src_lang = &quot;rus_Cyrl&quot;<br/>inputs = tokenizer(text=&quot;поля озарились утренним солнцем&quot;, return_tensors=&quot;pt&quot;)<br/>translated_tokens = model.generate(<br/>    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[&quot;eng_Latn&quot;]<br/>)<br/>print(tokenizer.decode(translated_tokens[0], skip_special_tokens=True))<br/># The fields were lit by the morning sun</span></pre><p id="b40d" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj"><em class="nq">(If this code snippet intimidates you, please consider taking </em><a class="af md" href="https://huggingface.co/learn/nlp-course/chapter1/1" rel="noopener ugc nofollow" target="_blank"><em class="nq">the Huggingface course</em></a><em class="nq"> before returning to this tutorial)</em></p><p id="c924" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The quality of translation critically depends on how well the tokenizer represents our languages:</p><ul class=""><li id="8a18" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc np mi mj bj">How many tokens per word do we have on average? For a good translation quality, we usually want one token to represent a word or a morpheme, or at least something of a comparable size.</li><li id="d2fd" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">Does the tokenizer support most of our vocabulary? All unsupported characters are converted to the special <code class="cw nr ns nt nu b">&lt;unk&gt;</code> token that carries very little information; many such cases again degrade the quality.</li></ul><p id="3e7a" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Tokenization is something that we can test before even touching the translation model itself. I extract a sample of the training data and count the number of words and tokens in it: </p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="c9e8" class="od mq fr nu b bf oe of l og oh">import re<br/><br/>def word_tokenize(text):<br/>    &quot;&quot;&quot;<br/>    Split a text into words, numbers, and punctuation marks<br/>    (for languages where words are separated by spaces)<br/>    &quot;&quot;&quot;<br/>    return re.findall(&#x27;(\w+|[^\w\s])&#x27;, text)<br/><br/>smpl = df_train.sample(10000, random_state=1)<br/>smpl[&#x27;rus_toks&#x27;] = smpl.ru.apply(tokenizer.tokenize)<br/>smpl[&#x27;tyv_toks&#x27;] = smpl.tyv.apply(tokenizer.tokenize)<br/>smpl[&#x27;rus_words&#x27;] = smpl.ru.apply(word_tokenize)<br/>smpl[&#x27;tyv_words&#x27;] = smpl.tyv.apply(word_tokenize)</span></pre><p id="2910" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Now I can take a glance at the tokens. They look adequately (at least, to my subjective eye); the average of 2–3 tokens per word is typical for well-tokenized texts in morphologically rich languages, such as Russian or Tyvan.</p><figure class="nv nw nx ny nz ol oi oj paragraph-image"><div role="button" tabindex="0" class="om on ee oo bg op"><div class="oi oj ps"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*llVgsAL208N0Ysaq2O74nw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*llVgsAL208N0Ysaq2O74nw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*llVgsAL208N0Ysaq2O74nw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*llVgsAL208N0Ysaq2O74nw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*llVgsAL208N0Ysaq2O74nw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*llVgsAL208N0Ysaq2O74nw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*llVgsAL208N0Ysaq2O74nw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*llVgsAL208N0Ysaq2O74nw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*llVgsAL208N0Ysaq2O74nw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*llVgsAL208N0Ysaq2O74nw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*llVgsAL208N0Ysaq2O74nw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*llVgsAL208N0Ysaq2O74nw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*llVgsAL208N0Ysaq2O74nw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*llVgsAL208N0Ysaq2O74nw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg oq or c" width="700" height="184" loading="lazy" role="presentation"/></picture></div></div><figcaption class="os ot ou oi oj ov ow be b bf z dw">A sample of tokenized Russian and Tyvan texts</figcaption></figure><p id="bbd3" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Actually, we can compute precise statistics of this (they are explained <a class="af md" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html" rel="noopener ugc nofollow" target="_blank">here</a>):</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="ad3c" class="od mq fr nu b bf oe of l og oh">stats = smpl[<br/>    [&#x27;rus_toks&#x27;, &#x27;tyv_toks&#x27;, &#x27;rus_words&#x27;, &#x27;tyv_words&#x27;]<br/>].applymap(len).describe()<br/>print(stats.rus_toks[&#x27;mean&#x27;] / stats.rus_words[&#x27;mean&#x27;])  # 2.0349<br/>print(stats.tyv_toks[&#x27;mean&#x27;] / stats.tyv_words[&#x27;mean&#x27;])  # 2.4234<br/>stats</span></pre><figure class="nv nw nx ny nz ol oi oj paragraph-image"><div class="oi oj pt"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ywMjfQy7ZYELR9-mJD-1LA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ywMjfQy7ZYELR9-mJD-1LA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ywMjfQy7ZYELR9-mJD-1LA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ywMjfQy7ZYELR9-mJD-1LA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ywMjfQy7ZYELR9-mJD-1LA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ywMjfQy7ZYELR9-mJD-1LA.png 1100w, https://miro.medium.com/v2/resize:fit:1054/format:webp/1*ywMjfQy7ZYELR9-mJD-1LA.png 1054w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 527px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*ywMjfQy7ZYELR9-mJD-1LA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ywMjfQy7ZYELR9-mJD-1LA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ywMjfQy7ZYELR9-mJD-1LA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ywMjfQy7ZYELR9-mJD-1LA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ywMjfQy7ZYELR9-mJD-1LA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ywMjfQy7ZYELR9-mJD-1LA.png 1100w, https://miro.medium.com/v2/resize:fit:1054/1*ywMjfQy7ZYELR9-mJD-1LA.png 1054w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 527px"/><img alt="" class="bg oq or c" width="527" height="315" loading="lazy" role="presentation"/></picture></div><figcaption class="os ot ou oi oj ov ow be b bf z dw">Statistics of the word and token counts</figcaption></figure><p id="627a" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Good news: for Tyvan, a new language, the NLLB tokenizer produces on average 2.4 tokens per word; almost as few as 2.0 for the well-supported Russian language. This implies that the translation quality of fine-tuned NLLB may be decent without extending its vocabulary with Tyvan tokens.</p><p id="f76b" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Another useful check: how often does the <code class="cw nr ns nt nu b">&lt;unk&gt;</code> token happen in the tokenizer output for Tyvan? If this is too often, we need to fix it somehow.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="71a4" class="od mq fr nu b bf oe of l og oh">from tqdm.auto import tqdm, trange<br/>texts_with_unk = [<br/>    text for text in tqdm(trans_df.tyv) <br/>    if tokenizer.unk_token_id in tokenizer(text).input_ids<br/>]<br/>print(len(texts_with_unk))<br/># 163<br/>s = random.sample(texts_with_unk, 5)<br/>print(s)<br/># [&#x27;Ынча деп турар болзуңза, сени кандыг далайжы дээрил?! – деп, Дриниан удурланган.&#x27;<br/># &#x27;Ыяштап чоруй барган ачам келгеш, мени аажок мактаар боор: «Ёзулуг аңчы-дыр сен, оглум!»&#x27;,<br/># &#x27;Ажыл хуваарга — арыг кирер, ажык хуваарга — куруг үнер&#x27;, ...</span></pre><p id="beab" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">We can see that out of 49K texts, 163 contain an “unknown symbol” after tokenization. Most of these cases seem to be associated with non-standard punctuation marks (as defined by <a class="af md" href="https://github.com/alvations/sacremoses#normalizer" rel="noopener ugc nofollow" target="_blank">MosesPunctNormalizer</a>), and there is a reason for that: the NLLB team preprocessed their texts before training the tokenizer and the model. The code for preprocessing (<a class="af md" href="https://github.com/facebookresearch/stopes/blob/main/stopes/pipelines/monolingual/monolingual_line_processor.py#L214" rel="noopener ugc nofollow" target="_blank">adapted from the Stopes repo</a>) looks like this:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="388f" class="od mq fr nu b bf oe of l og oh">import re<br/>import sys<br/>import unicodedata<br/>from sacremoses import MosesPunctNormalizer<br/><br/>mpn = MosesPunctNormalizer(lang=&quot;en&quot;)<br/>mpn.substitutions = [<br/>    (re.compile(r), sub) for r, sub in mpn.substitutions<br/>]<br/><br/>def get_non_printing_char_replacer(replace_by: str = &quot; &quot;):<br/>    non_printable_map = {<br/>        ord(c): replace_by<br/>        for c in (chr(i) for i in range(sys.maxunicode + 1))<br/>        # same as \p{C} in perl<br/>        # see https://www.unicode.org/reports/tr44/#General_Category_Values<br/>        if unicodedata.category(c) in {&quot;C&quot;, &quot;Cc&quot;, &quot;Cf&quot;, &quot;Cs&quot;, &quot;Co&quot;, &quot;Cn&quot;}<br/>    }<br/><br/>    def replace_non_printing_char(line) -&gt; str:<br/>        return line.translate(non_printable_map)<br/><br/>    return replace_non_printing_char<br/><br/>replace_nonprint = get_non_printing_char_replacer(&quot; &quot;)<br/><br/>def preproc(text):<br/>    clean = mpn.normalize(text)<br/>    clean = replace_nonprint(clean)<br/>    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca<br/>    clean = unicodedata.normalize(&quot;NFKC&quot;, clean)<br/>    return clean</span></pre><p id="fa3f" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">If we apply this normalization before tokenizing the texts, all the “unknown” characters disappear.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="eacf" class="od mq fr nu b bf oe of l og oh">texts_with_unk_normed = [<br/>    text for text in tqdm(texts_with_unk) <br/>    if tokenizer.unk_token_id in tokenizer(preproc(text)).input_ids<br/>]<br/>print(len(texts_with_unk_normed))  # 0</span></pre><p id="c2fb" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To sum up, NLLB tokenizer produces tokens for Tyvan which are long enough, and successfully recognizes all Tyvan characters. I will use it as evidence that it is not neccessary to update the tokenizer’s vocabulary to use it with Tyvan.</p><h2 id="f451" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Step 3 (optional): Expanding the vocabulary</h2><p id="51f1" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">As we have concluded on the previous step, expanding the tokenizer’s vocabulary is not necessary for Tuvan: there are no out-of vocabulary characters, and the average token length is comparable with that of a high-resource language (Russian). If this was not the case, we would have to add some new tokens for Tyvan into the tokenizer’s vocabulary (and into the neural network as well). This section explains how to achieve that.</p><p id="55c0" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">I start by getting more Tyvan texts for training a Tyvan-specific tokenizer. These texts don’t have to be parallel with another language, so I just download a parsed version of the Tyvan Wikipedia. This triples the total length of the Tyvan texts I had:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="b5cf" class="od mq fr nu b bf oe of l og oh">from datasets import load_dataset<br/>tyv_wiki = load_dataset(&quot;graelo/wikipedia&quot;, &quot;20230601.tyv&quot;)<br/>tyv_wiki<br/># DatasetDict({<br/>#     train: Dataset({<br/>#         features: [&#x27;id&#x27;, &#x27;url&#x27;, &#x27;title&#x27;, &#x27;text&#x27;],<br/>#         num_rows: 3459<br/>#     })<br/># })<br/>print(sum(len(t) for t in tyv_wiki[&#x27;train&#x27;][&#x27;text&#x27;]))  # 7568832<br/>print(sum(len(t) for t in trans_df.tyv.dropna()))      # 3573803</span></pre><p id="cf50" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">After that, I put all my text together and preprocess them. Then I count their characters, in order to force my tokenizer to include all the characters that appear at least 3 times (the characters that appear only once or twice won’t probably be learned by the model anyway). I also exclude the space, because it is always converted to an underscore character by sentencepiece.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="dc93" class="od mq fr nu b bf oe of l og oh">from collections import Counter<br/>all_texts = tyv_wiki[&#x27;train&#x27;][&#x27;text&#x27;] + df_train.tyv.dropna().tolist()<br/>all_text_normalized = [preproc(t) for t in tqdm(all_texts)]<br/>chars_cnt = Counter(c for t in all_text_normalized for c in t)<br/>required_chars = &#x27;&#x27;.join([<br/>    k for k, v in chars_cnt.most_common() <br/>    if v &gt;= 3 and k not in &#x27; &#x27;<br/>])</span></pre><p id="fd63" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">I dump the texts into a plaintext file, and train a new <a class="af md" href="https://github.com/google/sentencepiece" rel="noopener ugc nofollow" target="_blank">sentencepiece</a> tokenizer model on this file, in order to add its tokens to the existing NLLB tokenizer. Sentencepiece is one of the popular <a class="af md" href="https://huggingface.co/docs/transformers/tokenizer_summary" rel="noopener ugc nofollow" target="_blank">algorithms to train a tokenizer</a>, and the NLLB tokenizer already has it under the hood.</p><p id="804d" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">I chose the vocabulary size to be 16384 intuitively, because such a number of tokens can potentially cover the most important roots and suffixes in the language (to compare: NLLB vocabulary for 200 languages has 256000 tokens, but many of them are used by a lot of different languages). All the other parameters are not very important.</p><p id="b349" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">This code executes for several minutes.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="17f5" class="od mq fr nu b bf oe of l og oh">import sentencepiece as spm<br/>all_texts_file = &#x27;myv_texts_plain.txt&#x27;<br/>SPM_PREFIX = &#x27;spm_tyvan_16k&#x27;<br/>with open(all_texts_file, &#x27;w&#x27;) as f:<br/>    for i, text in enumerate(all_texts):<br/>        print(text, file=f)<br/><br/>spm.SentencePieceTrainer.train(<br/>    input=all_texts_file,<br/>    model_prefix=SPM_PREFIX,<br/>    vocab_size=2**14,  # 16K<br/>    character_coverage = 1,<br/>    num_threads=16,<br/>    train_extremely_large_corpus=False,<br/>    add_dummy_prefix=False,<br/>    max_sentencepiece_length=128,<br/>    max_sentence_length=4192*4,<br/>    pad_id=0,<br/>    eos_id=1,<br/>    unk_id=2,<br/>    bos_id=-1,<br/>    required_chars=required_chars,<br/>)</span></pre><p id="915a" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">After training a Tyvan tokenizer, I perform a “surgical operation” with it: extracting the sentencepiece model from the standard NLLB tokenizer and enriching it from all tokens from the Tyvan tokenizer that have been missing from the NLLB tokenizer (based on <a class="af md" href="https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb" rel="noopener ugc nofollow" target="_blank">the example from the sentencepiece repo</a>).</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="21b3" class="od mq fr nu b bf oe of l og oh">from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model<br/># At this step, the code may throw an error about protobuf. Do as it tells.<br/>from transformers import NllbTokenizer<br/><br/># reading the NLLB and the Tyvan sentencepiece models into a native format<br/>tokenizer = NllbTokenizer.from_pretrained(&#x27;facebook/nllb-200-distilled-600M&#x27;)<br/>sp_trained = spm.SentencePieceProcessor(model_file=f&#x27;{SPM_PREFIX}.model&#x27;)<br/>added_spm = sp_pb2_model.ModelProto()<br/>added_spm.ParseFromString(sp_trained.serialized_model_proto())<br/>old_spm = sp_pb2_model.ModelProto()<br/>old_spm.ParseFromString(tokenizer.sp_model.serialized_model_proto())<br/><br/># adding the missing tokens to the NLLB sentencepiece model<br/>nllb_tokens_set = {p.piece for p in old_spm.pieces}<br/>prev_min_score = old_spm.pieces[-1].score<br/>for p in added_spm.pieces:<br/>    piece = p.piece<br/>    if piece not in nllb_tokens_set:<br/>        new_p = sp_pb2_model.ModelProto().SentencePiece()<br/>        new_p.piece = piece<br/>        # for all new tokens, I&#x27;ll set a lower score (priority)<br/>        new_p.score = p.score + prev_min_score<br/>        old_spm.pieces.append(new_p)<br/><br/># saving the result to disk<br/>NEW_SPM_NAME = &#x27;spm_nllb_tyvan_268k.model&#x27;<br/>with open(NEW_SPM_NAME, &#x27;wb&#x27;) as f:<br/>    f.write(old_spm.SerializeToString())</span></pre><p id="eccf" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Finally, I need to update the neural network weights: add new embeddings for the freshly added tokens. In NLLB, the token embeddings reside in the parameter called <code class="cw nr ns nt nu b">shared</code>. It is used both in the encoder and decoder input embeddings and in the last decoder layer that predicts the distribution of the next token.</p><p id="fc5d" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">By default, the embeddings for the new tokens are initialized randomly. Instead, I re-initialize each one with the average of the embeddings of the old tokens that corresponded to the new token (or if there are none, with the embedding of the <code class="cw nr ns nt nu b">&lt;unk&gt;</code> token). This slightly improves the training speed, because the newly created tokken embeddings are already informative.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="8058" class="od mq fr nu b bf oe of l og oh">from transformers import AutoModelForSeq2SeqLM<br/>model_name = &#x27;facebook/nllb-200-distilled-600M&#x27;<br/><br/># loading the tokenizers<br/>tokenizer_old = NllbTokenizer.from_pretrained(model_name)<br/>tokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file=NEW_SPM_NAME)<br/>print(len(tokenizer_old), len(tokenizer)) # 256204, 268559<br/>added_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_old.get_vocab()))<br/>print(len(added_vocab))  # 12355<br/><br/># loading and resizing the model<br/>model = AutoModelForSeq2SeqLM.from_pretrained(model_name)<br/>model.resize_token_embeddings(len(tokenizer))<br/><br/># re-initializing the new embeddings<br/>for t in tqdm(added_vocab):<br/>    tt = tokenizer_old(t, add_special_tokens=False).input_ids<br/>    if len(tt) == 0:<br/>        tt = [tokenizer_old.unk_token_id]<br/>    idx = tokenizer.convert_tokens_to_ids(t)<br/>    model.model.shared.weight.data[idx] = model.model.shared.weight.data[tt].mean(0)</span></pre><p id="5451" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">In the next steps of the tutorial, I will ignore the results of this optional step, and instead just continue from Step 2. But if you want to run vocabulary extension on your data, you can adapt my code <a class="af md" href="https://colab.research.google.com/drive/1G1iB8H_XaspNJKyR8k__GDzcHbQsMUAG?usp=sharing" rel="noopener ugc nofollow" target="_blank">from this notebook</a>.</p><h2 id="2e7a" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Step 4 (optional): Adding a new language tag</h2><p id="a15a" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">In a NLLB tokenizer, language tags are special: they are tokens prepended to the source and target texts, and the model uses them to correctly identify source and target languages. If fine-tune a NLLB model, you may want to add a new language tag to the model and the tokenizer. However, if any of the following is true, you can skip this step and instead go directly to Step 5:</p><ul class=""><li id="2db4" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc np mi mj bj">The languages that you are fine-tuning with are already included in the model;</li><li id="b2da" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">You will reuse one of the existing language tags for your new language;</li><li id="650b" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">You are going to use the model only for a single pair of languages, so it can always just guess them from the source text.</li></ul><p id="2582" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The language tag tokens are not saved in the sentencepiece vocabulary; instead, <a class="af md" href="https://github.com/huggingface/transformers/blob/v4.33.3/src/transformers/models/nllb/tokenization_nllb.py#L45" rel="noopener ugc nofollow" target="_blank">they are stored in a hardcoded list</a>. And this poses a problem when we try adding a new language token: the list is hardcoded (at least, <a class="af md" href="https://github.com/huggingface/transformers/issues/26497" rel="noopener ugc nofollow" target="_blank">for now</a>).</p><p id="116f" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To offset this problem, I write a function that re-runs a part of the tokenizer’s <code class="cw nr ns nt nu b">init</code> code with a new language token. Unfortunately, calling this function once is not enough; you need to do this every time after you load the tokenizer from disk.</p><p id="5753" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj"><strong class="lh fs">Disclaimer: when I was working on this code, I used the package version transformers&lt;=4.33. <br/>Later, I will publish an update that supports the newer versions.</strong></p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="5305" class="od mq fr nu b bf oe of l og oh">def fix_tokenizer(tokenizer, new_lang=&#x27;tyv_Cyrl&#x27;):<br/>    &quot;&quot;&quot;<br/>    Add a new language token to the tokenizer vocabulary <br/>    (this should be done each time after its initialization)<br/>    &quot;&quot;&quot;<br/>    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)<br/>    tokenizer.lang_code_to_id[new_lang] = old_len-1<br/>    tokenizer.id_to_lang_code[old_len-1] = new_lang<br/>    # always move &quot;mask&quot; to the last position<br/>    tokenizer.fairseq_tokens_to_ids[&quot;&lt;mask&gt;&quot;] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset<br/><br/>    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)<br/>    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}<br/>    if new_lang not in tokenizer._additional_special_tokens:<br/>        tokenizer._additional_special_tokens.append(new_lang)<br/>    # clear the added token encoder; otherwise a new token may end up there by mistake<br/>    tokenizer.added_tokens_encoder = {}<br/>    tokenizer.added_tokens_decoder = {} </span></pre><p id="71d2" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">I apply this function to the tokenizer, and it adds one new language token to it. Then I expand the embedding layer of the model accordingly, after which I need to patch the embeddings of the model, for two reasons:</p><ol class=""><li id="ed1a" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc mh mi mj bj">In NLLB vocabulary, for some unknown reason, the <code class="cw nr ns nt nu b">&lt;mask&gt;</code> token always goes after all the language codes, so if I add one more, the <code class="cw nr ns nt nu b">&lt;mask&gt;</code> token also moves; thus, I move its embedding.</li><li id="c169" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc mh mi mj bj">The embedding for the new token is by default initialized randomly, but instead I choose to initialize it with the language code for a similar language: Kyrgyz.</li></ol><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="6498" class="od mq fr nu b bf oe of l og oh">from transformers import AutoModelForSeq2SeqLM, AutoTokenizer<br/>model_name = &quot;facebook/nllb-200-distilled-600M&quot;<br/># loading the tokenizer and the model<br/>tokenizer = AutoTokenizer.from_pretrained(model_name)<br/>model = AutoModelForSeq2SeqLM.from_pretrained(model_name)<br/># patching them<br/>fix_tokenizer(tokenizer)<br/>model.resize_token_embeddings(len(tokenizer))<br/><br/># fixing the new/moved token embeddings in the model<br/>added_token_id = tokenizer.convert_tokens_to_ids(&#x27;tyv_Cyrl&#x27;)<br/>similar_lang_id = tokenizer.convert_tokens_to_ids(&#x27;kir_Cyrl&#x27;)<br/>embeds = model.model.shared.weight.data<br/># moving the embedding for &quot;mask&quot; to its new position<br/>embeds[added_token_id+1] =embeds[added_token_id]<br/># initializing new language token with a token of a similar language<br/>embeds[added_token_id] = embeds[similar_lang_id]</span></pre><p id="4274" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Now the model and the tokenizer are prepared for processing Tyvan . Of course, the model still needs some training to actually learn it.</p><h2 id="47c3" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Step 5: Training the model</h2><p id="7342" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">One way to organize the training is to follow <a class="af md" href="https://huggingface.co/docs/transformers/tasks/translation" rel="noopener ugc nofollow" target="_blank">the translation tutorial by HF</a>: preprocess the whole dataset at once and feed it to a <code class="cw nr ns nt nu b">Seq2SeqTrainer</code> . However, I prefer a custom training loop (which can be made more robust to out-of-memory errors) and creating training batches on the fly.</p><p id="3cc6" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To save some GPU memory, I use Adafactor optimizer instead of the more popular AdamW. I train the model with a learning rate linearly increasing from zero for the first 1000 steps, and then staying at 0.0001. I set a <code class="cw nr ns nt nu b">weight_decay</code> to prevent the model parameters from becoming too big, and use a <code class="cw nr ns nt nu b">clip_threshold</code> for restricting the norm of the gradient to stabilize the training.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="7da1" class="od mq fr nu b bf oe of l og oh">from transformers.optimization import Adafactor<br/>from transformers import get_constant_schedule_with_warmup<br/>model.cuda();<br/>optimizer = Adafactor(<br/>    [p for p in model.parameters() if p.requires_grad],<br/>    scale_parameter=False,<br/>    relative_step=False,<br/>    lr=1e-4,<br/>    clip_threshold=1.0,<br/>    weight_decay=1e-3,<br/>)<br/>scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)</span></pre><p id="f1bf" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To create each training batch, I randomly choose the translation direction (Tyvan to Russian or reverse), and randomly sample the sentence pairs. For more advanced training, I could also apply some random data augmentation to them (e.g. replacing words or changing the orthography).</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="0c7c" class="od mq fr nu b bf oe of l og oh">import random<br/>LANGS = [(&#x27;ru&#x27;, &#x27;rus_Cyrl&#x27;), (&#x27;tyv&#x27;, &#x27;tyv_Cyrl&#x27;)]<br/><br/>def get_batch_pairs(batch_size, data=df_train):<br/>    (l1, long1), (l2, long2) = random.sample(LANGS, 2)<br/>    xx, yy = [], []<br/>    for _ in range(batch_size):<br/>        item = data.iloc[random.randint(0, len(data)-1)]<br/>        xx.append(preproc(item[l1]))<br/>        yy.append(preproc(item[l2]))<br/>    return xx, yy, long1, long2<br/><br/>print(get_batch_pairs(1))<br/># ([&#x27;чеди&#x27;], [&#x27;семь&#x27;], &#x27;tyv_Cyrl&#x27;, &#x27;rus_Cyrl&#x27;) </span></pre><p id="089b" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Sometimes, training is interrupted because GPU runs out of memory (either because the texts in the batch are too long or because of some memory not being cleaned). To make the training more robust to them, I create a function that tries to release some memory:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="49a8" class="od mq fr nu b bf oe of l og oh">import gc<br/>import torch<br/><br/>def cleanup():<br/>    &quot;&quot;&quot;Try to free GPU memory&quot;&quot;&quot;<br/>    gc.collect()<br/>    torch.cuda.empty_cache()</span></pre><p id="d86e" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">I set some more parameters before training:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="e59d" class="od mq fr nu b bf oe of l og oh">batch_size = 16  # 32 already doesn&#x27;t fit well to 15GB of GPU memory<br/>max_length = 128  # token sequences will be truncated<br/>training_steps = 60000  # Usually, I set a large number of steps,<br/># and then just interrupt the training manually<br/>losses = []  # with this list, I do very simple tracking of average loss<br/>MODEL_SAVE_PATH = &#x27;/gd/MyDrive/models/nllb-rus-tyv-v1&#x27;  # on my Google drive</span></pre><p id="3a77" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Now we are ready for the training loop! To be robust, we save the model every 1000 steps, and after each out-of-memory error, we just ignore it and continue the training. If there are too many OOMs, though, you may want to interrupt the useless training and reduce the <code class="cw nr ns nt nu b">batch_size</code>  or <code class="cw nr ns nt nu b">max_length</code>.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="bbe5" class="od mq fr nu b bf oe of l og oh">model.train()<br/>x, y, loss = None, None, None<br/>cleanup()<br/><br/>tq = trange(len(losses), training_steps)<br/>for i in tq:<br/>    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)<br/>    try:<br/>        tokenizer.src_lang = lang1<br/>        x = tokenizer(xx, return_tensors=&#x27;pt&#x27;, padding=True, truncation=True, max_length=max_length).to(model.device)<br/>        tokenizer.src_lang = lang2<br/>        y = tokenizer(yy, return_tensors=&#x27;pt&#x27;, padding=True, truncation=True, max_length=max_length).to(model.device)<br/>        # -100 is a magic value ignored in the loss function<br/>        # because we don&#x27;t want the model to learn to predict padding ids<br/>        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100<br/><br/>        loss = model(**x, labels=y.input_ids).loss<br/>        loss.backward()<br/>        losses.append(loss.item())<br/><br/>        optimizer.step()<br/>        optimizer.zero_grad(set_to_none=True)<br/>        scheduler.step()<br/><br/>    except RuntimeError as e:  # usually, it is out-of-memory<br/>        optimizer.zero_grad(set_to_none=True)<br/>        x, y, loss = None, None, None<br/>        cleanup()<br/>        print(&#x27;error&#x27;, max(len(s) for s in xx + yy), e)<br/>        continue<br/><br/>    if i % 1000 == 0:<br/>        # each 1000 steps, I report average loss at these steps<br/>        print(i, np.mean(losses[-1000:]))<br/><br/>    if i % 1000 == 0 and i &gt; 0:<br/>        model.save_pretrained(MODEL_SAVE_PATH)<br/>        tokenizer.save_pretrained(MODEL_SAVE_PATH)</span></pre><p id="ea5c" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">An advantage of such a training loop is that you can interrupt it at any moment and adjust something or take a look at how the current version of the model can translate a sample sentence. But of course, you can replace it with something more sophisticated, if you feel like it. </p><p id="b0eb" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">I usually run the training on Google Colab for at most 24 hours, and then Colab shuts it down. If I want to train longer, I restart the notebook, load the model from my last checkpoint, and just continue training it the same way as before. But for teaching NLLB a new language similar to the ones that it already knows, 24 hours is more than enough.</p><h2 id="4bfb" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Step 6: Evaluating and using the model</h2><p id="15bf" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">After the model has been trained for some time, you can test how well it translates. If the Colab instance has shut down, you can always load it back from the Google drive where you have saved it:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="f0e4" class="od mq fr nu b bf oe of l og oh">from transformers import NllbTokenizer, AutoModelForSeq2SeqLM<br/>model_load_name = &#x27;/gd/MyDrive/models/nllb-rus-tyv-v1&#x27;<br/>model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name).cuda()<br/>tokenizer = NllbTokenizer.from_pretrained(model_load_name)<br/>fix_tokenizer(tokenizer)</span></pre><p id="0572" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Here is an example of function that can serve for translation:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="2222" class="od mq fr nu b bf oe of l og oh">def translate(<br/>    text, src_lang=&#x27;rus_Cyrl&#x27;, tgt_lang=&#x27;eng_Latn&#x27;, <br/>    a=32, b=3, max_input_length=1024, num_beams=4, **kwargs<br/>):<br/>    &quot;&quot;&quot;Turn a text or a list of texts into a list of translations&quot;&quot;&quot;<br/>    tokenizer.src_lang = src_lang<br/>    tokenizer.tgt_lang = tgt_lang<br/>    inputs = tokenizer(<br/>        text, return_tensors=&#x27;pt&#x27;, padding=True, truncation=True, <br/>        max_length=max_input_length<br/>    )<br/>    model.eval() # turn off training mode<br/>    result = model.generate(<br/>        **inputs.to(model.device),<br/>        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),<br/>        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),<br/>        num_beams=num_beams, **kwargs<br/>    )<br/>    return tokenizer.batch_decode(result, skip_special_tokens=True)<br/><br/># Example usage:<br/>t = &#x27;мөңгүн үр чыткаш карарар&#x27;<br/>print(translate(t, &#x27;tyv_Cyrl&#x27;, &#x27;rus_Cyrl&#x27;))<br/># [&#x27;серебро от времени чернеет&#x27;]</span></pre><p id="f0f1" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">It has several important parameters:</p><ul class=""><li id="5a36" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc np mi mj bj"><code class="cw nr ns nt nu b">num_beams</code>: increasing this number usually improves the accuracy, but makes the translation slower and increases the memory consumption.</li><li id="4c57" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj"><code class="cw nr ns nt nu b">a</code> and <code class="cw nr ns nt nu b">b</code> control the maximal length of the generated text (in tokens); setting them to smaller values can speed up the translation, but may occasionally lead to undertranslation.</li></ul><p id="9389" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The transformers package has many other parameters that you can modify when translating texts; please read <a class="af md" href="https://huggingface.co/docs/transformers/generation_strategies" rel="noopener ugc nofollow" target="_blank">the generation strategies doc</a> to learn about them.</p><p id="91b7" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">This way, we can generate the translations for our development dataset (both rus-tyv and tyv-rus), and take a look at how accurate they are.</p><figure class="nv nw nx ny nz ol oi oj paragraph-image"><div role="button" tabindex="0" class="om on ee oo bg op"><div class="oi oj pu"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*IZ1JUe9Ieg9DLJl3wb-TyA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" class="bg oq or c" width="700" height="423" loading="lazy" role="presentation"/></picture></div></div><figcaption class="os ot ou oi oj ov ow be b bf z dw">Examples of translations. A screenshot by the author.</figcaption></figure><p id="bd05" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">From the sample, it looks like the machine translations match the references (for <code class="cw nr ns nt nu b">tyv_translated</code>, the reference is <code class="cw nr ns nt nu b">tyv</code>, and for <code class="cw nr ns nt nu b">rus_translated</code>, it is <code class="cw nr ns nt nu b">ru</code>) in about 50% cases. How can we quantify this intuitive quality measure automatically? </p><p id="b6c2" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The two most popular automatic metrics for machine translation quality are BLEU and ChrF++. Both of them compute a percentage similarity between the translation and the reference texts. However, they define the similarity slightly differently; e.g. BLEU reward only full-word matches, while ChrF++ gives positive scores even when only word parts match (so e.g. ChrF++ would treat the translation “течёт холод” to have similarity to the reference “несёт холодом” about 40%, while BLEU would report a zero similarity).</p><p id="d6b8" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">With these metrics, we can assign some numeric values to the quality of our translation model:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="ddc1" class="od mq fr nu b bf oe of l og oh">import sacrebleu<br/>bleu_calc = sacrebleu.BLEU()<br/>chrf_calc = sacrebleu.CHRF(word_order=2)  # this metric is called ChrF++<br/><br/>print(bleu_calc.corpus_score(df_dev[&#x27;rus_translated&#x27;].tolist(), [df_dev[&#x27;ru&#x27;].tolist()]))<br/>print(chrf_calc.corpus_score(df_dev[&#x27;rus_translated&#x27;].tolist(), [df_dev[&#x27;ru&#x27;].tolist()]))<br/>print(bleu_calc.corpus_score(df_dev[&#x27;tyv_translated&#x27;].tolist(), [df_dev[&#x27;tyv&#x27;].tolist()]))<br/>print(chrf_calc.corpus_score(df_dev[&#x27;tyv_translated&#x27;].tolist(), [df_dev[&#x27;tyv&#x27;].tolist()]))<br/><br/># BLEU = 24.14 52.5/30.4/18.9/12.1 (BP = 0.981 ratio = 0.981 hyp_len = 2281 ref_len = 2324)<br/># chrF2++ = 49.49<br/># BLEU = 23.41 52.1/31.0/18.9/11.3 (BP = 0.966 ratio = 0.967 hyp_len = 2292 ref_len = 2371)<br/># chrF2++ = 50.89</span></pre><p id="c315" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">BLEU and ChrF++ are useful for comparing the quality of different models on the same dataset. However, comparing their values for different target languages (or even for the same language, but using different data) is not so meaningful. For example, the table below doesn’t tell in which direction (tyv-rus or rus-tyv) the model is better. But it tells that in all cases, applying beam search is better than not applying it, and that in most cases, Model v2 is better than Model v1.</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="b50d" class="od mq fr nu b bf oe of l og oh">                                  | tyv-&gt;rus | rus-&gt;tyv<br/>Model v1 (no vocabulary update):  |<br/>    no beam search                |   23.21  |  22.03<br/>    num_beams = 4                 |   24.14  |  23.41<br/>Model v2 (extended vocabulary):   |<br/>    no beam search                |   24.08  |  22.50<br/>    num_beams = 4                 |   25.18  |  23.22</span></pre><p id="82cf" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">But wait, what on Earth is Model v2? Actually, it is a version of Model v1 for which I did go over the optional Step 3 of expanding the vocabulary. As you can see, this step provides a small but sustainable gain in quality, at the cost of additional complexity and increasing the model size.</p><p id="5e5a" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj"><strong class="lh fs">A tip</strong>: If you need to translate a long text, you should always split it into individual sentences and process them one by one (for many European languages, you can use the<a class="af md" href="https://github.com/mediacloud/sentence-splitter" rel="noopener ugc nofollow" target="_blank"> sentence-splitter</a> Python package; for other languages, you can look at the <a class="af md" href="https://github.com/facebookresearch/stopes/blob/main/stopes/pipelines/monolingual/utils/sentence_split.py" rel="noopener ugc nofollow" target="_blank">text preprocessing code</a> by the NLLB team).</p><p id="e7a5" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj"><strong class="lh fs">Another tip</strong>: If you want to translate multiple sentences using a GPU, it would be faster if you group them into batches which are translated in parallel.  Translating a batch takes as long as translating the longest sentence in it, so it would make sense to group batches by length:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="373f" class="od mq fr nu b bf oe of l og oh">def batched_translate(texts, batch_size=16, **kwargs):<br/>    &quot;&quot;&quot;Translate texts in batches of similar length&quot;&quot;&quot;<br/>    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))<br/>    results = []<br/>    for i in trange(0, len(texts2), batch_size):<br/>        results.extend(translate(texts2[i: i+batch_size], **kwargs))<br/>    return [p for i, p in sorted(zip(idxs, results))]</span></pre><h2 id="7723" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Step 7: Publishing</h2><p id="8506" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">We will push the model and the tokenizer to a repository on the HuggingFace space, so that the other users could easily find and download them. This is very simple, as long as you have a HF account.</p><p id="9941" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To connect to this account from a Colab notebook, you can type:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="fc0e" class="od mq fr nu b bf oe of l og oh">!huggingface-cli login</span></pre><p id="1bd2" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">You will be prompted to go to <a class="af md" href="https://huggingface.co/settings/tokens" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/settings/tokens</a> and copy-paste an authorization token from there. After that, you can execute the following code to create a repo for your model:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="918f" class="od mq fr nu b bf oe of l og oh">upload_repo = &quot;slone/nllb-rus-tyv-v1&quot;<br/>tokenizer.push_to_hub(upload_repo)<br/>model.push_to_hub(upload_repo)</span></pre><p id="3a9e" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">In a few minutes, the model and the tokenizer will be uploaded to a new repo like this one: <a class="af md" href="https://huggingface.co/slone/nllb-rus-tyv-v1" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/slone/nllb-rus-tyv-v1</a> (instead of <code class="cw nr ns nt nu b">slone</code>, your should use the name of your HF personal or organization account). By the way, the repo for the v2 model is <a class="af md" href="https://huggingface.co/slone/nllb-rus-tyv-v2-extvoc" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/slone/nllb-rus-tyv-v2-extvoc</a>.</p><p id="9add" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">As the last step, you will need to press the “Create model card” button and type some useful information about your model (please read <a class="af md" href="https://huggingface.co/docs/transformers/model_sharing#add-a-model-card" rel="noopener ugc nofollow" target="_blank">here</a> about it). In particular, indicate the supported languages and choose the <a class="af md" href="https://creativecommons.org/licenses/by-nc/4.0/" rel="noopener ugc nofollow" target="_blank"><em class="nq">cc-by-nc-4.0</em></a> license; this is the license with which the NLLB models were distributed, so all the derivative models must inherit it.</p><p id="7162" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">In addition to just publishing the model and writing its description, you can host an interactive demo application on the HF platform. Thanks to the magic of packages such as Gradio or Streamlit, this requres not much more that just writing the function that translates a text.</p><p id="3bcc" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">You can copy such a demo from <a class="af md" href="https://huggingface.co/spaces/slone/nllb-rus-tyv-v1-demo" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/spaces/slone/nllb-rus-tyv-v1-demo</a>, and adapt the path to the model, supported languages, and the description text in its <code class="cw nr ns nt nu b">app.py</code> file.</p><h2 id="0799" class="mp mq fr be mr ms mt mu mv mw mx my mz lq na nb nc lu nd ne nf ly ng nh ni nj bj">Step 8: (Optional) Serving the model with Docker</h2><p id="e340" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">Having a demo hosted for free at the HuggingFace website is good. But what if you want to use it elsewhere? The model is rather big (about 2.5GB), and it requires a lot of compute, so the only realistic option is to run it as a micro-service on some powerful server. </p><p id="c98d" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">Such services are often run with <a class="af md" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank">Docker</a>. I do the same, and I create a very simple web API using the <a class="af md" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank">FastAPI </a>package. Here is the repository with the minimal working code: <a class="af md" href="https://github.com/slone-nlp/nllb-docker-demo" rel="noopener ugc nofollow" target="_blank">https://github.com/slone-nlp/nllb-docker-demo</a>.</p><p id="01a7" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">This code consists of a Dockerfile and two Python files. The file <code class="cw nr ns nt nu b">translation.py</code> defines a <code class="cw nr ns nt nu b">Translator</code> class, which is a wrapper that encapsulates <code class="cw nr ns nt nu b">model</code> , <code class="cw nr ns nt nu b">tokenizer</code>, and the <code class="cw nr ns nt nu b">translate</code> function; everything that we have already seen. The file <code class="cw nr ns nt nu b">main.py</code> defines a web API, implemented with FastAPI. It defines two endpoints: <code class="cw nr ns nt nu b">/translate</code>, for running the translation, and <code class="cw nr ns nt nu b">/list-languages</code>, for telling the potential frontend what languages are available: </p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="5e06" class="od mq fr nu b bf oe of l og oh">from fastapi import FastAPI<br/>from pydantic import BaseModel<br/>from translation import Translator<br/><br/><br/>class TranslationRequest(BaseModel):<br/>    text: str<br/>    src_lang: str = &#x27;rus_Cyrl&#x27;<br/>    tgt_lang = &#x27;tyv_Cyrl&#x27;<br/><br/><br/>app = FastAPI()<br/>translator = Translator()<br/><br/><br/>@app.post(&quot;/translate&quot;)<br/>def translate(request: TranslationRequest):<br/>    &quot;&quot;&quot;<br/>    Perform translation with a fine-tuned NLLB model.<br/>    The language codes are supposed to be in 8-letter format, like &quot;eng_Latn&quot;.<br/>    Their list can be returned by /list-languages.<br/>    &quot;&quot;&quot;<br/>    output = translator.translate(request.text, src_lang=request.src_lang, tgt_lang=request.tgt_lang)<br/>    return {&quot;translation&quot;: output}<br/><br/><br/>@app.get(&quot;/list-languages&quot;)<br/>def list_languages():<br/>    &quot;&quot;&quot;Show the mapping of supported languages: from their English names to their 8-letter codes.&quot;&quot;&quot;<br/>    return translator.languages</span></pre><p id="555d" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">To run the application, you need to execute two commands in the command line (assuming that you have already started Docker), from the <code class="cw nr ns nt nu b">nllb-docker-demo</code> directory:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="fae7" class="od mq fr nu b bf oe of l og oh">docker build -t nllb .<br/>docker run  -it -p 7860:7860 nllb</span></pre><p id="9108" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The first command will take a few minutes to build the image, but once it is built, the second command runs quite fast. Upon completion, it will run a web server that supports the endpoints defined above. As a bonus, FastAPI creates an endpoint with automatic documentation: <a class="af md" href="http://localhost:7860/docs" rel="noopener ugc nofollow" target="_blank">http://localhost:7860/docs</a>. It will show you the signatures of the endpoints, and will let you test both of them just from the browser.</p><p id="5b06" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">As a direct test of the API, you can open <a class="af md" href="http://localhost:7860/list-languages" rel="noopener ugc nofollow" target="_blank">http://localhost:7860/list-languages</a> in your browser, and see the returned list of languages as a JSON object: <code class="cw nr ns nt nu b">{&quot;Russian&quot;: &quot;rus_Cyrl&quot;, &quot;Tyvan&quot;: &quot;tyv_Cyrl&quot;}</code>.</p><p id="f2fd" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The <code class="cw nr ns nt nu b">/translate</code> endpoint supports only POST requests, so you can’t easily test it with a browser. But you can test it e.g. with the <a class="af md" href="https://curl.se/" rel="noopener ugc nofollow" target="_blank">curl</a> tool:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="370c" class="od mq fr nu b bf oe of l og oh">curl -X &#x27;POST&#x27; \<br/>  &#x27;http://localhost:7860/translate&#x27; \<br/>  -H &#x27;accept: application/json&#x27; \<br/>  -H &#x27;Content-Type: application/json&#x27; \<br/>  -d &#x27;{&quot;text&quot;: &quot;Нет войне!&quot;, &quot;src_lang&quot;: &quot;rus_Cyrl&quot;, &quot;tgt_lang&quot;: &quot;tyv_Cyrl&quot;}&#x27;</span></pre><p id="b93e" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">(if you use Windows, you’ll have to write the command without <code class="cw nr ns nt nu b">\</code>, in a single line, with <code class="cw nr ns nt nu b">&#x27;</code> replaced by <code class="cw nr ns nt nu b">&quot;</code> and <code class="cw nr ns nt nu b">&quot;</code> in the JSON body replaced by <code class="cw nr ns nt nu b">\&quot;</code>)</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="bbd6" class="od mq fr nu b bf oe of l og oh">curl -X &quot;POST&quot; http://localhost:7860/translate -H &quot;Content-Type: application/json&quot; -d &quot;{\&quot;text\&quot;: \&quot;Нет войне!\&quot;, \&quot;src_lang\&quot;: \&quot;rus_Cyrl\&quot;, \&quot;tgt_lang\&quot;: \&quot;tyv_Cyrl\&quot;}&quot;</span></pre><p id="6d85" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">The command will call the translation service and return a JSON response:</p><pre class="nv nw nx ny nz oa nu ob bo oc ba bj"><span id="219c" class="od mq fr nu b bf oe of l og oh">{&quot;translation&quot;:&quot;Дайын-чаа чок!&quot;}</span></pre><p id="63f0" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">You can deploy this backend application on some virtual server (e.g. AWS or DigitalOcean), and make your frontend app (e.g. a mobile application or a web frontend written in JS) talk to the backend with this API.</p><h1 id="c718" class="ox mq fr be mr oy oz pa mv pb pc pd mz pe pf pg ph pi pj pk pl pm pn po pp pq bj">What next?</h1><p id="4d33" class="pw-post-body-paragraph lf lg fr lh b li nk lk ll lm nl lo lp lq nm ls lt lu nn lw lx ly no ma mb mc fk bj">In this tutorial, I have shown how to examine a training dataset for machine translation, how to update the NLLB tokenizer with new tokens and a new language code, how to fine-tune a NLLB model, and how to publish and serve it. </p><p id="aaac" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">This should be enough for creating a proof-of-concept machine translation service for a new language. However, there are some problems that I didn’t cover here (but may address in some future tutorials):</p><ul class=""><li id="e6c9" class="lf lg fr lh b li lj lk ll lm ln lo lp lq me ls lt lu mf lw lx ly mg ma mb mc np mi mj bj">How to collect a parallel dataset for a new language from existing sources (some tips are in my <a class="af md" href="https://aclanthology.org/2022.fieldmatters-1.6/" rel="noopener ugc nofollow" target="_blank">paper about machine translation for Erzya</a>)</li><li id="dc97" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">How to organize manual translation to create even more parallel texts</li><li id="a365" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">How to clean and filter the existing parallel data (some tips are in my <a class="af md" href="https://habr.com/ru/articles/744972/" rel="noopener ugc nofollow" target="_blank">post about Bashkir corpus cleaning</a>, in Russian)</li><li id="4d9e" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">How to prevent the model from forgetting other languages (the one in this tutorial has learned to translate only between Russian and Tyvan, but forgot how to translate into English)</li><li id="4a20" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">How to decrease the model size (e.g. quantization and distillation), to make its deployment more affordable and faster</li><li id="d97a" class="lf lg fr lh b li mk lk ll lm ml lo lp lq mm ls lt lu mn lw lx ly mo ma mb mc np mi mj bj">How to create an advanced machine translation system, robust both in terms of high load and translation quality.</li></ul><p id="4e11" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">If you want to contact me on any of these issues (or on some other), don’t hesitate to leave your comments here, or to write directly to my Telegram (@cointegrated). You can also subscribe to my channel about NLP (in Russian): <a class="af md" href="https://t.me/izolenta_mebiusa" rel="noopener ugc nofollow" target="_blank">https://t.me/izolenta_mebiusa</a>.</p><p id="9ce5" class="pw-post-body-paragraph lf lg fr lh b li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc fk bj">And one last appeal: if you have data or models for lower-resourced languages, please publish them! Meta did a wonderful job by publishing the NLLB-200 models, but only as a community, we can truly make no language left behind, even if we address them just one at a time.</p></div></div></div></div></section></div></div></article><div class="ab ca"><div class="ch bg ew ex ey ez"></div></div></div><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="pv pw ab hy"><div class="px ab"><a class="py ax am ao" href="https://medium.com/tag/machine-translation?source=post_page-----a37fc706b865---------------machine_translation-----------------" rel="noopener follow"><div class="pz ee cw qa fb qb qc be b bf z bj qd">Machine Translation</div></a></div><div class="px ab"><a class="py ax am ao" href="https://medium.com/tag/ai?source=post_page-----a37fc706b865---------------ai-----------------" rel="noopener follow"><div class="pz ee cw qa fb qb qc be b bf z bj qd">AI</div></a></div><div class="px ab"><a class="py ax am ao" href="https://medium.com/tag/nlp?source=post_page-----a37fc706b865---------------nlp-----------------" rel="noopener follow"><div class="pz ee cw qa fb qb qc be b bf z bj qd">NLP</div></a></div><div class="px ab"><a class="py ax am ao" href="https://medium.com/tag/nllb?source=post_page-----a37fc706b865---------------nllb-----------------" rel="noopener follow"><div class="pz ee cw qa fb qb qc be b bf z bj qd">Nllb</div></a></div><div class="px ab"><a class="py ax am ao" href="https://medium.com/tag/low-resource-language?source=post_page-----a37fc706b865---------------low_resource_language-----------------" rel="noopener follow"><div class="pz ee cw qa fb qb qc be b bf z bj qd">Low Resource Language</div></a></div></div></div></div><div class="l"></div><footer class="qe qf qg qh qi qj qk ql qm ab q qn qo c"><div class="l ae"><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="ab co qp"><div class="ab q jh"><div class="qq l"><span class="l qr qs qt e d"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div></span><span class="l h g f qu qv"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div></span></div><div class="bp ab"><div><div class="bl" aria-hidden="false"><button class="kd jm ke kf ab q ef" aria-label="responses" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" class="kd ef jr"><path fill-rule="evenodd" clip-rule="evenodd" d="M18.47 20.27a6.08 6.08 0 0 1-4.06-1.55c-.74.2-1.51.3-2.29.3-4.48 0-8.12-3.35-8.12-7.48 0-4.15 3.64-7.5 8.12-7.5 4.48 0 8.12 3.35 8.12 7.48 0 1.98-.81 3.83-2.3 5.23.02.17.05.34.1.53.2.66.52 1.33 1 1.96a.66.66 0 0 1-.53 1.04h-.04z"></path></svg></button></div></div></div></div><div class="ab q"><div class="qw l hv"><div><div class="bl" aria-hidden="false"><div class="kh ki kj kd kk kl" data-testid="footerBookmarkButton"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="km" aria-label="Add to list bookmark button" disabled=""><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></div></div></div></div><div class="qw l hv"><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg></button></div></div></div></div><div class="bl" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="More options" data-testid="footerStoryOptionsButton" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></footer><div class="qx qy qz ra rb l bw"><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="ck ab rc co"><div class="ab hc"><a rel="noopener follow" href="/?source=post_page-----a37fc706b865--------------------------------"><div class="l rd re bx rf hg"><div class="l ee"><img alt="David Dale" class="l eq bx rg rh cw" src="https://miro.medium.com/v2/resize:fill:144:144/0*Lnxo9LylIaenwvDF." width="72" height="72" loading="lazy"/><div class="hh bx l rg rh eo n hi ep"></div></div></div></a></div><div class="j i d"><div class="ab"><button class="be b bf z ri pz rj rk rl rm rn ro rp rq rr hs rs rt ru rv rw rx eq bl ry ot">Follow</button><div class="dv l"><div><div><div class="bl" aria-hidden="false"><div class="l"><button class="be b bf z ri am rj rk rl rm rn ro rp rq rr hs rs rt ru rw rx eq bl ry ot" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="rz sa sb"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></div></div></div></div></div></div></div></div><div class="ab cm co"><div class="l"><div class="ab q"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab q" rel="noopener follow" href="/?source=post_page-----a37fc706b865--------------------------------"><h2 class="pw-author-name be sc sd se sf bj"><span class="fk">Written by <!-- -->David Dale</span></h2></a></div><div class="px ab"><div class="l hv"><span class="pw-follower-count be b bf z bj"><a class="af ag ah ai aj ak al am an ao ap aq ar hn" rel="noopener follow" href="/followers?source=post_page-----a37fc706b865--------------------------------">125 Followers</a></span></div></div><div class="sg l"><p class="be b bf z bj">NLP researcher at Meta FAIR, low-resource language enthusiast. See <a class="af ag ah ai aj ak al am an ao ap aq ar md fl" href="http://daviddale.ru" rel="noopener  ugc nofollow">daviddale.ru</a>.</p></div></div><div class="h k"><div class="ab"><button class="be b bf z ri pz rj rk rl rm rn ro rp rq rr hs rs rt ru rv rw rx eq bl ry ot">Follow</button><div class="dv l"><div><div><div class="bl" aria-hidden="false"><div class="l"><button class="be b bf z ri am rj rk rl rm rn ro rp rq rr hs rs rt ru rw rx eq bl ry ot" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="rz sa sb"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></div></div></div></div></div></div></div></div><div class="sh bg si sj sk sl sm sn"></div></div></div><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="sy sz l"><h2 class="be sc hl z fq bj">More from <!-- -->David Dale</h2></div><div class="ta ab jh hy tb tc td te tf tg th ti tj tk tl tm tn to tp"><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="How to adapt a multilingual T5 model for a single language" rel="noopener follow" href="/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90?source=author_recirc-----a37fc706b865----0---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="vf vg vh vi vj"><img alt="How to adapt a multilingual T5 model for a single language" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*59IfFHhcDXtd0hsc1IfTUw.png" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----0---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="l ee"><img alt="David Dale" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/0*Lnxo9LylIaenwvDF." width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----0---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><p class="be b dx z vw vx vy vz wa wb wc wd bj">David Dale</p></a></div></div></div><div class="vv l"><p class="be b dx z dw">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://towardsdatascience.com/?source=author_recirc-----a37fc706b865----0---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Towards Data Science</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90?source=author_recirc-----a37fc706b865----0---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">How to adapt a multilingual T5 model for a single language</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">Load embeddings only for the tokens from your language to reduce model size</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90?source=author_recirc-----a37fc706b865----0---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><span class="be b dx z dw"><div class="ab q"><span>4 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>May 4, 2021</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" rel="noopener follow" href="/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----a37fc706b865----0---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">6</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div><div class="j i d"><div class="sh bg si xz"></div></div></div></div></div></div></div></article></div></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Compressing unsupervised fastText models" rel="noopener follow" href="/compressing-unsupervised-fasttext-models-eb212e9919ca?source=author_recirc-----a37fc706b865----1---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="vf vg vh vi vj"><img alt="Compressing unsupervised fastText models" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*haP6Y29QGU9lin5d8QMAgw.png" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----1---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="l ee"><img alt="David Dale" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/0*Lnxo9LylIaenwvDF." width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----1---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><p class="be b dx z vw vx vy vz wa wb wc wd bj">David Dale</p></a></div></div></div><div class="vv l"><p class="be b dx z dw">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://towardsdatascience.com/?source=author_recirc-----a37fc706b865----1---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Towards Data Science</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/compressing-unsupervised-fasttext-models-eb212e9919ca?source=author_recirc-----a37fc706b865----1---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">Compressing unsupervised fastText models</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">A python package to reduce word embeddings models by 300 times, with almost the same performance on downstream NLP tasks.</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/compressing-unsupervised-fasttext-models-eb212e9919ca?source=author_recirc-----a37fc706b865----1---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><span class="be b dx z dw"><div class="ab q"><span>6 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>Dec 14, 2021</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" rel="noopener follow" href="/compressing-unsupervised-fasttext-models-eb212e9919ca?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----a37fc706b865----1---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">1</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div><div class="j i d"><div class="sh bg si xz"></div></div></div></div></div></div></div></article></div></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="A machine learning model to understand fancy abbreviations, trained on Tolkien" rel="noopener follow" href="/a-machine-learning-model-to-understand-fancy-abbreviations-trained-on-tolkien-36601b73ecbb?source=author_recirc-----a37fc706b865----2---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="vf vg vh vi vj"><img alt="A machine learning model to understand fancy abbreviations, trained on Tolkien" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*QUNXxPDm5b7CvExfhz9tkg.png" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----2---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="l ee"><img alt="David Dale" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/0*Lnxo9LylIaenwvDF." width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----2---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><p class="be b dx z vw vx vy vz wa wb wc wd bj">David Dale</p></a></div></div></div><div class="vv l"><p class="be b dx z dw">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://medium.com/swlh?source=author_recirc-----a37fc706b865----2---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">The Startup</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/a-machine-learning-model-to-understand-fancy-abbreviations-trained-on-tolkien-36601b73ecbb?source=author_recirc-----a37fc706b865----2---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div title="A machine learning model to understand fancy abbreviations, trained on Tolkien"><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">A machine learning model to understand fancy abbreviations, trained on Tolkien</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">Recently I bumped into a question on Stackoverflow, how to recover phrases from abbreviations, e.g. turn “wtrbtl” into “water bottle”, and…</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/a-machine-learning-model-to-understand-fancy-abbreviations-trained-on-tolkien-36601b73ecbb?source=author_recirc-----a37fc706b865----2---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><span class="be b dx z dw"><div class="ab q"><span>9 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>Jan 14, 2018</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" rel="noopener follow" href="/a-machine-learning-model-to-understand-fancy-abbreviations-trained-on-tolkien-36601b73ecbb?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----a37fc706b865----2---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">5</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div><div class="j i d"><div class="sh bg si xz"></div></div></div></div></div></div></div></article></div></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Do you have to try to love math?" rel="noopener follow" href="/do-you-have-to-try-to-love-math-8d47f2103da6?source=author_recirc-----a37fc706b865----3---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="vf vg vh vi vj"><img alt="Do you have to try to love math?" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*70jSJbcdhNWfKK5UVRDtWQ.png" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----3---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div class="l ee"><img alt="David Dale" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/0*Lnxo9LylIaenwvDF." width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" rel="noopener follow" href="/?source=author_recirc-----a37fc706b865----3---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><p class="be b dx z vw vx vy vz wa wb wc wd bj">David Dale</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/do-you-have-to-try-to-love-math-8d47f2103da6?source=author_recirc-----a37fc706b865----3---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">Do you have to try to love math?</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">If you don’t like and don’t understand math, does it mean you are stupid?
Do you need to love math to achieve at…</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="/do-you-have-to-try-to-love-math-8d47f2103da6?source=author_recirc-----a37fc706b865----3---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><span class="be b dx z dw"><div class="ab q"><span>6 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>Feb 13, 2018</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" rel="noopener follow" href="/do-you-have-to-try-to-love-math-8d47f2103da6?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=author_recirc-----a37fc706b865----3---------------------83dd8699_d79b_45b4_be3c_6a210d841367-------"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div></div><div class="sh bg si dj dk ya yb yc"></div><div class="ab hw hx yd ye yf"><a class="be b bf z bj pz yg yh yi yj xt yk yl rr hs ym yn yo ru yp yq yr ys yt rw rx eq bl ry ot" rel="noopener follow" href="/?source=post_page-----a37fc706b865--------------------------------"><div class="l ot">See all from <!-- -->David Dale</div></a></div></div></div><div class="sh bg si yu yv yw yx yy"></div><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="yz za l"><h2 class="be sc oy pa mv pb pd mz pe pg ph pi pk pl pm po pp bj">Recommended from Medium</h2><div class="nv nw nx ny nz l"><div class="ta ab jh hy tb tc td te tf tg th ti tj tk tl tm tn to tp"><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="10 Seconds That Ended My 20 Year Marriage" href="https://medium.com/@UnbecomingStories/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="vf vg vh vi vj"><img alt="10 Seconds That Ended My 20 Year Marriage" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*2PEPQ0LxKFELp2lojVF-lw.jpeg" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@UnbecomingStories?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="l ee"><img alt="Unbecoming" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*_FyD8-JlhRRmfaZnEgCrkA.jpeg" width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://medium.com/@UnbecomingStories?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Unbecoming</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@UnbecomingStories/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">10 Seconds That Ended My 20 Year Marriage</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">It’s August in Northern Virginia, hot and humid. I still haven’t showered from my morning trail run. I’m wearing my stay-at-home mom…</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@UnbecomingStories/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><span class="be b dx z dw"><div class="ab q"><div class="qm ab"><div class="bl" aria-hidden="false"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false"><svg width="16" height="16" viewBox="0 0 20 20" fill="none"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>4 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>Feb 16, 2022</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" href="https://medium.com/@UnbecomingStories/10-seconds-that-ended-my-20-year-marriage-a6f367f02e53?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">973</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div><div class="j i d"><div class="sh bg si xz"></div></div></div></div></div></div></div></article></div></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="A personal, non-partisan perspective on the Israel-Hamas war" href="https://medium.com/@isaac_1884/the-attacks-on-israel-and-the-response-15ed50e63da6?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="vf vg vh vi vj"><img alt="A personal, non-partisan perspective on the Israel-Hamas war" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*CX3RDZAwMVtNku1-ONB13w.jpeg" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@isaac_1884?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="l ee"><img alt="Isaac Saul" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*5bMy4uvQlQxjhi_xhBlJJQ.jpeg" width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://medium.com/@isaac_1884?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Isaac Saul</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@isaac_1884/the-attacks-on-israel-and-the-response-15ed50e63da6?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">A personal, non-partisan perspective on the Israel-Hamas war</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">To understand this war, we must understand the thousand-year history that led us here</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@isaac_1884/the-attacks-on-israel-and-the-response-15ed50e63da6?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><span class="be b dx z dw"><div class="ab q"><span>11 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span>4 days ago</div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" href="https://medium.com/@isaac_1884/the-attacks-on-israel-and-the-response-15ed50e63da6?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">316</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div></div></div><div class="sh bg si zb"></div><h2 class="be sc hl z fq bj">Lists</h2><div class="xz l"><div class="cm ab jh hy tb tc td te tf tg th ti tj tk tl tm tn to tp"><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@MediumStaff/list/the-new-chatbots-chatgpt-bard-and-beyond-5969c7449b7f?source=read_next_recirc-----a37fc706b865--------------------------------" rel="noopener follow"><div class="zi zj vw ab hv ee"><div class="ee vk zd bw ze"><div class="vk he vw l"><img alt="Image by vectorjuice on FreePik" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*3OsUtsnlTx9Svm4c.jpg" width="48" height="48" loading="lazy"/></div></div><div class="ee vk zd bw zf zg"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*IPZF1hcDWwpPqOz2vL7NxQ.png" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="ee vk bw qo zh"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*0fHUKyg3xtpNWpop35PR4g.png" width="48" height="48" loading="lazy" role="presentation"/></div></div></div><div class="aw l"><h2 class="be sc hl z vw wz vy vz xa wb wd fq bj">The New Chatbots: ChatGPT, Bard, and Beyond</h2><div class="be b dx z dw ab zc">13 stories<span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span>145<!-- --> <!-- -->saves</div></div></a></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----a37fc706b865--------------------------------" rel="noopener follow"><div class="zi zj vw ab hv ee"><div class="ee vk zd bw ze"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*i2EDU0pBgMeqcg2J" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="ee vk zd bw zf zg"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*pt3GvRsCd12vtlolavMSZg.png" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="ee vk bw qo zh"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*0xySt8rSH3BQ2yIwvyGMqA.png" width="48" height="48" loading="lazy" role="presentation"/></div></div></div><div class="aw l"><h2 class="be sc hl z vw wz vy vz xa wb wd fq bj">Natural Language Processing</h2><div class="be b dx z dw ab zc">709 stories<span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span>317<!-- --> <!-- -->saves</div></div></a></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://tomsmith585.medium.com/list/generative-ai-recommended-reading-508b0743c247?source=read_next_recirc-----a37fc706b865--------------------------------" rel="noopener follow"><div class="zi zj vw ab hv ee"><div class="ee vk zd bw ze"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*M8Jq6btD0YsgaRM1" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="ee vk zd bw zf zg"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*rsp22rKwFDjiwwCcUly56Q.jpeg" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="ee vk bw qo zh"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*PNVLDmurJ5LoCjB9Ovdnpw.png" width="48" height="48" loading="lazy" role="presentation"/></div></div></div><div class="aw l"><h2 class="be sc hl z vw wz vy vz xa wb wd fq bj">Generative AI Recommended Reading</h2><div class="be b dx z dw ab zc">52 stories<span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span>309<!-- --> <!-- -->saves</div></div></a></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@MediumForTeams/list/what-is-chatgpt-7a5756752f49?source=read_next_recirc-----a37fc706b865--------------------------------" rel="noopener follow"><div class="zi zj vw ab hv ee"><div class="ee vk zd bw ze"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*_eYHSSUS0abUxmDU" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="ee vk zd bw zf zg"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/1*wXgeNtz5OJ5O9T3c3mQRRw.png" width="48" height="48" loading="lazy" role="presentation"/></div></div><div class="ee vk bw qo zh"><div class="vk he vw l"><img alt="" class="" src="https://miro.medium.com/v2/resize:fill:96:96/0*tIipcmrInD5UMpQI.png" width="48" height="48" loading="lazy" role="presentation"/></div></div></div><div class="aw l"><h2 class="be sc hl z vw wz vy vz xa wb wd fq bj">What is ChatGPT?</h2><div class="be b dx z dw ab zc">9 stories<span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span>193<!-- --> <!-- -->saves</div></div></a></div></div></div><div class="sh bg si zk dj zl dk zm zn zo zp zq zr"></div><div class="ta ab jh hy tb tc td te tf tg th ti tj tk tl tm tn to tp"><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="The ChatGPT Hype Is Over — Now Watch How Google Will Kill ChatGPT." href="https://entreprenal.com/the-chatgpt-hype-is-over-now-watch-how-google-will-kill-chatgpt-426d5e3f7d05?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="vf vg vh vi vj"><img alt="The ChatGPT Hype Is Over — Now Watch How Google Will Kill ChatGPT." class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*3FtLa-nHJB8KOb-Wu9bqbg.png" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://entreprenal.com/?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="l ee"><img alt="AL Anany" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*VreqR95b_bWD8Oxp8M6Rtg@2x.jpeg" width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://entreprenal.com/?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">AL Anany</p><div class="zs zt l"><div class="ab zu"><div class="ab"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M15.16 8c0 .65-.46 1.14-.86 1.57-.23.25-.47.5-.56.72-.1.22-.09.55-.1.88 0 .6-.01 1.3-.48 1.78-.48.48-1.16.5-1.75.5-.32 0-.65.01-.86.1-.2.07-.46.33-.7.57-.42.41-.9.88-1.54.88s-1.12-.47-1.54-.88a2.87 2.87 0 0 0-.7-.58c-.22-.09-.54-.08-.87-.09-.59 0-1.27-.02-1.74-.5s-.48-1.17-.49-1.78c0-.33-.01-.67-.1-.88-.07-.2-.32-.47-.55-.71-.4-.44-.87-.93-.87-1.58s.46-1.14.87-1.58c.23-.24.47-.5.56-.71.09-.22.08-.55.09-.88 0-.6.02-1.3.49-1.78s1.15-.5 1.74-.5c.33 0 .66-.01.86-.1.2-.08.47-.33.7-.57.43-.41.91-.88 1.55-.88.63 0 1.12.47 1.54.88.24.24.49.48.7.58.22.09.54.08.86.09.6 0 1.27.02 1.75.5.47.48.48 1.17.49 1.78 0 .33 0 .67.09.88.08.2.33.47.56.71.4.44.86.93.86 1.58z" fill="#437AFF"></path><path d="M7.33 10.5c.2 0 .38.08.52.22.13.14.21.33.21.53 0 .07.03.13.07.18a.24.24 0 0 0 .35 0 .25.25 0 0 0 .07-.18c0-.2.08-.39.22-.53a.73.73 0 0 1 .52-.22h1.96c.13 0 .25-.05.34-.15a.5.5 0 0 0 .15-.35V6a.5.5 0 0 0-.15-.35.48.48 0 0 0-.34-.15H9.78c-.33 0-.64.13-.87.37-.23.23-.36.55-.36.88v2.5c0 .07-.02.13-.07.18a.24.24 0 0 1-.35 0 .25.25 0 0 1-.07-.18v-2.5c0-.33-.13-.65-.36-.88a1.21 1.21 0 0 0-.86-.37H5.37a.48.48 0 0 0-.35.15.5.5 0 0 0-.14.35v4c0 .13.05.26.14.35.1.1.22.15.35.15h1.96z" fill="#fff"></path></svg></div></div></div></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://entreprenal.com/the-chatgpt-hype-is-over-now-watch-how-google-will-kill-chatgpt-426d5e3f7d05?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">The ChatGPT Hype Is Over — Now Watch How Google Will Kill ChatGPT.</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">It never happens instantly. The business game is longer than you know.</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://entreprenal.com/the-chatgpt-hype-is-over-now-watch-how-google-will-kill-chatgpt-426d5e3f7d05?source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><span class="be b dx z dw"><div class="ab q"><div class="qm ab"><div class="bl" aria-hidden="false"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false"><svg width="16" height="16" viewBox="0 0 20 20" fill="none"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>6 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>Sep 1</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" href="https://entreprenal.com/the-chatgpt-hype-is-over-now-watch-how-google-will-kill-chatgpt-426d5e3f7d05?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----a37fc706b865----0---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">428</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div><div class="j i d"><div class="sh bg si xz"></div></div></div></div></div></div></div></article></div></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Why Japanese Websites Look So Different" href="https://medium.com/@mirijam.missbichler/why-japanese-websites-look-so-different-2c7273e8be1e?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="vf vg vh vi vj"><img alt="Why Japanese Websites Look So Different" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*D-TiKrBADjkMrnHjBAQ4bQ.png" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mirijam.missbichler?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="l ee"><img alt="Mirijam Missbichler" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*HRgar2NE2lXMHGvV7wH0QA.jpeg" width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://medium.com/@mirijam.missbichler?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Mirijam Missbichler</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mirijam.missbichler/why-japanese-websites-look-so-different-2c7273e8be1e?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">Why Japanese Websites Look So Different</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">&amp; how to analyze design choices without jumping to conclusions</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@mirijam.missbichler/why-japanese-websites-look-so-different-2c7273e8be1e?source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><span class="be b dx z dw"><div class="ab q"><span>8 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>May 1</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" href="https://medium.com/@mirijam.missbichler/why-japanese-websites-look-so-different-2c7273e8be1e?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----a37fc706b865----1---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">218</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div><div class="j i d"><div class="sh bg si xz"></div></div></div></div></div></div></div></article></div></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Bye Bye, Spotify" href="https://medium.com/@73srabt/bye-bye-spotify-37bb823839d2?source=read_next_recirc-----a37fc706b865----2---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="vf vg vh vi vj"><img alt="Bye Bye, Spotify" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/0*CIFEv8QIEjuBWqTB" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@73srabt?source=read_next_recirc-----a37fc706b865----2---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="l ee"><img alt="Scott-Ryan Abt" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*4MpeAtM5-fGCa3vOPM6XXg.jpeg" width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://medium.com/@73srabt?source=read_next_recirc-----a37fc706b865----2---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Scott-Ryan Abt</p></a></div></div></div><div class="vv l"><p class="be b dx z dw">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://medium.com/pitfall?source=read_next_recirc-----a37fc706b865----2---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Pitfall</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@73srabt/bye-bye-spotify-37bb823839d2?source=read_next_recirc-----a37fc706b865----2---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">Bye Bye, Spotify</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">And see ya later, all you subscription services in my little empire</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@73srabt/bye-bye-spotify-37bb823839d2?source=read_next_recirc-----a37fc706b865----2---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><span class="be b dx z dw"><div class="ab q"><div class="qm ab"><div class="bl" aria-hidden="false"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false"><svg width="16" height="16" viewBox="0 0 20 20" fill="none"><path d="M12.4 12.77l-1.81 4.99a.63.63 0 0 1-1.18 0l-1.8-4.99a.63.63 0 0 0-.38-.37l-4.99-1.81a.62.62 0 0 1 0-1.18l4.99-1.8a.63.63 0 0 0 .37-.38l1.81-4.99a.63.63 0 0 1 1.18 0l1.8 4.99a.63.63 0 0 0 .38.37l4.99 1.81a.63.63 0 0 1 0 1.18l-4.99 1.8a.63.63 0 0 0-.37.38z" fill="#FFC017"></path></svg></div></div></div></button></div></div><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>4 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>Aug 19</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" href="https://medium.com/@73srabt/bye-bye-spotify-37bb823839d2?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----a37fc706b865----2---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">303</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div><div class="j i d"><div class="sh bg si xz"></div></div></div></div></div></div></div></article></div></div><div class="tq tr ts tt tu tv tw tx ty tz ua ub uc ud ue uf ug uh ui uj uk"><div class="ul um un uo up dy l"><article class="dy"><div class="dy qm l"><div class="bg dy"><div class="dy l"><div class="dy uq ur us ut uu uv uw ux uy uz va vb vc"><div class="vd"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" aria-label="Advice From a Software Engineer With 8 Years of Experience" href="https://medium.com/@ruizb/advices-from-a-software-engineer-with-8-years-of-experience-8df5111d4d55?source=read_next_recirc-----a37fc706b865----3---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="vf vg vh vi vj"><img alt="Advice From a Software Engineer With 8 Years of Experience" class="bg vk vl vm vn bw" src="https://miro.medium.com/v2/resize:fit:1358/1*r5coQ_IUZNe0yjOH7vq_6w.jpeg" loading="lazy"/></div></a></div><div class="ve ab ca cn"><div class="vo vp vq vr vs ab"><div class="py l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@ruizb?source=read_next_recirc-----a37fc706b865----3---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div class="l ee"><img alt="Benoit Ruiz" class="l eq bx vt vu cw" src="https://miro.medium.com/v2/resize:fill:40:40/1*-7F-xdEpfDbogXdyl141OQ.jpeg" width="20" height="20" loading="lazy"/><div class="em bx l vt vu eo n ax ep"></div></div></a></div></div></div><div class="vv l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://medium.com/@ruizb?source=read_next_recirc-----a37fc706b865----3---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Benoit Ruiz</p></a></div></div></div><div class="vv l"><p class="be b dx z dw">in</p></div><div class="l"><div><div class="bl" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hn ab q" href="https://betterprogramming.pub/?source=read_next_recirc-----a37fc706b865----3---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><p class="be b dx z vw vx vy vz wa wb wc wd bj">Better Programming</p></a></div></div></div></div><div class="we wf wg wh wi wj wk wl wm wn l fk"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@ruizb/advices-from-a-software-engineer-with-8-years-of-experience-8df5111d4d55?source=read_next_recirc-----a37fc706b865----3---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><div title=""><h2 class="be fs oy pa wo wp mv pb pd wq wr mz lq nb ws wt nc lu ne wu wv nf ly nh ww wx ni vw vy vz wb wd bj">Advice From a Software Engineer With 8 Years of Experience</h2></div><div class="wy l"><h3 class="be b hl z vw wz vy vz xa wb wd dw">Practical tips for those who want to advance in their careers</h3></div></a></div><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@ruizb/advices-from-a-software-engineer-with-8-years-of-experience-8df5111d4d55?source=read_next_recirc-----a37fc706b865----3---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><span class="be b dx z dw"><div class="ab q"><span>22 min read</span><span class="ho l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span>Mar 20</span></div></span></a><div class="xb xc xd xe xf l"><div class="ab co"><div class="am xg xh xi xj xk xl xm xn xo xp ab q"><div class="ab q jh"><div class="pw-multi-vote-icon ee ji jj jk jl"><div class=""><div class="jm jn jo jp jq jr js am jt ju jv jl"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l jw jx jy jz ka kb kc"><p class="be b dx z dw"><span class="jn">--</span></p></div></div><div class="xq l"><div><div class="bl" aria-hidden="false"><a class="af ef ah jm aj ak al kf an ao ap aq ar as at ke ab q xt kp" aria-label="responses" href="https://medium.com/@ruizb/advices-from-a-software-engineer-with-8-years-of-experience-8df5111d4d55?responsesOpen=true&amp;sortBy=REVERSE_CHRON&amp;source=read_next_recirc-----a37fc706b865----3---------------------0a79ee37_1141_4afe_91c6_43e7b32b6155-------" rel="noopener follow"><svg width="24" height="24" viewBox="0 0 24 24" class=""><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count xr xs">205</span></p></a></div></div></div></div><div class="ab q xu xv"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="af ef ah ai aj ak al kh an ao ap hs xw xx kl" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="km"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div><div class="xy l"><div class="bl" aria-hidden="false" aria-describedby="expandablePostCardReaderMenu" aria-labelledby="expandablePostCardReaderMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="expandablePostCardReaderMenu" aria-expanded="false" aria-label="More options" class="af ef ah ai aj ak al kh an ao ap hs kn ko kp kq"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div></div><div class="sh bg si dj dk ya yb yc"></div><a class="be b bf z bj pz yg yh yi yj xt yk yl rr hs ym yn yo ru yp yq yr ys yt rw rx eq bl ry ot" href="https://medium.com/?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><div class="l ot">See more recommendations</div></a></div></div></div><div class="h k j"><div class="sh bg si so"></div><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="sp ab jh hy"><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Help</p></a></div><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Status</p></a></div><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/about?autoplay=1&amp;source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">About</p></a></div><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Careers</p></a></div><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Blog</p></a></div><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Privacy</p></a></div><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Terms</p></a></div><div class="sq sr l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Text to speech</p></a></div><div class="sq l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/business?source=post_page-----a37fc706b865--------------------------------" rel="noopener follow"><p class="be b dx z dw">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20231016-211539-2603c3c58e"</script><script>window.__GRAPHQL_URI__ = "https://cointegrated.medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"User is logged in","group":"disabled","tags":["group-edgeCachePosts","post-a37fc706b865","user-4554ccd3a122"],"serverVariantState":"","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":false,"vary":[],"inDisabledExperiment":false,"loHomepageEnabled":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"USER","id":"4554ccd3a122","explicit":true},"viewerIsBot":false},"debug":{"requestId":"49598c8c-b255-48b0-b66a-3de472252e58","hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"1514db277d81ad42","ot-tracer-traceid":"1d58554e307bcef4","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fcointegrated.medium.com\u002Fa37fc706b865","host":"cointegrated.medium.com","hostname":"cointegrated.medium.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"config":{"nodeEnv":"production","version":"main-20231016-211539-2603c3c58e","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20231016-211539-2603c3c58e","commit":"2603c3c58e6b27fa734b529fddb6321796de36eb"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":"be3df0a27179"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","isLoggedIn":true,"variantFlags":[{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"bevy_rds_double_write","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_autorefresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_members_only_audio","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefined_top_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_image_sharer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_triton_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members_username_selection","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_dynamic_aspirational_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"author_fair_distribution_non_qp3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_topic_portals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"price_smoke_test_monthly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_pill_based_home_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_premium_plan","valueType":{"__typename":"VariantFlagString","value":"4a442ace1476"}},{"__typename":"VariantFlag","name":"enable_ios_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_generation_pipeline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_verifications_service","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_two_hour_refresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_aspiriational","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_twitter_auth_suggestions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_premium_plan","valueType":{"__typename":"VariantFlagString","value":"12a660186432"}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_verified_book_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_entities_to_follow_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_seamless_social_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sprig","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_receive_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_new_user_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"explicit_signals_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_partner_program_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"web_enable_syntax_highlighting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_maim_the_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deprecate_legacy_providers_v3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_in_app_free_trial","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"onboarding_tags_from_top_views","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_programming","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_remove_twitter_onboarding_step","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reader_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"textshots_userid","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_dashboard_referred_earnings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pre_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_edge_cache","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_auto_follow_on_subscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_fs_cache_user_vals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_follower_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"android_enable_syntax_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_group_gifting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_import_logged_out_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_simplified_digest_v2_b","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"enable_tipping_v0_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"crm_send_contact_to_sendgrid","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_verified_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_aggregator_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_recs","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cache_less_following_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recirc_model","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_new_push_notification_endpoint","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound-source-serif-pro"}},{"__typename":"VariantFlag","name":"price_smoke_test_yearly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"enable_speechify_widget","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_lists_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_send_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_partner_program_enrollment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_display_paywall_after_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_intercom_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_avatar_upload","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_easy_resubscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automod","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_about_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_editor_new_publishing_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_miro_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_archive_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_social_share_sheet","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_legacy_feed_in_iceland","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_dynamic_programming_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:9a99d8ca5e92"},"collectionByDomainOrSlug({\"domainOrSlug\":\"cointegrated.medium.com\"})":null,"postResult({\"id\":\"a37fc706b865\"})":{"__ref":"Post:a37fc706b865"},"post({\"id\":\"a37fc706b865\"})":{"__ref":"Post:a37fc706b865"},"authorCollectionRecircFeed({\"input\":{\"authorId\":\"4554ccd3a122\",\"paging\":{\"limit\":4},\"postId\":\"a37fc706b865\"}})":{"__typename":"AuthorCollectionRecircFeedResult","items":[{"__typename":"HomeFeedItem","post":{"__ref":"Post:b9f94f3d9c90"},"feedId":"83dd8699-d79b-45b4-be3c-6a210d841367"},{"__typename":"HomeFeedItem","post":{"__ref":"Post:eb212e9919ca"},"feedId":"83dd8699-d79b-45b4-be3c-6a210d841367"},{"__typename":"HomeFeedItem","post":{"__ref":"Post:36601b73ecbb"},"feedId":"83dd8699-d79b-45b4-be3c-6a210d841367"},{"__typename":"HomeFeedItem","post":{"__ref":"Post:8d47f2103da6"},"feedId":"83dd8699-d79b-45b4-be3c-6a210d841367"}]},"recirc({\"paging\":{\"limit\":6},\"postId\":\"a37fc706b865\"})":{"__typename":"RexRecircResult","items":[{"__typename":"RexRecircItem","feedId":"0a79ee37-1141-4afe-91c6-43e7b32b6155","post":{"__ref":"Post:a6f367f02e53"}},{"__typename":"RexRecircItem","feedId":"0a79ee37-1141-4afe-91c6-43e7b32b6155","post":{"__ref":"Post:15ed50e63da6"}},{"__typename":"RexRecircItem","feedId":"0a79ee37-1141-4afe-91c6-43e7b32b6155","post":{"__ref":"Post:426d5e3f7d05"}},{"__typename":"RexRecircItem","feedId":"0a79ee37-1141-4afe-91c6-43e7b32b6155","post":{"__ref":"Post:2c7273e8be1e"}},{"__typename":"RexRecircItem","feedId":"0a79ee37-1141-4afe-91c6-43e7b32b6155","post":{"__ref":"Post:37bb823839d2"}},{"__typename":"RexRecircItem","feedId":"0a79ee37-1141-4afe-91c6-43e7b32b6155","post":{"__ref":"Post:8df5111d4d55"}}]},"postCatalogRecirc({\"pagingOptions\":{\"limit\":4},\"postId\":\"a37fc706b865\"})":{"__typename":"CatalogsConnection","catalogs":[{"__ref":"Catalog:5969c7449b7f"},{"__ref":"Catalog:0a856388a93a"},{"__ref":"Catalog:508b0743c247"},{"__ref":"Catalog:7a5756752f49"}]}},"User:9a99d8ca5e92":{"__typename":"User","id":"9a99d8ca5e92","username":"aigizk","name":"Aygiz Kunafin","imageId":"0*W4kEaTY72ng3L8ng.jpg","mediumMemberAt":0,"membership":null,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"emailObfuscated":"ai••••@gmail.com","unverifiedEmail":"","createdAt":1588663272617,"isAuroraVisible":true,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"styleEditorOnboardingVersionSeen":0,"allowEmailAddressSharingEditorWriter":false,"hasSubdomain":false,"dismissableFlags":[],"hasGroupGiftingEnabled":false,"twitterScreenName":"","geolocation":{"__typename":"Geolocation","country":"DE"},"atsQualifiedAt":0,"postSubscribeMembershipUpsellShownAt":0,"hightowerTermsAcceptedAt":null,"isEligibleToImportEmails":false},"LinkedAccounts:4554ccd3a122":{"__typename":"LinkedAccounts","mastodon":null,"id":"4554ccd3a122"},"UserViewerEdge:userId:4554ccd3a122-viewerId:9a99d8ca5e92":{"__typename":"UserViewerEdge","id":"userId:4554ccd3a122-viewerId:9a99d8ca5e92","isFollowing":false,"isUser":false},"NewsletterV3:699e7aa4e1e6":{"__typename":"NewsletterV3","id":"699e7aa4e1e6","type":"NEWSLETTER_TYPE_AUTHOR","slug":"4554ccd3a122","name":"4554ccd3a122","collection":null,"user":{"__ref":"User:4554ccd3a122"}},"User:4554ccd3a122":{"__typename":"User","id":"4554ccd3a122","name":"David Dale","username":"cointegrated","newsletterV3":{"__ref":"NewsletterV3:699e7aa4e1e6"},"linkedAccounts":{"__ref":"LinkedAccounts:4554ccd3a122"},"isSuspended":false,"imageId":"0*Lnxo9LylIaenwvDF.","mediumMemberAt":0,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":125},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"cointegrated.medium.com"}},"hasSubdomain":true,"bio":"NLP researcher at Meta FAIR, low-resource language enthusiast. See daviddale.ru.","isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:4554ccd3a122-viewerId:9a99d8ca5e92"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"membership":null,"twitterScreenName":""},"Paragraph:_0":{"__typename":"Paragraph","id":"_0","name":"729f","type":"H3","href":null,"layout":null,"metadata":null,"text":"How to fine-tune a NLLB-200 model for translating a new language","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_1":{"__typename":"Paragraph","id":"_1","name":"7c01","type":"P","href":null,"layout":null,"metadata":null,"text":"“NLLB” (which stands for “no language left behind”) is a family of machine translation models published by Meta AI in 2022. These models can translate a sentence between any of the 202 language varieties, which is a huge research breakthrough. But there are about 7000 languages in the world, so most of them are still left behind so far. ","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":94,"end":122,"href":"https:\u002F\u002Fai.meta.com\u002Fblog\u002Fnllb-200-high-quality-machine-translation\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":181,"end":203,"href":"https:\u002F\u002Fai.meta.com\u002Fresearch\u002Fno-language-left-behind\u002F#200-languages-accordion","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_2":{"__typename":"Paragraph","id":"_2","name":"d009","type":"P","href":null,"layout":null,"metadata":null,"text":"How can we teach this model one more language? In this tutorial, I show it. As a demonstration, we will teach the model to translate between Russian and a low-resource Tyvan language, which is new to this model. The Russian language is Slavic, while Tyvan is a Turkic language with a lot of Mongolian influence. Therefore, while they are written in similar Cyrillic scripts, translating between them is not trivial. Nevertheless, thanks to the pretraining with 202 languages, NLLB can do it pretty well. ","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":168,"end":182,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTuvan_language","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_3":{"__typename":"Paragraph","id":"_3","name":"ec1a","type":"P","href":null,"layout":null,"metadata":null,"text":"The full code of the tutorial can be found in this Colab notebook. The tutorial includes 8 steps:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":46,"end":65,"href":"https:\u002F\u002Fcolab.research.google.com\u002Fdrive\u002F1bayEaw2fz_9Mhg9jFFZhrmDlQlBj1YZf?usp=sharing","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":46,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_4":{"__typename":"Paragraph","id":"_4","name":"911c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Looking at the training data","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_5":{"__typename":"Paragraph","id":"_5","name":"c4cb","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Examining tokenization of the new language","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_6":{"__typename":"Paragraph","id":"_6","name":"7188","type":"OLI","href":null,"layout":null,"metadata":null,"text":"(Optionally) Updating the vocabulary","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_7":{"__typename":"Paragraph","id":"_7","name":"b2cf","type":"OLI","href":null,"layout":null,"metadata":null,"text":"(Optionally) Adding a new language token","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_8":{"__typename":"Paragraph","id":"_8","name":"4765","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Training the neural model","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_9":{"__typename":"Paragraph","id":"_9","name":"0ee7","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Evaluating the model","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_10":{"__typename":"Paragraph","id":"_10","name":"f159","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Publishing","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_11":{"__typename":"Paragraph","id":"_11","name":"e47d","type":"OLI","href":null,"layout":null,"metadata":null,"text":"(Optionally) Serving the model with a Docker app","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_12":{"__typename":"Paragraph","id":"_12","name":"9cbf","type":"H4","href":null,"layout":null,"metadata":null,"text":"Why NLLB?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_13":{"__typename":"Paragraph","id":"_13","name":"9bba","type":"P","href":null,"layout":null,"metadata":null,"text":"It is one of the most popular publicly available translation models (according e.g. to the number of likes on HF), and probably the most multilingual one. ","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":87,"end":112,"href":"https:\u002F\u002Fhuggingface.co\u002Fmodels?pipeline_tag=translation&sort=likes","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_14":{"__typename":"Paragraph","id":"_14","name":"5a32","type":"P","href":null,"layout":null,"metadata":null,"text":"Alternatively, you may want to use open LLMs, such as LLaMA. In the last year, LLMs became popular for multiple tasks, including translation, and they are reported to translate into English better than NLLB. However, these models are at least x10 larger than the NLLB-200-600M checkpoint which we will fine-tune, and this makes their training and deployment more difficult and expensive. Also, the pretraining data for most LLMs is usually 90% or more English, so they may not generate texts so well in other languages, especially lower-resourced ones. Thus, we stick to NLLB.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":151,"end":163,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F2309.11674","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":263,"end":276,"href":"https:\u002F\u002Fhuggingface.co\u002Ffacebook\u002Fnllb-200-distilled-600M","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_15":{"__typename":"Paragraph","id":"_15","name":"8fbf","type":"H4","href":null,"layout":null,"metadata":null,"text":"Key concepts","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_16":{"__typename":"Paragraph","id":"_16","name":"f506","type":"P","href":null,"layout":null,"metadata":null,"text":"In this section, I briefly list the terms that we will be using. If too many of them are unfamiliar to you, please consider doing some introductory reading (e.g. NLP course by Lena Voita) before proceeding with this article.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":162,"end":186,"href":"https:\u002F\u002Flena-voita.github.io\u002Fnlp_course.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_17":{"__typename":"Paragraph","id":"_17","name":"5ac3","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Corpus: a diverse collection of texts (or, if it is a parallel corpus, of text pairs in two different languages).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":54,"end":69,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_18":{"__typename":"Paragraph","id":"_18","name":"0fc9","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Normalization: conversion of something to its “normal” form, where “normal” is defined arbitrarily. For example, if we decide that quotation marks should look like \", then replacing « with \" would be a normalization.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":164,"end":165,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":182,"end":183,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":189,"end":190,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_19":{"__typename":"Paragraph","id":"_19","name":"4850","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Token: a minimal unit of text with which a neural network operates. Their size is usually between a single character and a whole word with a space before it. Each model usually support a fixed set of tokens, called its vocabulary. \nFor example, the vocabulary of NLLB by default has 256204 tokens. The word \"preprocess\" is not in this vocabulary, but can be represented as 3 tokens, \"▁pre\" + \"pro\" + \"cess\", where \"▁\" is a substitute for space and it signals the start of a new word. These tokens occupy the positions 951, 4573, and 8786 in the vocabulary.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":307,"end":319,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":383,"end":406,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":414,"end":417,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":219,"end":229,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_20":{"__typename":"Paragraph","id":"_20","name":"3faa","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Tokenizer: a tool for converting texts into token ids (their positions in the vocabulary) and back.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_21":{"__typename":"Paragraph","id":"_21","name":"07c3","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Embedding: a numerical vector that represents some object. In our case, embeddings are parts of the translation model and they represent tokens. For example, NLLB-200–600M model has 256204 embeddings (one per each token in its vocabulary), and they are 1024-dimensional vectors. Embeddings are trained together with the rest of the neural network.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_22":{"__typename":"Paragraph","id":"_22","name":"e615","type":"H4","href":null,"layout":null,"metadata":null,"text":"Prerequisites","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_23":{"__typename":"Paragraph","id":"_23","name":"d95c","type":"P","href":null,"layout":null,"metadata":null,"text":"I assume that you are familiar with the Python programming language and the Google Colab environment. Ideally, you should also understand the Huggingface ecosystem (they have a good course about it).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":40,"end":47,"href":"https:\u002F\u002Fwww.python.org","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":76,"end":88,"href":"https:\u002F\u002Fcolab.research.google.com","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":175,"end":197,"href":"https:\u002F\u002Fhuggingface.co\u002Flearn\u002Fnlp-course\u002Fchapter1\u002F1","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_24":{"__typename":"Paragraph","id":"_24","name":"9ddb","type":"P","href":null,"layout":null,"metadata":null,"text":"To reproduce this tutorial, you will need a Tyvan-Russian parallel corpus, that is, a collection of translated sentences, phrases or words. You can download a version of it from https:\u002F\u002Ftyvan.ru. This version contains 50K translation pairs, whereas the one I used in the notebook is larger, almost 120K pairs, so my results won’t be reproduced exactly. Nevertheless, even 50K training pairs can be enough for decent translation.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":178,"end":194,"href":"https:\u002F\u002Ftyvan.ru\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_25":{"__typename":"Paragraph","id":"_25","name":"2261","type":"P","href":null,"layout":null,"metadata":null,"text":"If you want to use another language pair, you will have to find the data for it (at least a few thousand pairs; a few hundred thousand for really good translation). Two good spots to start the search are OPUS and HF datasets.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":204,"end":208,"href":"https:\u002F\u002Fopus.nlpl.eu\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":213,"end":224,"href":"https:\u002F\u002Fhuggingface.co\u002Fdatasets?task_categories=task_categories:translation&sort=likes","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_26":{"__typename":"Paragraph","id":"_26","name":"a4ae","type":"P","href":null,"layout":null,"metadata":null,"text":"Originally, I ran the tutorial code in a Google Colab notebook with a Tesla T4 GPU (15Gb of memory). I fine-tuned the model for about 20 hours (50K training steps); running the notebook for this duration without interruption required from me a paid Colab subscription. Fine-tuning for a few hours can usually be done on a free plan; fine-tuning with less GPU memory may require adjusting the batch size.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_27":{"__typename":"Paragraph","id":"_27","name":"f65f","type":"P","href":null,"layout":null,"metadata":null,"text":"To load the dataset and save the model, I use my Google Drive, which I mount to the \u002Fgd directory in Colab:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":84,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_28":{"__typename":"Paragraph","id":"_28","name":"38bb","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from google.colab import drive\nimport os\nif not os.path.exists('\u002Fgd'):\n    drive.mount('\u002Fgd')","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_29":{"__typename":"Paragraph","id":"_29","name":"6191","type":"P","href":null,"layout":null,"metadata":null,"text":"For uploading the dataset, instead of Google Drive, you can just use the “upload” graphical interface of Colab (and change accordingly the path to the file in the code in the next section):","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*qNRkzVmsA4GkrsTQwWKJfw.png":{"__typename":"ImageMetadata","id":"1*qNRkzVmsA4GkrsTQwWKJfw.png","originalHeight":387,"originalWidth":704,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:_30":{"__typename":"Paragraph","id":"_30","name":"bf29","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*qNRkzVmsA4GkrsTQwWKJfw.png"},"text":"File upload interface. Colab screenshot by author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_31":{"__typename":"Paragraph","id":"_31","name":"94d5","type":"P","href":null,"layout":null,"metadata":null,"text":"Before running my notebook, I install a few Python modules:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_32":{"__typename":"Paragraph","id":"_32","name":"3c70","type":"PRE","href":null,"layout":null,"metadata":null,"text":"!pip install sentencepiece transformers==4.33 datasets sacremoses sacrebleu -q","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_33":{"__typename":"Paragraph","id":"_33","name":"1980","type":"P","href":null,"layout":null,"metadata":null,"text":"The specific version transformers==4.33 is important, because the way I mingle with the tokenizer at the Step 4 depends on it. In the version 4.34, the package started introducing breaking changes to the tokenizer, and when these changes stabilize, the recommended code for updating the tokenizer will be different.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":21,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":142,"end":146,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_34":{"__typename":"Paragraph","id":"_34","name":"32bf","type":"H3","href":null,"layout":null,"metadata":null,"text":"The steps for adding a new language to NLLB","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_35":{"__typename":"Paragraph","id":"_35","name":"0425","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 1: looking at the data","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_36":{"__typename":"Paragraph","id":"_36","name":"4a29","type":"P","href":null,"layout":null,"metadata":null,"text":"I start by reading the training dataset and taking a look at it:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_37":{"__typename":"Paragraph","id":"_37","name":"cea5","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import pandas as pd\ntrans_df = pd.read_csv('\u002Fgd\u002FMyDrive\u002Fdatasets\u002Fnlp\u002Ftyvan\u002Frus_tyv_parallel_50k.tsv', sep=\"\\t\")\nprint(trans_df.shape) # (50000, 5)\nprint(trans_df.columns) # ['row_id', 'ind', 'tyv', 'ru', 'split']\ntrans_df.sample(10)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_38":{"__typename":"Paragraph","id":"_38","name":"c6f8","type":"P","href":null,"layout":null,"metadata":null,"text":"Here is the output; tyv and ru are the columns with the translation pairs. ","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":20,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":28,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*55P9jCCcCvl89cp0bJ4beQ.png":{"__typename":"ImageMetadata","id":"1*55P9jCCcCvl89cp0bJ4beQ.png","originalHeight":399,"originalWidth":866,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:_39":{"__typename":"Paragraph","id":"_39","name":"2b81","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*55P9jCCcCvl89cp0bJ4beQ.png"},"text":"A sample of the source dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_40":{"__typename":"Paragraph","id":"_40","name":"1d03","type":"P","href":null,"layout":null,"metadata":null,"text":"In this dataset, most of the texts are individual words or short phrases, and only a few of them are complete sentences. This is not typical for the machine translation data; normally it is collected on the sentence level. However, with low-resource languages, dictionaries may be a dominant data source, and they are constructed mostly on the word or phrase level, so such distribution of texts is not totally unusual. In any case, you need to make sure that the training texts do contain many diverse sentences, and that they look more or less clean and consistent. If there are longer training texts (e.g. paragraphs), it is recommended to split them in sentences (e.g. with this tool), and re-align them into translation pairs on the sentence level (e.g. with lingtrain-aligner or vecalign).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":678,"end":687,"href":"https:\u002F\u002Fpypi.org\u002Fproject\u002Fsentence-splitter\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":764,"end":781,"href":"https:\u002F\u002Fpypi.org\u002Fproject\u002Flingtrain-aligner\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":785,"end":793,"href":"https:\u002F\u002Fgithub.com\u002Fthompsonb\u002Fvecalign","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_41":{"__typename":"Paragraph","id":"_41","name":"fe4e","type":"P","href":null,"layout":null,"metadata":null,"text":"This dataset is pre-split into train, dev and test subsets. I did it previously with sklearn.model_selection.train_test_split ; in the training notebook, I load the splits into separate dataframes:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":85,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_42":{"__typename":"Paragraph","id":"_42","name":"d7a9","type":"PRE","href":null,"layout":null,"metadata":null,"text":"df_train = trans_df[trans_df.split=='train'].copy() # 49000 items\ndf_dev = trans_df[trans_df.split=='dev'].copy()     # 500 items\ndf_test = trans_df[trans_df.split=='test'].copy()   # 500 items","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_43":{"__typename":"Paragraph","id":"_43","name":"8745","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 2: How well does the data fit into a NLLB tokenizer?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_44":{"__typename":"Paragraph","id":"_44","name":"4b59","type":"P","href":null,"layout":null,"metadata":null,"text":"The NLLB models (as most of other modern NLP neural model) consist of 2 components: ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_45":{"__typename":"Paragraph","id":"_45","name":"7d3e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"the tokenizer (a thing that splits the text into chunks and maps each chunk into a number, according to a pre-defined vocabulary);","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":4,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_46":{"__typename":"Paragraph","id":"_46","name":"fe07","type":"OLI","href":null,"layout":null,"metadata":null,"text":"the neural network itself (it performs the translation based on these numbers and outputs some new numbers; then the tokenizer converts them back to texts).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_47":{"__typename":"Paragraph","id":"_47","name":"96c0","type":"P","href":null,"layout":null,"metadata":null,"text":"So the translation always goes the way “input text -\u003E input tokens -\u003E translated tokens -\u003E translated text”, like in this example:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_48":{"__typename":"Paragraph","id":"_48","name":"6324","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nmodel_name = \"facebook\u002Fnllb-200-distilled-600M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntokenizer.src_lang = \"rus_Cyrl\"\ninputs = tokenizer(text=\"поля озарились утренним солнцем\", return_tensors=\"pt\")\ntranslated_tokens = model.generate(\n    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"]\n)\nprint(tokenizer.decode(translated_tokens[0], skip_special_tokens=True))\n# The fields were lit by the morning sun","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_49":{"__typename":"Paragraph","id":"_49","name":"b40d","type":"P","href":null,"layout":null,"metadata":null,"text":"(If this code snippet intimidates you, please consider taking the Huggingface course before returning to this tutorial)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":62,"end":84,"href":"https:\u002F\u002Fhuggingface.co\u002Flearn\u002Fnlp-course\u002Fchapter1\u002F1","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":119,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_50":{"__typename":"Paragraph","id":"_50","name":"c924","type":"P","href":null,"layout":null,"metadata":null,"text":"The quality of translation critically depends on how well the tokenizer represents our languages:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_51":{"__typename":"Paragraph","id":"_51","name":"8a18","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How many tokens per word do we have on average? For a good translation quality, we usually want one token to represent a word or a morpheme, or at least something of a comparable size.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_52":{"__typename":"Paragraph","id":"_52","name":"d2fd","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Does the tokenizer support most of our vocabulary? All unsupported characters are converted to the special \u003Cunk\u003E token that carries very little information; many such cases again degrade the quality.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":107,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_53":{"__typename":"Paragraph","id":"_53","name":"3e7a","type":"P","href":null,"layout":null,"metadata":null,"text":"Tokenization is something that we can test before even touching the translation model itself. I extract a sample of the training data and count the number of words and tokens in it: ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_54":{"__typename":"Paragraph","id":"_54","name":"c9e8","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import re\n\ndef word_tokenize(text):\n    \"\"\"\n    Split a text into words, numbers, and punctuation marks\n    (for languages where words are separated by spaces)\n    \"\"\"\n    return re.findall('(\\w+|[^\\w\\s])', text)\n\nsmpl = df_train.sample(10000, random_state=1)\nsmpl['rus_toks'] = smpl.ru.apply(tokenizer.tokenize)\nsmpl['tyv_toks'] = smpl.tyv.apply(tokenizer.tokenize)\nsmpl['rus_words'] = smpl.ru.apply(word_tokenize)\nsmpl['tyv_words'] = smpl.tyv.apply(word_tokenize)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_55":{"__typename":"Paragraph","id":"_55","name":"2910","type":"P","href":null,"layout":null,"metadata":null,"text":"Now I can take a glance at the tokens. They look adequately (at least, to my subjective eye); the average of 2–3 tokens per word is typical for well-tokenized texts in morphologically rich languages, such as Russian or Tyvan.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*llVgsAL208N0Ysaq2O74nw.png":{"__typename":"ImageMetadata","id":"1*llVgsAL208N0Ysaq2O74nw.png","originalHeight":355,"originalWidth":1351,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:_56":{"__typename":"Paragraph","id":"_56","name":"5383","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*llVgsAL208N0Ysaq2O74nw.png"},"text":"A sample of tokenized Russian and Tyvan texts","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_57":{"__typename":"Paragraph","id":"_57","name":"bbd3","type":"P","href":null,"layout":null,"metadata":null,"text":"Actually, we can compute precise statistics of this (they are explained here):","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":72,"end":76,"href":"https:\u002F\u002Fpandas.pydata.org\u002Fdocs\u002Freference\u002Fapi\u002Fpandas.DataFrame.describe.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_58":{"__typename":"Paragraph","id":"_58","name":"ad3c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"stats = smpl[\n    ['rus_toks', 'tyv_toks', 'rus_words', 'tyv_words']\n].applymap(len).describe()\nprint(stats.rus_toks['mean'] \u002F stats.rus_words['mean'])  # 2.0349\nprint(stats.tyv_toks['mean'] \u002F stats.tyv_words['mean'])  # 2.4234\nstats","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ywMjfQy7ZYELR9-mJD-1LA.png":{"__typename":"ImageMetadata","id":"1*ywMjfQy7ZYELR9-mJD-1LA.png","originalHeight":315,"originalWidth":527,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:_59":{"__typename":"Paragraph","id":"_59","name":"e52f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ywMjfQy7ZYELR9-mJD-1LA.png"},"text":"Statistics of the word and token counts","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_60":{"__typename":"Paragraph","id":"_60","name":"627a","type":"P","href":null,"layout":null,"metadata":null,"text":"Good news: for Tyvan, a new language, the NLLB tokenizer produces on average 2.4 tokens per word; almost as few as 2.0 for the well-supported Russian language. This implies that the translation quality of fine-tuned NLLB may be decent without extending its vocabulary with Tyvan tokens.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_61":{"__typename":"Paragraph","id":"_61","name":"f76b","type":"P","href":null,"layout":null,"metadata":null,"text":"Another useful check: how often does the \u003Cunk\u003E token happen in the tokenizer output for Tyvan? If this is too often, we need to fix it somehow.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":41,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_62":{"__typename":"Paragraph","id":"_62","name":"71a4","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from tqdm.auto import tqdm, trange\ntexts_with_unk = [\n    text for text in tqdm(trans_df.tyv) \n    if tokenizer.unk_token_id in tokenizer(text).input_ids\n]\nprint(len(texts_with_unk))\n# 163\ns = random.sample(texts_with_unk, 5)\nprint(s)\n# ['Ынча деп турар болзуңза, сени кандыг далайжы дээрил?! – деп, Дриниан удурланган.'\n# 'Ыяштап чоруй барган ачам келгеш, мени аажок мактаар боор: «Ёзулуг аңчы-дыр сен, оглум!»',\n# 'Ажыл хуваарга — арыг кирер, ажык хуваарга — куруг үнер', ...","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_63":{"__typename":"Paragraph","id":"_63","name":"beab","type":"P","href":null,"layout":null,"metadata":null,"text":"We can see that out of 49K texts, 163 contain an “unknown symbol” after tokenization. Most of these cases seem to be associated with non-standard punctuation marks (as defined by MosesPunctNormalizer), and there is a reason for that: the NLLB team preprocessed their texts before training the tokenizer and the model. The code for preprocessing (adapted from the Stopes repo) looks like this:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":179,"end":199,"href":"https:\u002F\u002Fgithub.com\u002Falvations\u002Fsacremoses#normalizer","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":346,"end":374,"href":"https:\u002F\u002Fgithub.com\u002Ffacebookresearch\u002Fstopes\u002Fblob\u002Fmain\u002Fstopes\u002Fpipelines\u002Fmonolingual\u002Fmonolingual_line_processor.py#L214","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_64":{"__typename":"Paragraph","id":"_64","name":"388f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import re\nimport sys\nimport unicodedata\nfrom sacremoses import MosesPunctNormalizer\n\nmpn = MosesPunctNormalizer(lang=\"en\")\nmpn.substitutions = [\n    (re.compile(r), sub) for r, sub in mpn.substitutions\n]\n\ndef get_non_printing_char_replacer(replace_by: str = \" \"):\n    non_printable_map = {\n        ord(c): replace_by\n        for c in (chr(i) for i in range(sys.maxunicode + 1))\n        # same as \\p{C} in perl\n        # see https:\u002F\u002Fwww.unicode.org\u002Freports\u002Ftr44\u002F#General_Category_Values\n        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n    }\n\n    def replace_non_printing_char(line) -\u003E str:\n        return line.translate(non_printable_map)\n\n    return replace_non_printing_char\n\nreplace_nonprint = get_non_printing_char_replacer(\" \")\n\ndef preproc(text):\n    clean = mpn.normalize(text)\n    clean = replace_nonprint(clean)\n    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n    clean = unicodedata.normalize(\"NFKC\", clean)\n    return clean","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_65":{"__typename":"Paragraph","id":"_65","name":"fa3f","type":"P","href":null,"layout":null,"metadata":null,"text":"If we apply this normalization before tokenizing the texts, all the “unknown” characters disappear.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_66":{"__typename":"Paragraph","id":"_66","name":"eacf","type":"PRE","href":null,"layout":null,"metadata":null,"text":"texts_with_unk_normed = [\n    text for text in tqdm(texts_with_unk) \n    if tokenizer.unk_token_id in tokenizer(preproc(text)).input_ids\n]\nprint(len(texts_with_unk_normed))  # 0","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_67":{"__typename":"Paragraph","id":"_67","name":"c2fb","type":"P","href":null,"layout":null,"metadata":null,"text":"To sum up, NLLB tokenizer produces tokens for Tyvan which are long enough, and successfully recognizes all Tyvan characters. I will use it as evidence that it is not neccessary to update the tokenizer’s vocabulary to use it with Tyvan.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_68":{"__typename":"Paragraph","id":"_68","name":"f451","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 3 (optional): Expanding the vocabulary","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_69":{"__typename":"Paragraph","id":"_69","name":"51f1","type":"P","href":null,"layout":null,"metadata":null,"text":"As we have concluded on the previous step, expanding the tokenizer’s vocabulary is not necessary for Tuvan: there are no out-of vocabulary characters, and the average token length is comparable with that of a high-resource language (Russian). If this was not the case, we would have to add some new tokens for Tyvan into the tokenizer’s vocabulary (and into the neural network as well). This section explains how to achieve that.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_70":{"__typename":"Paragraph","id":"_70","name":"55c0","type":"P","href":null,"layout":null,"metadata":null,"text":"I start by getting more Tyvan texts for training a Tyvan-specific tokenizer. These texts don’t have to be parallel with another language, so I just download a parsed version of the Tyvan Wikipedia. This triples the total length of the Tyvan texts I had:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_71":{"__typename":"Paragraph","id":"_71","name":"b5cf","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from datasets import load_dataset\ntyv_wiki = load_dataset(\"graelo\u002Fwikipedia\", \"20230601.tyv\")\ntyv_wiki\n# DatasetDict({\n#     train: Dataset({\n#         features: ['id', 'url', 'title', 'text'],\n#         num_rows: 3459\n#     })\n# })\nprint(sum(len(t) for t in tyv_wiki['train']['text']))  # 7568832\nprint(sum(len(t) for t in trans_df.tyv.dropna()))      # 3573803","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_72":{"__typename":"Paragraph","id":"_72","name":"cf50","type":"P","href":null,"layout":null,"metadata":null,"text":"After that, I put all my text together and preprocess them. Then I count their characters, in order to force my tokenizer to include all the characters that appear at least 3 times (the characters that appear only once or twice won’t probably be learned by the model anyway). I also exclude the space, because it is always converted to an underscore character by sentencepiece.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_73":{"__typename":"Paragraph","id":"_73","name":"dc93","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from collections import Counter\nall_texts = tyv_wiki['train']['text'] + df_train.tyv.dropna().tolist()\nall_text_normalized = [preproc(t) for t in tqdm(all_texts)]\nchars_cnt = Counter(c for t in all_text_normalized for c in t)\nrequired_chars = ''.join([\n    k for k, v in chars_cnt.most_common() \n    if v \u003E= 3 and k not in ' '\n])","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"csharp"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_74":{"__typename":"Paragraph","id":"_74","name":"fd63","type":"P","href":null,"layout":null,"metadata":null,"text":"I dump the texts into a plaintext file, and train a new sentencepiece tokenizer model on this file, in order to add its tokens to the existing NLLB tokenizer. Sentencepiece is one of the popular algorithms to train a tokenizer, and the NLLB tokenizer already has it under the hood.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":56,"end":69,"href":"https:\u002F\u002Fgithub.com\u002Fgoogle\u002Fsentencepiece","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":195,"end":226,"href":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Ftokenizer_summary","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_75":{"__typename":"Paragraph","id":"_75","name":"804d","type":"P","href":null,"layout":null,"metadata":null,"text":"I chose the vocabulary size to be 16384 intuitively, because such a number of tokens can potentially cover the most important roots and suffixes in the language (to compare: NLLB vocabulary for 200 languages has 256000 tokens, but many of them are used by a lot of different languages). All the other parameters are not very important.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_76":{"__typename":"Paragraph","id":"_76","name":"b349","type":"P","href":null,"layout":null,"metadata":null,"text":"This code executes for several minutes.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_77":{"__typename":"Paragraph","id":"_77","name":"17f5","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import sentencepiece as spm\nall_texts_file = 'myv_texts_plain.txt'\nSPM_PREFIX = 'spm_tyvan_16k'\nwith open(all_texts_file, 'w') as f:\n    for i, text in enumerate(all_texts):\n        print(text, file=f)\n\nspm.SentencePieceTrainer.train(\n    input=all_texts_file,\n    model_prefix=SPM_PREFIX,\n    vocab_size=2**14,  # 16K\n    character_coverage = 1,\n    num_threads=16,\n    train_extremely_large_corpus=False,\n    add_dummy_prefix=False,\n    max_sentencepiece_length=128,\n    max_sentence_length=4192*4,\n    pad_id=0,\n    eos_id=1,\n    unk_id=2,\n    bos_id=-1,\n    required_chars=required_chars,\n)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_78":{"__typename":"Paragraph","id":"_78","name":"915a","type":"P","href":null,"layout":null,"metadata":null,"text":"After training a Tyvan tokenizer, I perform a “surgical operation” with it: extracting the sentencepiece model from the standard NLLB tokenizer and enriching it from all tokens from the Tyvan tokenizer that have been missing from the NLLB tokenizer (based on the example from the sentencepiece repo).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":259,"end":298,"href":"https:\u002F\u002Fgithub.com\u002Fgoogle\u002Fsentencepiece\u002Fblob\u002Fmaster\u002Fpython\u002Fadd_new_vocab.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_79":{"__typename":"Paragraph","id":"_79","name":"21b3","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n# At this step, the code may throw an error about protobuf. Do as it tells.\nfrom transformers import NllbTokenizer\n\n# reading the NLLB and the Tyvan sentencepiece models into a native format\ntokenizer = NllbTokenizer.from_pretrained('facebook\u002Fnllb-200-distilled-600M')\nsp_trained = spm.SentencePieceProcessor(model_file=f'{SPM_PREFIX}.model')\nadded_spm = sp_pb2_model.ModelProto()\nadded_spm.ParseFromString(sp_trained.serialized_model_proto())\nold_spm = sp_pb2_model.ModelProto()\nold_spm.ParseFromString(tokenizer.sp_model.serialized_model_proto())\n\n# adding the missing tokens to the NLLB sentencepiece model\nnllb_tokens_set = {p.piece for p in old_spm.pieces}\nprev_min_score = old_spm.pieces[-1].score\nfor p in added_spm.pieces:\n    piece = p.piece\n    if piece not in nllb_tokens_set:\n        new_p = sp_pb2_model.ModelProto().SentencePiece()\n        new_p.piece = piece\n        # for all new tokens, I'll set a lower score (priority)\n        new_p.score = p.score + prev_min_score\n        old_spm.pieces.append(new_p)\n\n# saving the result to disk\nNEW_SPM_NAME = 'spm_nllb_tyvan_268k.model'\nwith open(NEW_SPM_NAME, 'wb') as f:\n    f.write(old_spm.SerializeToString())","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_80":{"__typename":"Paragraph","id":"_80","name":"eccf","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, I need to update the neural network weights: add new embeddings for the freshly added tokens. In NLLB, the token embeddings reside in the parameter called shared. It is used both in the encoder and decoder input embeddings and in the last decoder layer that predicts the distribution of the next token.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":164,"end":170,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_81":{"__typename":"Paragraph","id":"_81","name":"fc5d","type":"P","href":null,"layout":null,"metadata":null,"text":"By default, the embeddings for the new tokens are initialized randomly. Instead, I re-initialize each one with the average of the embeddings of the old tokens that corresponded to the new token (or if there are none, with the embedding of the \u003Cunk\u003E token). This slightly improves the training speed, because the newly created tokken embeddings are already informative.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":243,"end":248,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_82":{"__typename":"Paragraph","id":"_82","name":"8058","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from transformers import AutoModelForSeq2SeqLM\nmodel_name = 'facebook\u002Fnllb-200-distilled-600M'\n\n# loading the tokenizers\ntokenizer_old = NllbTokenizer.from_pretrained(model_name)\ntokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file=NEW_SPM_NAME)\nprint(len(tokenizer_old), len(tokenizer)) # 256204, 268559\nadded_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_old.get_vocab()))\nprint(len(added_vocab))  # 12355\n\n# loading and resizing the model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# re-initializing the new embeddings\nfor t in tqdm(added_vocab):\n    tt = tokenizer_old(t, add_special_tokens=False).input_ids\n    if len(tt) == 0:\n        tt = [tokenizer_old.unk_token_id]\n    idx = tokenizer.convert_tokens_to_ids(t)\n    model.model.shared.weight.data[idx] = model.model.shared.weight.data[tt].mean(0)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_83":{"__typename":"Paragraph","id":"_83","name":"5451","type":"P","href":null,"layout":null,"metadata":null,"text":"In the next steps of the tutorial, I will ignore the results of this optional step, and instead just continue from Step 2. But if you want to run vocabulary extension on your data, you can adapt my code from this notebook.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":203,"end":221,"href":"https:\u002F\u002Fcolab.research.google.com\u002Fdrive\u002F1G1iB8H_XaspNJKyR8k__GDzcHbQsMUAG?usp=sharing","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_84":{"__typename":"Paragraph","id":"_84","name":"2e7a","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 4 (optional): Adding a new language tag","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_85":{"__typename":"Paragraph","id":"_85","name":"a15a","type":"P","href":null,"layout":null,"metadata":null,"text":"In a NLLB tokenizer, language tags are special: they are tokens prepended to the source and target texts, and the model uses them to correctly identify source and target languages. If fine-tune a NLLB model, you may want to add a new language tag to the model and the tokenizer. However, if any of the following is true, you can skip this step and instead go directly to Step 5:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_86":{"__typename":"Paragraph","id":"_86","name":"2db4","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The languages that you are fine-tuning with are already included in the model;","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_87":{"__typename":"Paragraph","id":"_87","name":"b2da","type":"ULI","href":null,"layout":null,"metadata":null,"text":"You will reuse one of the existing language tags for your new language;","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_88":{"__typename":"Paragraph","id":"_88","name":"650b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"You are going to use the model only for a single pair of languages, so it can always just guess them from the source text.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_89":{"__typename":"Paragraph","id":"_89","name":"2582","type":"P","href":null,"layout":null,"metadata":null,"text":"The language tag tokens are not saved in the sentencepiece vocabulary; instead, they are stored in a hardcoded list. And this poses a problem when we try adding a new language token: the list is hardcoded (at least, for now).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":80,"end":115,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers\u002Fblob\u002Fv4.33.3\u002Fsrc\u002Ftransformers\u002Fmodels\u002Fnllb\u002Ftokenization_nllb.py#L45","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":216,"end":223,"href":"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers\u002Fissues\u002F26497","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_90":{"__typename":"Paragraph","id":"_90","name":"116f","type":"P","href":null,"layout":null,"metadata":null,"text":"To offset this problem, I write a function that re-runs a part of the tokenizer’s init code with a new language token. Unfortunately, calling this function once is not enough; you need to do this every time after you load the tokenizer from disk.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":82,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_91":{"__typename":"Paragraph","id":"_91","name":"5753","type":"P","href":null,"layout":null,"metadata":null,"text":"Disclaimer: when I was working on this code, I used the package version transformers\u003C=4.33. \nLater, I will publish an update that supports the newer versions.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":158,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_92":{"__typename":"Paragraph","id":"_92","name":"5305","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def fix_tokenizer(tokenizer, new_lang='tyv_Cyrl'):\n    \"\"\"\n    Add a new language token to the tokenizer vocabulary \n    (this should be done each time after its initialization)\n    \"\"\"\n    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n    tokenizer.lang_code_to_id[new_lang] = old_len-1\n    tokenizer.id_to_lang_code[old_len-1] = new_lang\n    # always move \"mask\" to the last position\n    tokenizer.fairseq_tokens_to_ids[\"\u003Cmask\u003E\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n\n    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n    if new_lang not in tokenizer._additional_special_tokens:\n        tokenizer._additional_special_tokens.append(new_lang)\n    # clear the added token encoder; otherwise a new token may end up there by mistake\n    tokenizer.added_tokens_encoder = {}\n    tokenizer.added_tokens_decoder = {} ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_93":{"__typename":"Paragraph","id":"_93","name":"71d2","type":"P","href":null,"layout":null,"metadata":null,"text":"I apply this function to the tokenizer, and it adds one new language token to it. Then I expand the embedding layer of the model accordingly, after which I need to patch the embeddings of the model, for two reasons:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_94":{"__typename":"Paragraph","id":"_94","name":"ed1a","type":"OLI","href":null,"layout":null,"metadata":null,"text":"In NLLB vocabulary, for some unknown reason, the \u003Cmask\u003E token always goes after all the language codes, so if I add one more, the \u003Cmask\u003E token also moves; thus, I move its embedding.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":49,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":130,"end":136,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_95":{"__typename":"Paragraph","id":"_95","name":"c169","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The embedding for the new token is by default initialized randomly, but instead I choose to initialize it with the language code for a similar language: Kyrgyz.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_96":{"__typename":"Paragraph","id":"_96","name":"6498","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nmodel_name = \"facebook\u002Fnllb-200-distilled-600M\"\n# loading the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n# patching them\nfix_tokenizer(tokenizer)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# fixing the new\u002Fmoved token embeddings in the model\nadded_token_id = tokenizer.convert_tokens_to_ids('tyv_Cyrl')\nsimilar_lang_id = tokenizer.convert_tokens_to_ids('kir_Cyrl')\nembeds = model.model.shared.weight.data\n# moving the embedding for \"mask\" to its new position\nembeds[added_token_id+1] =embeds[added_token_id]\n# initializing new language token with a token of a similar language\nembeds[added_token_id] = embeds[similar_lang_id]","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_97":{"__typename":"Paragraph","id":"_97","name":"4274","type":"P","href":null,"layout":null,"metadata":null,"text":"Now the model and the tokenizer are prepared for processing Tyvan . Of course, the model still needs some training to actually learn it.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_98":{"__typename":"Paragraph","id":"_98","name":"47c3","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 5: Training the model","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_99":{"__typename":"Paragraph","id":"_99","name":"7342","type":"P","href":null,"layout":null,"metadata":null,"text":"One way to organize the training is to follow the translation tutorial by HF: preprocess the whole dataset at once and feed it to a Seq2SeqTrainer . However, I prefer a custom training loop (which can be made more robust to out-of-memory errors) and creating training batches on the fly.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":132,"end":146,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":46,"end":76,"href":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Ftasks\u002Ftranslation","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_100":{"__typename":"Paragraph","id":"_100","name":"3cc6","type":"P","href":null,"layout":null,"metadata":null,"text":"To save some GPU memory, I use Adafactor optimizer instead of the more popular AdamW. I train the model with a learning rate linearly increasing from zero for the first 1000 steps, and then staying at 0.0001. I set a weight_decay to prevent the model parameters from becoming too big, and use a clip_threshold for restricting the norm of the gradient to stabilize the training.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":217,"end":229,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":295,"end":309,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_101":{"__typename":"Paragraph","id":"_101","name":"7da1","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from transformers.optimization import Adafactor\nfrom transformers import get_constant_schedule_with_warmup\nmodel.cuda();\noptimizer = Adafactor(\n    [p for p in model.parameters() if p.requires_grad],\n    scale_parameter=False,\n    relative_step=False,\n    lr=1e-4,\n    clip_threshold=1.0,\n    weight_decay=1e-3,\n)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_102":{"__typename":"Paragraph","id":"_102","name":"f1bf","type":"P","href":null,"layout":null,"metadata":null,"text":"To create each training batch, I randomly choose the translation direction (Tyvan to Russian or reverse), and randomly sample the sentence pairs. For more advanced training, I could also apply some random data augmentation to them (e.g. replacing words or changing the orthography).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_103":{"__typename":"Paragraph","id":"_103","name":"0c7c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import random\nLANGS = [('ru', 'rus_Cyrl'), ('tyv', 'tyv_Cyrl')]\n\ndef get_batch_pairs(batch_size, data=df_train):\n    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n    xx, yy = [], []\n    for _ in range(batch_size):\n        item = data.iloc[random.randint(0, len(data)-1)]\n        xx.append(preproc(item[l1]))\n        yy.append(preproc(item[l2]))\n    return xx, yy, long1, long2\n\nprint(get_batch_pairs(1))\n# (['чеди'], ['семь'], 'tyv_Cyrl', 'rus_Cyrl') ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_104":{"__typename":"Paragraph","id":"_104","name":"089b","type":"P","href":null,"layout":null,"metadata":null,"text":"Sometimes, training is interrupted because GPU runs out of memory (either because the texts in the batch are too long or because of some memory not being cleaned). To make the training more robust to them, I create a function that tries to release some memory:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_105":{"__typename":"Paragraph","id":"_105","name":"49a8","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import gc\nimport torch\n\ndef cleanup():\n    \"\"\"Try to free GPU memory\"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_106":{"__typename":"Paragraph","id":"_106","name":"d86e","type":"P","href":null,"layout":null,"metadata":null,"text":"I set some more parameters before training:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_107":{"__typename":"Paragraph","id":"_107","name":"e59d","type":"PRE","href":null,"layout":null,"metadata":null,"text":"batch_size = 16  # 32 already doesn't fit well to 15GB of GPU memory\nmax_length = 128  # token sequences will be truncated\ntraining_steps = 60000  # Usually, I set a large number of steps,\n# and then just interrupt the training manually\nlosses = []  # with this list, I do very simple tracking of average loss\nMODEL_SAVE_PATH = '\u002Fgd\u002FMyDrive\u002Fmodels\u002Fnllb-rus-tyv-v1'  # on my Google drive","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_108":{"__typename":"Paragraph","id":"_108","name":"3a77","type":"P","href":null,"layout":null,"metadata":null,"text":"Now we are ready for the training loop! To be robust, we save the model every 1000 steps, and after each out-of-memory error, we just ignore it and continue the training. If there are too many OOMs, though, you may want to interrupt the useless training and reduce the batch_size  or max_length.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":269,"end":279,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":284,"end":294,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_109":{"__typename":"Paragraph","id":"_109","name":"bbe5","type":"PRE","href":null,"layout":null,"metadata":null,"text":"model.train()\nx, y, loss = None, None, None\ncleanup()\n\ntq = trange(len(losses), training_steps)\nfor i in tq:\n    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n    try:\n        tokenizer.src_lang = lang1\n        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        tokenizer.src_lang = lang2\n        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        # -100 is a magic value ignored in the loss function\n        # because we don't want the model to learn to predict padding ids\n        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n\n        loss = model(**x, labels=y.input_ids).loss\n        loss.backward()\n        losses.append(loss.item())\n\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n        scheduler.step()\n\n    except RuntimeError as e:  # usually, it is out-of-memory\n        optimizer.zero_grad(set_to_none=True)\n        x, y, loss = None, None, None\n        cleanup()\n        print('error', max(len(s) for s in xx + yy), e)\n        continue\n\n    if i % 1000 == 0:\n        # each 1000 steps, I report average loss at these steps\n        print(i, np.mean(losses[-1000:]))\n\n    if i % 1000 == 0 and i \u003E 0:\n        model.save_pretrained(MODEL_SAVE_PATH)\n        tokenizer.save_pretrained(MODEL_SAVE_PATH)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_110":{"__typename":"Paragraph","id":"_110","name":"ea5c","type":"P","href":null,"layout":null,"metadata":null,"text":"An advantage of such a training loop is that you can interrupt it at any moment and adjust something or take a look at how the current version of the model can translate a sample sentence. But of course, you can replace it with something more sophisticated, if you feel like it. ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_111":{"__typename":"Paragraph","id":"_111","name":"b0eb","type":"P","href":null,"layout":null,"metadata":null,"text":"I usually run the training on Google Colab for at most 24 hours, and then Colab shuts it down. If I want to train longer, I restart the notebook, load the model from my last checkpoint, and just continue training it the same way as before. But for teaching NLLB a new language similar to the ones that it already knows, 24 hours is more than enough.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_112":{"__typename":"Paragraph","id":"_112","name":"4bfb","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 6: Evaluating and using the model","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_113":{"__typename":"Paragraph","id":"_113","name":"15bf","type":"P","href":null,"layout":null,"metadata":null,"text":"After the model has been trained for some time, you can test how well it translates. If the Colab instance has shut down, you can always load it back from the Google drive where you have saved it:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_114":{"__typename":"Paragraph","id":"_114","name":"f0e4","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\nmodel_load_name = '\u002Fgd\u002FMyDrive\u002Fmodels\u002Fnllb-rus-tyv-v1'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_load_name).cuda()\ntokenizer = NllbTokenizer.from_pretrained(model_load_name)\nfix_tokenizer(tokenizer)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_115":{"__typename":"Paragraph","id":"_115","name":"0572","type":"P","href":null,"layout":null,"metadata":null,"text":"Here is an example of function that can serve for translation:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_116":{"__typename":"Paragraph","id":"_116","name":"2222","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def translate(\n    text, src_lang='rus_Cyrl', tgt_lang='eng_Latn', \n    a=32, b=3, max_input_length=1024, num_beams=4, **kwargs\n):\n    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n    tokenizer.src_lang = src_lang\n    tokenizer.tgt_lang = tgt_lang\n    inputs = tokenizer(\n        text, return_tensors='pt', padding=True, truncation=True, \n        max_length=max_input_length\n    )\n    model.eval() # turn off training mode\n    result = model.generate(\n        **inputs.to(model.device),\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n        num_beams=num_beams, **kwargs\n    )\n    return tokenizer.batch_decode(result, skip_special_tokens=True)\n\n# Example usage:\nt = 'мөңгүн үр чыткаш карарар'\nprint(translate(t, 'tyv_Cyrl', 'rus_Cyrl'))\n# ['серебро от времени чернеет']","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_117":{"__typename":"Paragraph","id":"_117","name":"f0f1","type":"P","href":null,"layout":null,"metadata":null,"text":"It has several important parameters:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_118":{"__typename":"Paragraph","id":"_118","name":"5a36","type":"ULI","href":null,"layout":null,"metadata":null,"text":"num_beams: increasing this number usually improves the accuracy, but makes the translation slower and increases the memory consumption.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_119":{"__typename":"Paragraph","id":"_119","name":"4c57","type":"ULI","href":null,"layout":null,"metadata":null,"text":"a and b control the maximal length of the generated text (in tokens); setting them to smaller values can speed up the translation, but may occasionally lead to undertranslation.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":1,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":6,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_120":{"__typename":"Paragraph","id":"_120","name":"9389","type":"P","href":null,"layout":null,"metadata":null,"text":"The transformers package has many other parameters that you can modify when translating texts; please read the generation strategies doc to learn about them.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":107,"end":136,"href":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fgeneration_strategies","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_121":{"__typename":"Paragraph","id":"_121","name":"91b7","type":"P","href":null,"layout":null,"metadata":null,"text":"This way, we can generate the translations for our development dataset (both rus-tyv and tyv-rus), and take a look at how accurate they are.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*IZ1JUe9Ieg9DLJl3wb-TyA.png":{"__typename":"ImageMetadata","id":"1*IZ1JUe9Ieg9DLJl3wb-TyA.png","originalHeight":691,"originalWidth":1146,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:_122":{"__typename":"Paragraph","id":"_122","name":"368d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*IZ1JUe9Ieg9DLJl3wb-TyA.png"},"text":"Examples of translations. A screenshot by the author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_123":{"__typename":"Paragraph","id":"_123","name":"bd05","type":"P","href":null,"layout":null,"metadata":null,"text":"From the sample, it looks like the machine translations match the references (for tyv_translated, the reference is tyv, and for rus_translated, it is ru) in about 50% cases. How can we quantify this intuitive quality measure automatically? ","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":82,"end":96,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":115,"end":118,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":128,"end":142,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":150,"end":152,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_124":{"__typename":"Paragraph","id":"_124","name":"b6c2","type":"P","href":null,"layout":null,"metadata":null,"text":"The two most popular automatic metrics for machine translation quality are BLEU and ChrF++. Both of them compute a percentage similarity between the translation and the reference texts. However, they define the similarity slightly differently; e.g. BLEU reward only full-word matches, while ChrF++ gives positive scores even when only word parts match (so e.g. ChrF++ would treat the translation “течёт холод” to have similarity to the reference “несёт холодом” about 40%, while BLEU would report a zero similarity).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_125":{"__typename":"Paragraph","id":"_125","name":"d6b8","type":"P","href":null,"layout":null,"metadata":null,"text":"With these metrics, we can assign some numeric values to the quality of our translation model:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_126":{"__typename":"Paragraph","id":"_126","name":"ddc1","type":"PRE","href":null,"layout":null,"metadata":null,"text":"import sacrebleu\nbleu_calc = sacrebleu.BLEU()\nchrf_calc = sacrebleu.CHRF(word_order=2)  # this metric is called ChrF++\n\nprint(bleu_calc.corpus_score(df_dev['rus_translated'].tolist(), [df_dev['ru'].tolist()]))\nprint(chrf_calc.corpus_score(df_dev['rus_translated'].tolist(), [df_dev['ru'].tolist()]))\nprint(bleu_calc.corpus_score(df_dev['tyv_translated'].tolist(), [df_dev['tyv'].tolist()]))\nprint(chrf_calc.corpus_score(df_dev['tyv_translated'].tolist(), [df_dev['tyv'].tolist()]))\n\n# BLEU = 24.14 52.5\u002F30.4\u002F18.9\u002F12.1 (BP = 0.981 ratio = 0.981 hyp_len = 2281 ref_len = 2324)\n# chrF2++ = 49.49\n# BLEU = 23.41 52.1\u002F31.0\u002F18.9\u002F11.3 (BP = 0.966 ratio = 0.967 hyp_len = 2292 ref_len = 2371)\n# chrF2++ = 50.89","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_127":{"__typename":"Paragraph","id":"_127","name":"c315","type":"P","href":null,"layout":null,"metadata":null,"text":"BLEU and ChrF++ are useful for comparing the quality of different models on the same dataset. However, comparing their values for different target languages (or even for the same language, but using different data) is not so meaningful. For example, the table below doesn’t tell in which direction (tyv-rus or rus-tyv) the model is better. But it tells that in all cases, applying beam search is better than not applying it, and that in most cases, Model v2 is better than Model v1.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_128":{"__typename":"Paragraph","id":"_128","name":"b50d","type":"PRE","href":null,"layout":null,"metadata":null,"text":"                                  | tyv-\u003Erus | rus-\u003Etyv\nModel v1 (no vocabulary update):  |\n    no beam search                |   23.21  |  22.03\n    num_beams = 4                 |   24.14  |  23.41\nModel v2 (extended vocabulary):   |\n    no beam search                |   24.08  |  22.50\n    num_beams = 4                 |   25.18  |  23.22","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"cpp"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_129":{"__typename":"Paragraph","id":"_129","name":"82cf","type":"P","href":null,"layout":null,"metadata":null,"text":"But wait, what on Earth is Model v2? Actually, it is a version of Model v1 for which I did go over the optional Step 3 of expanding the vocabulary. As you can see, this step provides a small but sustainable gain in quality, at the cost of additional complexity and increasing the model size.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_130":{"__typename":"Paragraph","id":"_130","name":"5e5a","type":"P","href":null,"layout":null,"metadata":null,"text":"A tip: If you need to translate a long text, you should always split it into individual sentences and process them one by one (for many European languages, you can use the sentence-splitter Python package; for other languages, you can look at the text preprocessing code by the NLLB team).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":171,"end":189,"href":"https:\u002F\u002Fgithub.com\u002Fmediacloud\u002Fsentence-splitter","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":247,"end":270,"href":"https:\u002F\u002Fgithub.com\u002Ffacebookresearch\u002Fstopes\u002Fblob\u002Fmain\u002Fstopes\u002Fpipelines\u002Fmonolingual\u002Futils\u002Fsentence_split.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_131":{"__typename":"Paragraph","id":"_131","name":"e7a5","type":"P","href":null,"layout":null,"metadata":null,"text":"Another tip: If you want to translate multiple sentences using a GPU, it would be faster if you group them into batches which are translated in parallel.  Translating a batch takes as long as translating the longest sentence in it, so it would make sense to group batches by length:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_132":{"__typename":"Paragraph","id":"_132","name":"373f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"def batched_translate(texts, batch_size=16, **kwargs):\n    \"\"\"Translate texts in batches of similar length\"\"\"\n    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))\n    results = []\n    for i in trange(0, len(texts2), batch_size):\n        results.extend(translate(texts2[i: i+batch_size], **kwargs))\n    return [p for i, p in sorted(zip(idxs, results))]","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_133":{"__typename":"Paragraph","id":"_133","name":"7723","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 7: Publishing","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_134":{"__typename":"Paragraph","id":"_134","name":"8506","type":"P","href":null,"layout":null,"metadata":null,"text":"We will push the model and the tokenizer to a repository on the HuggingFace space, so that the other users could easily find and download them. This is very simple, as long as you have a HF account.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_135":{"__typename":"Paragraph","id":"_135","name":"9941","type":"P","href":null,"layout":null,"metadata":null,"text":"To connect to this account from a Colab notebook, you can type:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_136":{"__typename":"Paragraph","id":"_136","name":"fc0e","type":"PRE","href":null,"layout":null,"metadata":null,"text":"!huggingface-cli login","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_137":{"__typename":"Paragraph","id":"_137","name":"1bd2","type":"P","href":null,"layout":null,"metadata":null,"text":"You will be prompted to go to https:\u002F\u002Fhuggingface.co\u002Fsettings\u002Ftokens and copy-paste an authorization token from there. After that, you can execute the following code to create a repo for your model:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":30,"end":68,"href":"https:\u002F\u002Fhuggingface.co\u002Fsettings\u002Ftokens","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_138":{"__typename":"Paragraph","id":"_138","name":"918f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"upload_repo = \"slone\u002Fnllb-rus-tyv-v1\"\ntokenizer.push_to_hub(upload_repo)\nmodel.push_to_hub(upload_repo)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_139":{"__typename":"Paragraph","id":"_139","name":"3a9e","type":"P","href":null,"layout":null,"metadata":null,"text":"In a few minutes, the model and the tokenizer will be uploaded to a new repo like this one: https:\u002F\u002Fhuggingface.co\u002Fslone\u002Fnllb-rus-tyv-v1 (instead of slone, your should use the name of your HF personal or organization account). By the way, the repo for the v2 model is https:\u002F\u002Fhuggingface.co\u002Fslone\u002Fnllb-rus-tyv-v2-extvoc.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":149,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":92,"end":136,"href":"https:\u002F\u002Fhuggingface.co\u002Fslone\u002Fnllb-rus-tyv-v1","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":268,"end":319,"href":"https:\u002F\u002Fhuggingface.co\u002Fslone\u002Fnllb-rus-tyv-v2-extvoc","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_140":{"__typename":"Paragraph","id":"_140","name":"9add","type":"P","href":null,"layout":null,"metadata":null,"text":"As the last step, you will need to press the “Create model card” button and type some useful information about your model (please read here about it). In particular, indicate the supported languages and choose the cc-by-nc-4.0 license; this is the license with which the NLLB models were distributed, so all the derivative models must inherit it.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":135,"end":139,"href":"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmodel_sharing#add-a-model-card","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":214,"end":226,"href":"https:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby-nc\u002F4.0\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":214,"end":226,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_141":{"__typename":"Paragraph","id":"_141","name":"7162","type":"P","href":null,"layout":null,"metadata":null,"text":"In addition to just publishing the model and writing its description, you can host an interactive demo application on the HF platform. Thanks to the magic of packages such as Gradio or Streamlit, this requres not much more that just writing the function that translates a text.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_142":{"__typename":"Paragraph","id":"_142","name":"3bcc","type":"P","href":null,"layout":null,"metadata":null,"text":"You can copy such a demo from https:\u002F\u002Fhuggingface.co\u002Fspaces\u002Fslone\u002Fnllb-rus-tyv-v1-demo, and adapt the path to the model, supported languages, and the description text in its app.py file.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":174,"end":180,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":30,"end":86,"href":"https:\u002F\u002Fhuggingface.co\u002Fspaces\u002Fslone\u002Fnllb-rus-tyv-v1-demo","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_143":{"__typename":"Paragraph","id":"_143","name":"0799","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 8: (Optional) Serving the model with Docker","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_144":{"__typename":"Paragraph","id":"_144","name":"e340","type":"P","href":null,"layout":null,"metadata":null,"text":"Having a demo hosted for free at the HuggingFace website is good. But what if you want to use it elsewhere? The model is rather big (about 2.5GB), and it requires a lot of compute, so the only realistic option is to run it as a micro-service on some powerful server. ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_145":{"__typename":"Paragraph","id":"_145","name":"c98d","type":"P","href":null,"layout":null,"metadata":null,"text":"Such services are often run with Docker. I do the same, and I create a very simple web API using the FastAPI package. Here is the repository with the minimal working code: https:\u002F\u002Fgithub.com\u002Fslone-nlp\u002Fnllb-docker-demo.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":33,"end":39,"href":"https:\u002F\u002Fwww.docker.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":101,"end":109,"href":"https:\u002F\u002Ffastapi.tiangolo.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":172,"end":217,"href":"https:\u002F\u002Fgithub.com\u002Fslone-nlp\u002Fnllb-docker-demo","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_146":{"__typename":"Paragraph","id":"_146","name":"01a7","type":"P","href":null,"layout":null,"metadata":null,"text":"This code consists of a Dockerfile and two Python files. The file translation.py defines a Translator class, which is a wrapper that encapsulates model , tokenizer, and the translate function; everything that we have already seen. The file main.py defines a web API, implemented with FastAPI. It defines two endpoints: \u002Ftranslate, for running the translation, and \u002Flist-languages, for telling the potential frontend what languages are available: ","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":66,"end":80,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":91,"end":101,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":146,"end":151,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":154,"end":163,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":173,"end":182,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":240,"end":247,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":319,"end":329,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":364,"end":379,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_147":{"__typename":"Paragraph","id":"_147","name":"5e06","type":"PRE","href":null,"layout":null,"metadata":null,"text":"from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom translation import Translator\n\n\nclass TranslationRequest(BaseModel):\n    text: str\n    src_lang: str = 'rus_Cyrl'\n    tgt_lang = 'tyv_Cyrl'\n\n\napp = FastAPI()\ntranslator = Translator()\n\n\n@app.post(\"\u002Ftranslate\")\ndef translate(request: TranslationRequest):\n    \"\"\"\n    Perform translation with a fine-tuned NLLB model.\n    The language codes are supposed to be in 8-letter format, like \"eng_Latn\".\n    Their list can be returned by \u002Flist-languages.\n    \"\"\"\n    output = translator.translate(request.text, src_lang=request.src_lang, tgt_lang=request.tgt_lang)\n    return {\"translation\": output}\n\n\n@app.get(\"\u002Flist-languages\")\ndef list_languages():\n    \"\"\"Show the mapping of supported languages: from their English names to their 8-letter codes.\"\"\"\n    return translator.languages","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_148":{"__typename":"Paragraph","id":"_148","name":"555d","type":"P","href":null,"layout":null,"metadata":null,"text":"To run the application, you need to execute two commands in the command line (assuming that you have already started Docker), from the nllb-docker-demo directory:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":135,"end":151,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_149":{"__typename":"Paragraph","id":"_149","name":"fae7","type":"PRE","href":null,"layout":null,"metadata":null,"text":"docker build -t nllb .\ndocker run  -it -p 7860:7860 nllb","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_150":{"__typename":"Paragraph","id":"_150","name":"9108","type":"P","href":null,"layout":null,"metadata":null,"text":"The first command will take a few minutes to build the image, but once it is built, the second command runs quite fast. Upon completion, it will run a web server that supports the endpoints defined above. As a bonus, FastAPI creates an endpoint with automatic documentation: http:\u002F\u002Flocalhost:7860\u002Fdocs. It will show you the signatures of the endpoints, and will let you test both of them just from the browser.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":275,"end":301,"href":"http:\u002F\u002Flocalhost:7860\u002Fdocs","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_151":{"__typename":"Paragraph","id":"_151","name":"5b06","type":"P","href":null,"layout":null,"metadata":null,"text":"As a direct test of the API, you can open http:\u002F\u002Flocalhost:7860\u002Flist-languages in your browser, and see the returned list of languages as a JSON object: {\"Russian\": \"rus_Cyrl\", \"Tyvan\": \"tyv_Cyrl\"}.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":153,"end":197,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":42,"end":78,"href":"http:\u002F\u002Flocalhost:7860\u002Flist-languages","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_152":{"__typename":"Paragraph","id":"_152","name":"f2fd","type":"P","href":null,"layout":null,"metadata":null,"text":"The \u002Ftranslate endpoint supports only POST requests, so you can’t easily test it with a browser. But you can test it e.g. with the curl tool:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":4,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":131,"end":135,"href":"https:\u002F\u002Fcurl.se\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_153":{"__typename":"Paragraph","id":"_153","name":"370c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"curl -X 'POST' \\\n  'http:\u002F\u002Flocalhost:7860\u002Ftranslate' \\\n  -H 'accept: application\u002Fjson' \\\n  -H 'Content-Type: application\u002Fjson' \\\n  -d '{\"text\": \"Нет войне!\", \"src_lang\": \"rus_Cyrl\", \"tgt_lang\": \"tyv_Cyrl\"}'","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_154":{"__typename":"Paragraph","id":"_154","name":"b93e","type":"P","href":null,"layout":null,"metadata":null,"text":"(if you use Windows, you’ll have to write the command without \\, in a single line, with ' replaced by \" and \" in the JSON body replaced by \\\")","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":62,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":88,"end":89,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":102,"end":103,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":108,"end":109,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":139,"end":141,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_155":{"__typename":"Paragraph","id":"_155","name":"bbd6","type":"PRE","href":null,"layout":null,"metadata":null,"text":"curl -X \"POST\" http:\u002F\u002Flocalhost:7860\u002Ftranslate -H \"Content-Type: application\u002Fjson\" -d \"{\\\"text\\\": \\\"Нет войне!\\\", \\\"src_lang\\\": \\\"rus_Cyrl\\\", \\\"tgt_lang\\\": \\\"tyv_Cyrl\\\"}\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"bash"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_156":{"__typename":"Paragraph","id":"_156","name":"6d85","type":"P","href":null,"layout":null,"metadata":null,"text":"The command will call the translation service and return a JSON response:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_157":{"__typename":"Paragraph","id":"_157","name":"219c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"{\"translation\":\"Дайын-чаа чок!\"}","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"json"},"iframe":null,"mixtapeMetadata":null},"Paragraph:_158":{"__typename":"Paragraph","id":"_158","name":"63f0","type":"P","href":null,"layout":null,"metadata":null,"text":"You can deploy this backend application on some virtual server (e.g. AWS or DigitalOcean), and make your frontend app (e.g. a mobile application or a web frontend written in JS) talk to the backend with this API.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_159":{"__typename":"Paragraph","id":"_159","name":"c718","type":"H3","href":null,"layout":null,"metadata":null,"text":"What next?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_160":{"__typename":"Paragraph","id":"_160","name":"4d33","type":"P","href":null,"layout":null,"metadata":null,"text":"In this tutorial, I have shown how to examine a training dataset for machine translation, how to update the NLLB tokenizer with new tokens and a new language code, how to fine-tune a NLLB model, and how to publish and serve it. ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_161":{"__typename":"Paragraph","id":"_161","name":"aaac","type":"P","href":null,"layout":null,"metadata":null,"text":"This should be enough for creating a proof-of-concept machine translation service for a new language. However, there are some problems that I didn’t cover here (but may address in some future tutorials):","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_162":{"__typename":"Paragraph","id":"_162","name":"e6c9","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How to collect a parallel dataset for a new language from existing sources (some tips are in my paper about machine translation for Erzya)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":96,"end":137,"href":"https:\u002F\u002Faclanthology.org\u002F2022.fieldmatters-1.6\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_163":{"__typename":"Paragraph","id":"_163","name":"dc97","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How to organize manual translation to create even more parallel texts","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_164":{"__typename":"Paragraph","id":"_164","name":"a365","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How to clean and filter the existing parallel data (some tips are in my post about Bashkir corpus cleaning, in Russian)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":72,"end":106,"href":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F744972\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_165":{"__typename":"Paragraph","id":"_165","name":"4d9e","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How to prevent the model from forgetting other languages (the one in this tutorial has learned to translate only between Russian and Tyvan, but forgot how to translate into English)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_166":{"__typename":"Paragraph","id":"_166","name":"4a20","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How to decrease the model size (e.g. quantization and distillation), to make its deployment more affordable and faster","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_167":{"__typename":"Paragraph","id":"_167","name":"d97a","type":"ULI","href":null,"layout":null,"metadata":null,"text":"How to create an advanced machine translation system, robust both in terms of high load and translation quality.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_168":{"__typename":"Paragraph","id":"_168","name":"4e11","type":"P","href":null,"layout":null,"metadata":null,"text":"If you want to contact me on any of these issues (or on some other), don’t hesitate to leave your comments here, or to write directly to my Telegram (@cointegrated). You can also subscribe to my channel about NLP (in Russian): https:\u002F\u002Ft.me\u002Fizolenta_mebiusa.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":227,"end":256,"href":"https:\u002F\u002Ft.me\u002Fizolenta_mebiusa","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:_169":{"__typename":"Paragraph","id":"_169","name":"9ce5","type":"P","href":null,"layout":null,"metadata":null,"text":"And one last appeal: if you have data or models for lower-resourced languages, please publish them! Meta did a wonderful job by publishing the NLLB-200 models, but only as a community, we can truly make no language left behind, even if we address them just one at a time.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Tag:machine-translation":{"__typename":"Tag","id":"machine-translation","displayTitle":"Machine Translation","normalizedTagSlug":"machine-translation"},"Tag:ai":{"__typename":"Tag","id":"ai","displayTitle":"AI","normalizedTagSlug":"ai"},"Tag:nlp":{"__typename":"Tag","id":"nlp","displayTitle":"NLP","normalizedTagSlug":"nlp"},"Tag:nllb":{"__typename":"Tag","id":"nllb","displayTitle":"Nllb","normalizedTagSlug":"nllb"},"Tag:low-resource-language":{"__typename":"Tag","id":"low-resource-language","displayTitle":"Low Resource Language","normalizedTagSlug":"low-resource-language"},"Post:a37fc706b865":{"__typename":"Post","id":"a37fc706b865","collection":null,"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"ac89","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:_0"},{"__ref":"Paragraph:_1"},{"__ref":"Paragraph:_2"},{"__ref":"Paragraph:_3"},{"__ref":"Paragraph:_4"},{"__ref":"Paragraph:_5"},{"__ref":"Paragraph:_6"},{"__ref":"Paragraph:_7"},{"__ref":"Paragraph:_8"},{"__ref":"Paragraph:_9"},{"__ref":"Paragraph:_10"},{"__ref":"Paragraph:_11"},{"__ref":"Paragraph:_12"},{"__ref":"Paragraph:_13"},{"__ref":"Paragraph:_14"},{"__ref":"Paragraph:_15"},{"__ref":"Paragraph:_16"},{"__ref":"Paragraph:_17"},{"__ref":"Paragraph:_18"},{"__ref":"Paragraph:_19"},{"__ref":"Paragraph:_20"},{"__ref":"Paragraph:_21"},{"__ref":"Paragraph:_22"},{"__ref":"Paragraph:_23"},{"__ref":"Paragraph:_24"},{"__ref":"Paragraph:_25"},{"__ref":"Paragraph:_26"},{"__ref":"Paragraph:_27"},{"__ref":"Paragraph:_28"},{"__ref":"Paragraph:_29"},{"__ref":"Paragraph:_30"},{"__ref":"Paragraph:_31"},{"__ref":"Paragraph:_32"},{"__ref":"Paragraph:_33"},{"__ref":"Paragraph:_34"},{"__ref":"Paragraph:_35"},{"__ref":"Paragraph:_36"},{"__ref":"Paragraph:_37"},{"__ref":"Paragraph:_38"},{"__ref":"Paragraph:_39"},{"__ref":"Paragraph:_40"},{"__ref":"Paragraph:_41"},{"__ref":"Paragraph:_42"},{"__ref":"Paragraph:_43"},{"__ref":"Paragraph:_44"},{"__ref":"Paragraph:_45"},{"__ref":"Paragraph:_46"},{"__ref":"Paragraph:_47"},{"__ref":"Paragraph:_48"},{"__ref":"Paragraph:_49"},{"__ref":"Paragraph:_50"},{"__ref":"Paragraph:_51"},{"__ref":"Paragraph:_52"},{"__ref":"Paragraph:_53"},{"__ref":"Paragraph:_54"},{"__ref":"Paragraph:_55"},{"__ref":"Paragraph:_56"},{"__ref":"Paragraph:_57"},{"__ref":"Paragraph:_58"},{"__ref":"Paragraph:_59"},{"__ref":"Paragraph:_60"},{"__ref":"Paragraph:_61"},{"__ref":"Paragraph:_62"},{"__ref":"Paragraph:_63"},{"__ref":"Paragraph:_64"},{"__ref":"Paragraph:_65"},{"__ref":"Paragraph:_66"},{"__ref":"Paragraph:_67"},{"__ref":"Paragraph:_68"},{"__ref":"Paragraph:_69"},{"__ref":"Paragraph:_70"},{"__ref":"Paragraph:_71"},{"__ref":"Paragraph:_72"},{"__ref":"Paragraph:_73"},{"__ref":"Paragraph:_74"},{"__ref":"Paragraph:_75"},{"__ref":"Paragraph:_76"},{"__ref":"Paragraph:_77"},{"__ref":"Paragraph:_78"},{"__ref":"Paragraph:_79"},{"__ref":"Paragraph:_80"},{"__ref":"Paragraph:_81"},{"__ref":"Paragraph:_82"},{"__ref":"Paragraph:_83"},{"__ref":"Paragraph:_84"},{"__ref":"Paragraph:_85"},{"__ref":"Paragraph:_86"},{"__ref":"Paragraph:_87"},{"__ref":"Paragraph:_88"},{"__ref":"Paragraph:_89"},{"__ref":"Paragraph:_90"},{"__ref":"Paragraph:_91"},{"__ref":"Paragraph:_92"},{"__ref":"Paragraph:_93"},{"__ref":"Paragraph:_94"},{"__ref":"Paragraph:_95"},{"__ref":"Paragraph:_96"},{"__ref":"Paragraph:_97"},{"__ref":"Paragraph:_98"},{"__ref":"Paragraph:_99"},{"__ref":"Paragraph:_100"},{"__ref":"Paragraph:_101"},{"__ref":"Paragraph:_102"},{"__ref":"Paragraph:_103"},{"__ref":"Paragraph:_104"},{"__ref":"Paragraph:_105"},{"__ref":"Paragraph:_106"},{"__ref":"Paragraph:_107"},{"__ref":"Paragraph:_108"},{"__ref":"Paragraph:_109"},{"__ref":"Paragraph:_110"},{"__ref":"Paragraph:_111"},{"__ref":"Paragraph:_112"},{"__ref":"Paragraph:_113"},{"__ref":"Paragraph:_114"},{"__ref":"Paragraph:_115"},{"__ref":"Paragraph:_116"},{"__ref":"Paragraph:_117"},{"__ref":"Paragraph:_118"},{"__ref":"Paragraph:_119"},{"__ref":"Paragraph:_120"},{"__ref":"Paragraph:_121"},{"__ref":"Paragraph:_122"},{"__ref":"Paragraph:_123"},{"__ref":"Paragraph:_124"},{"__ref":"Paragraph:_125"},{"__ref":"Paragraph:_126"},{"__ref":"Paragraph:_127"},{"__ref":"Paragraph:_128"},{"__ref":"Paragraph:_129"},{"__ref":"Paragraph:_130"},{"__ref":"Paragraph:_131"},{"__ref":"Paragraph:_132"},{"__ref":"Paragraph:_133"},{"__ref":"Paragraph:_134"},{"__ref":"Paragraph:_135"},{"__ref":"Paragraph:_136"},{"__ref":"Paragraph:_137"},{"__ref":"Paragraph:_138"},{"__ref":"Paragraph:_139"},{"__ref":"Paragraph:_140"},{"__ref":"Paragraph:_141"},{"__ref":"Paragraph:_142"},{"__ref":"Paragraph:_143"},{"__ref":"Paragraph:_144"},{"__ref":"Paragraph:_145"},{"__ref":"Paragraph:_146"},{"__ref":"Paragraph:_147"},{"__ref":"Paragraph:_148"},{"__ref":"Paragraph:_149"},{"__ref":"Paragraph:_150"},{"__ref":"Paragraph:_151"},{"__ref":"Paragraph:_152"},{"__ref":"Paragraph:_153"},{"__ref":"Paragraph:_154"},{"__ref":"Paragraph:_155"},{"__ref":"Paragraph:_156"},{"__ref":"Paragraph:_157"},{"__ref":"Paragraph:_158"},{"__ref":"Paragraph:_159"},{"__ref":"Paragraph:_160"},{"__ref":"Paragraph:_161"},{"__ref":"Paragraph:_162"},{"__ref":"Paragraph:_163"},{"__ref":"Paragraph:_164"},{"__ref":"Paragraph:_165"},{"__ref":"Paragraph:_166"},{"__ref":"Paragraph:_167"},{"__ref":"Paragraph:_168"},{"__ref":"Paragraph:_169"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:4554ccd3a122"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Fcointegrated.medium.com\u002Fa37fc706b865","primaryTopic":null,"topics":[],"isPublished":false,"latestPublishedVersion":"","visibility":"PUBLIC","postResponses":{"__typename":"PostResponses","count":0},"createdAt":1696276168580,"firstPublishedAt":0,"latestPublishedAt":0,"clapCount":0,"allowResponses":true,"isLimitedState":false,"title":"How to fine-tune a NLLB-200 model for translating a new language","isSeries":false,"sequence":null,"uniqueSlug":"","socialTitle":"","socialDek":"","noIndex":null,"canonicalUrl":"","metaDescription":"","readingTime":23.063522012578616,"previewContent":{"__typename":"PreviewContent","subtitle":"“NLLB” (which stands for “no language left behind”) is a family of machine translation models published by Meta AI in 2022. These models…"},"previewImage":{"__ref":"ImageMetadata:1*qNRkzVmsA4GkrsTQwWKJfw.png"},"isShortform":false,"seoTitle":"","updatedAt":1697319769373,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:machine-translation"},{"__ref":"Tag:ai"},{"__ref":"Tag:nlp"},{"__ref":"Tag:nllb"},{"__ref":"Tag:low-resource-language"}],"pendingCollection":null,"statusForCollection":null,"layerCake":0,"detectedLanguage":"","wordCount":5891,"inResponseToPostResult":null,"inResponseToCatalogResult":null,"curationEligibleAt":0,"isNewsletter":false,"isPublishToEmail":false},"ImageMetadata:1*59IfFHhcDXtd0hsc1IfTUw.png":{"__typename":"ImageMetadata","id":"1*59IfFHhcDXtd0hsc1IfTUw.png","focusPercentX":null,"focusPercentY":null,"alt":null},"Collection:7f60cf5620c9":{"__typename":"Collection","id":"7f60cf5620c9","slug":"towards-data-science","name":"Towards Data Science","domain":"towardsdatascience.com","description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","subscriberCount":675328,"avatar":{"__ref":"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg"}},"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg":{"__typename":"ImageMetadata","id":"1*CJe3891yB1A1mzMdqemkdg.jpeg"},"Post:b9f94f3d9c90":{"__typename":"Post","id":"b9f94f3d9c90","title":"How to adapt a multilingual T5 model for a single language","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Load embeddings only for the tokens from your language to reduce model size","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*59IfFHhcDXtd0hsc1IfTUw.png"},"creator":{"__ref":"User:4554ccd3a122"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90","collection":{"__ref":"Collection:7f60cf5620c9"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":6},"visibility":"PUBLIC","clapCount":145,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1622413384235,"firstPublishedAt":1620158211737,"readingTime":3.377358490566038,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90"},"ImageMetadata:1*haP6Y29QGU9lin5d8QMAgw.png":{"__typename":"ImageMetadata","id":"1*haP6Y29QGU9lin5d8QMAgw.png","focusPercentX":null,"focusPercentY":null,"alt":null},"Post:eb212e9919ca":{"__typename":"Post","id":"eb212e9919ca","title":"Compressing unsupervised fastText models","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"A python package to reduce word embeddings models by 300 times, with almost the same performance on downstream NLP tasks.","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*haP6Y29QGU9lin5d8QMAgw.png"},"creator":{"__ref":"User:4554ccd3a122"},"isPublished":true,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fcompressing-unsupervised-fasttext-models-eb212e9919ca","collection":{"__ref":"Collection:7f60cf5620c9"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":1},"visibility":"PUBLIC","clapCount":61,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1639464682144,"firstPublishedAt":1639460837617,"readingTime":5.493396226415094,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"compressing-unsupervised-fasttext-models-eb212e9919ca"},"ImageMetadata:1*QUNXxPDm5b7CvExfhz9tkg.png":{"__typename":"ImageMetadata","id":"1*QUNXxPDm5b7CvExfhz9tkg.png","focusPercentX":null,"focusPercentY":null,"alt":null},"Collection:f5af2b715248":{"__typename":"Collection","id":"f5af2b715248","slug":"swlh","name":"The Startup","domain":null,"description":"Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +772K followers.","subscriberCount":776296,"avatar":{"__ref":"ImageMetadata:1*pKOfOAOvx-fWzfITATgGRg.jpeg"}},"ImageMetadata:1*pKOfOAOvx-fWzfITATgGRg.jpeg":{"__typename":"ImageMetadata","id":"1*pKOfOAOvx-fWzfITATgGRg.jpeg"},"Post:36601b73ecbb":{"__typename":"Post","id":"36601b73ecbb","title":"A machine learning model to understand fancy abbreviations, trained on Tolkien","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Recently I bumped into a question on Stackoverflow, how to recover phrases from abbreviations, e.g. turn “wtrbtl” into “water bottle”, and…","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*QUNXxPDm5b7CvExfhz9tkg.png"},"creator":{"__ref":"User:4554ccd3a122"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002Fswlh\u002Fa-machine-learning-model-to-understand-fancy-abbreviations-trained-on-tolkien-36601b73ecbb","collection":{"__ref":"Collection:f5af2b715248"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":5},"visibility":"PUBLIC","clapCount":283,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1515958622758,"firstPublishedAt":1515889668223,"readingTime":8.049056603773584,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"a-machine-learning-model-to-understand-fancy-abbreviations-trained-on-tolkien-36601b73ecbb"},"ImageMetadata:1*70jSJbcdhNWfKK5UVRDtWQ.png":{"__typename":"ImageMetadata","id":"1*70jSJbcdhNWfKK5UVRDtWQ.png","focusPercentX":null,"focusPercentY":null,"alt":null},"Post:8d47f2103da6":{"__typename":"Post","id":"8d47f2103da6","title":"Do you have to try to love math?","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"If you don’t like and don’t understand math, does it mean you are stupid?\nDo you need to love math to achieve at…","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*70jSJbcdhNWfKK5UVRDtWQ.png"},"creator":{"__ref":"User:4554ccd3a122"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fcointegrated.medium.com\u002Fdo-you-have-to-try-to-love-math-8d47f2103da6","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":0},"visibility":"PUBLIC","clapCount":3,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1518555308091,"firstPublishedAt":1518554491582,"readingTime":5.950943396226415,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"do-you-have-to-try-to-love-math-8d47f2103da6"},"ImageMetadata:1*2PEPQ0LxKFELp2lojVF-lw.jpeg":{"__typename":"ImageMetadata","id":"1*2PEPQ0LxKFELp2lojVF-lw.jpeg","focusPercentX":null,"focusPercentY":null,"alt":null},"User:822077a7247e":{"__typename":"User","id":"822077a7247e","name":"Unbecoming","username":"UnbecomingStories","mediumMemberAt":1643986065000,"socialStats":{"__typename":"SocialStats","followerCount":19440},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Stories of a former military wife, her flawed marriage and its ending. Questions? Lemme know at UnbecomingStories(at)gmail(dot)com","imageId":"1*_FyD8-JlhRRmfaZnEgCrkA.jpeg","membership":{"__ref":"Membership:4b276be708a"}},"Membership:4b276be708a":{"__typename":"Membership","tier":"MEMBER","id":"4b276be708a"},"Post:a6f367f02e53":{"__typename":"Post","id":"a6f367f02e53","title":"10 Seconds That Ended My 20 Year Marriage","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"It’s August in Northern Virginia, hot and humid. I still haven’t showered from my morning trail run. I’m wearing my stay-at-home mom…","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*2PEPQ0LxKFELp2lojVF-lw.jpeg"},"creator":{"__ref":"User:822077a7247e"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@UnbecomingStories\u002F10-seconds-that-ended-my-20-year-marriage-a6f367f02e53","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":973},"visibility":"LOCKED","clapCount":67068,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1689534316549,"firstPublishedAt":1645029855294,"readingTime":3.7433962264150944,"isLocked":true,"sequence":null,"isSeries":false,"uniqueSlug":"10-seconds-that-ended-my-20-year-marriage-a6f367f02e53"},"ImageMetadata:1*CX3RDZAwMVtNku1-ONB13w.jpeg":{"__typename":"ImageMetadata","id":"1*CX3RDZAwMVtNku1-ONB13w.jpeg","focusPercentX":null,"focusPercentY":null,"alt":null},"User:22ceabaa20f6":{"__typename":"User","id":"22ceabaa20f6","name":"Isaac Saul","username":"isaac_1884","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":469},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Going to war with partisan news — Executive Editor, Tangle News — www.readtangle.com","imageId":"1*5bMy4uvQlQxjhi_xhBlJJQ.jpeg","membership":null},"Post:15ed50e63da6":{"__typename":"Post","id":"15ed50e63da6","title":"A personal, non-partisan perspective on the Israel-Hamas war","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"To understand this war, we must understand the thousand-year history that led us here","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*CX3RDZAwMVtNku1-ONB13w.jpeg"},"creator":{"__ref":"User:22ceabaa20f6"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@isaac_1884\u002Fthe-attacks-on-israel-and-the-response-15ed50e63da6","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":316},"visibility":"PUBLIC","clapCount":11779,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1697139834630,"firstPublishedAt":1697127418237,"readingTime":10.471698113207546,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"the-attacks-on-israel-and-the-response-15ed50e63da6"},"ImageMetadata:1*3FtLa-nHJB8KOb-Wu9bqbg.png":{"__typename":"ImageMetadata","id":"1*3FtLa-nHJB8KOb-Wu9bqbg.png","focusPercentX":null,"focusPercentY":null,"alt":null},"User:4a25c00139e6":{"__typename":"User","id":"4a25c00139e6","name":"AL Anany","username":"alanany","mediumMemberAt":1669211356000,"socialStats":{"__typename":"SocialStats","followerCount":30797},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":true},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"entreprenal.com"}},"hasSubdomain":true,"bio":"96% Of My Time Is Invested In My Newsletter → alanany.com | Entrepreneurship | CEO @ Albusi GmbH | Zurich, Switzerland | www.albusi.com","imageId":"1*VreqR95b_bWD8Oxp8M6Rtg@2x.jpeg","membership":{"__ref":"Membership:549e48fdfe29"}},"Membership:549e48fdfe29":{"__typename":"Membership","tier":"MEMBER","id":"549e48fdfe29"},"Post:426d5e3f7d05":{"__typename":"Post","id":"426d5e3f7d05","title":"The ChatGPT Hype Is Over — Now Watch How Google Will Kill ChatGPT.","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"It never happens instantly. The business game is longer than you know.","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*3FtLa-nHJB8KOb-Wu9bqbg.png"},"creator":{"__ref":"User:4a25c00139e6"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fentreprenal.com\u002Fthe-chatgpt-hype-is-over-now-watch-how-google-will-kill-chatgpt-426d5e3f7d05","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":428},"visibility":"LOCKED","clapCount":14238,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1693644346675,"firstPublishedAt":1693584748571,"readingTime":5.44811320754717,"isLocked":true,"sequence":null,"isSeries":false,"uniqueSlug":"the-chatgpt-hype-is-over-now-watch-how-google-will-kill-chatgpt-426d5e3f7d05"},"ImageMetadata:1*D-TiKrBADjkMrnHjBAQ4bQ.png":{"__typename":"ImageMetadata","id":"1*D-TiKrBADjkMrnHjBAQ4bQ.png","focusPercentX":null,"focusPercentY":null,"alt":null},"User:afdf6e24e4da":{"__typename":"User","id":"afdf6e24e4da","name":"Mirijam Missbichler","username":"mirijam.missbichler","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":1529},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Expertise in the Video Game Industry, Production Management & Japanese","imageId":"1*HRgar2NE2lXMHGvV7wH0QA.jpeg","membership":null},"Post:2c7273e8be1e":{"__typename":"Post","id":"2c7273e8be1e","title":"Why Japanese Websites Look So Different","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"& how to analyze design choices without jumping to conclusions","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*D-TiKrBADjkMrnHjBAQ4bQ.png"},"creator":{"__ref":"User:afdf6e24e4da"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@mirijam.missbichler\u002Fwhy-japanese-websites-look-so-different-2c7273e8be1e","collection":null,"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":218},"visibility":"PUBLIC","clapCount":14714,"pendingCollection":null,"statusForCollection":null,"pinnedAt":0,"latestPublishedAt":1689974023795,"firstPublishedAt":1682979214856,"readingTime":7.370754716981132,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"why-japanese-websites-look-so-different-2c7273e8be1e"},"ImageMetadata:0*CIFEv8QIEjuBWqTB":{"__typename":"ImageMetadata","id":"0*CIFEv8QIEjuBWqTB","focusPercentX":null,"focusPercentY":null,"alt":null},"User:1a1227e369d":{"__typename":"User","id":"1a1227e369d","name":"Scott-Ryan Abt","username":"73srabt","mediumMemberAt":1586587024000,"socialStats":{"__typename":"SocialStats","followerCount":11998},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Enthusiast, in transit.Pithy bon mots of life abroad, the state of the world, travel, music, food, cocktails, cleverish observations. Top Writer in Travel+Music","imageId":"1*4MpeAtM5-fGCa3vOPM6XXg.jpeg","membership":{"__ref":"Membership:c8cd451f25a1"}},"Collection:3b31039017a5":{"__typename":"Collection","id":"3b31039017a5","slug":"pitfall","name":"Pitfall","domain":null,"description":"Filling In The Gaps — For Writing That Doesn’t Fit","subscriberCount":267,"avatar":{"__ref":"ImageMetadata:1*4CXbViMdZ43-0EYs3HXrFA.png"}},"Membership:c8cd451f25a1":{"__typename":"Membership","tier":"MEMBER","id":"c8cd451f25a1"},"ImageMetadata:1*4CXbViMdZ43-0EYs3HXrFA.png":{"__typename":"ImageMetadata","id":"1*4CXbViMdZ43-0EYs3HXrFA.png"},"Post:37bb823839d2":{"__typename":"Post","id":"37bb823839d2","title":"Bye Bye, Spotify","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"And see ya later, all you subscription services in my little empire","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:0*CIFEv8QIEjuBWqTB"},"creator":{"__ref":"User:1a1227e369d"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fmedium.com\u002Fpitfall\u002Fbye-bye-spotify-37bb823839d2","collection":{"__ref":"Collection:3b31039017a5"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":303},"visibility":"LOCKED","clapCount":13017,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"latestPublishedAt":1693849800961,"firstPublishedAt":1692478709841,"readingTime":3.8867924528301887,"isLocked":true,"sequence":null,"isSeries":false,"uniqueSlug":"bye-bye-spotify-37bb823839d2"},"ImageMetadata:1*r5coQ_IUZNe0yjOH7vq_6w.jpeg":{"__typename":"ImageMetadata","id":"1*r5coQ_IUZNe0yjOH7vq_6w.jpeg","focusPercentX":null,"focusPercentY":null,"alt":null},"User:c26c12a349df":{"__typename":"User","id":"c26c12a349df","name":"Benoit Ruiz","username":"ruizb","mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":1700},"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"customDomainState":null,"hasSubdomain":false,"bio":"Software Engineer at Datadog. Twitter: @Haaress","imageId":"1*-7F-xdEpfDbogXdyl141OQ.jpeg","membership":null},"Collection:d0b105d10f0a":{"__typename":"Collection","id":"d0b105d10f0a","slug":"better-programming","name":"Better Programming","domain":"betterprogramming.pub","description":"Advice for programmers.","subscriberCount":217363,"avatar":{"__ref":"ImageMetadata:1*QNoA3XlXLHz22zQazc0syg.png"}},"ImageMetadata:1*QNoA3XlXLHz22zQazc0syg.png":{"__typename":"ImageMetadata","id":"1*QNoA3XlXLHz22zQazc0syg.png"},"Post:8df5111d4d55":{"__typename":"Post","id":"8df5111d4d55","title":"Advice From a Software Engineer With 8 Years of Experience","extendedPreviewContent":{"__typename":"PreviewContent","subtitle":"Practical tips for those who want to advance in their careers","isFullContent":false},"previewImage":{"__ref":"ImageMetadata:1*r5coQ_IUZNe0yjOH7vq_6w.jpeg"},"creator":{"__ref":"User:c26c12a349df"},"isPublished":true,"mediumUrl":"https:\u002F\u002Fbetterprogramming.pub\u002Fadvices-from-a-software-engineer-with-8-years-of-experience-8df5111d4d55","collection":{"__ref":"Collection:d0b105d10f0a"},"isLimitedState":false,"allowResponses":true,"postResponses":{"__typename":"PostResponses","count":205},"visibility":"PUBLIC","clapCount":10522,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":1679934851947,"latestPublishedAt":1679344405941,"firstPublishedAt":1678799110957,"readingTime":21.473899371069184,"isLocked":false,"sequence":null,"isSeries":false,"uniqueSlug":"advices-from-a-software-engineer-with-8-years-of-experience-8df5111d4d55"},"User:a32c340ea342":{"__typename":"User","username":"MediumStaff","id":"a32c340ea342","customDomainState":null,"hasSubdomain":false},"CatalogViewerEdge:catalogId:5969c7449b7f-viewerId:9a99d8ca5e92":{"__typename":"CatalogViewerEdge","followersCount":145,"id":"catalogId:5969c7449b7f-viewerId:9a99d8ca5e92"},"ImageMetadata:0*3OsUtsnlTx9Svm4c.jpg":{"__typename":"ImageMetadata","id":"0*3OsUtsnlTx9Svm4c.jpg","alt":"Image by vectorjuice on FreePik"},"Post:23a2173eecae":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*3OsUtsnlTx9Svm4c.jpg"},"id":"23a2173eecae"},"CatalogItemV2:{\"catalogItemId\":\"6482363e212e6c41a481d903\"}":{"__typename":"CatalogItemV2","catalogItemId":"6482363e212e6c41a481d903","entity":{"__ref":"Post:23a2173eecae"}},"ImageMetadata:1*IPZF1hcDWwpPqOz2vL7NxQ.png":{"__typename":"ImageMetadata","id":"1*IPZF1hcDWwpPqOz2vL7NxQ.png","alt":null},"Post:3bc2644d4507":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*IPZF1hcDWwpPqOz2vL7NxQ.png"},"id":"3bc2644d4507"},"CatalogItemV2:{\"catalogItemId\":\"641cac35672446f81159a840\"}":{"__typename":"CatalogItemV2","catalogItemId":"641cac35672446f81159a840","entity":{"__ref":"Post:3bc2644d4507"}},"ImageMetadata:1*0fHUKyg3xtpNWpop35PR4g.png":{"__typename":"ImageMetadata","id":"1*0fHUKyg3xtpNWpop35PR4g.png","alt":null},"Post:59c16ae76e3e":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*0fHUKyg3xtpNWpop35PR4g.png"},"id":"59c16ae76e3e"},"CatalogItemV2:{\"catalogItemId\":\"6422e78d7bc8cca169b3ce68\"}":{"__typename":"CatalogItemV2","catalogItemId":"6422e78d7bc8cca169b3ce68","entity":{"__ref":"Post:59c16ae76e3e"}},"ImageMetadata:1*FS8L4XLW_3mHDyRKNwjKvA.jpeg":{"__typename":"ImageMetadata","id":"1*FS8L4XLW_3mHDyRKNwjKvA.jpeg","alt":null},"Post:2bf00851551e":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*FS8L4XLW_3mHDyRKNwjKvA.jpeg"},"id":"2bf00851551e"},"CatalogItemV2:{\"catalogItemId\":\"6421a9dc3b730f2980719d64\"}":{"__typename":"CatalogItemV2","catalogItemId":"6421a9dc3b730f2980719d64","entity":{"__ref":"Post:2bf00851551e"}},"ImageMetadata:1*OhWzHPkxXbslpwYyCZ2HKA.png":{"__typename":"ImageMetadata","id":"1*OhWzHPkxXbslpwYyCZ2HKA.png","alt":null},"Post:1ce5fca96286":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*OhWzHPkxXbslpwYyCZ2HKA.png"},"id":"1ce5fca96286"},"CatalogItemV2:{\"catalogItemId\":\"641cac41b70e26f89f868177\"}":{"__typename":"CatalogItemV2","catalogItemId":"641cac41b70e26f89f868177","entity":{"__ref":"Post:1ce5fca96286"}},"Catalog:5969c7449b7f":{"__typename":"Catalog","id":"5969c7449b7f","name":"The New Chatbots: ChatGPT, Bard, and Beyond","postItemsCount":13,"predefined":null,"creator":{"__ref":"User:a32c340ea342"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:5969c7449b7f-viewerId:9a99d8ca5e92"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"6482363e212e6c41a481d903\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"641cac35672446f81159a840\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"6422e78d7bc8cca169b3ce68\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"6421a9dc3b730f2980719d64\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"641cac41b70e26f89f868177\"}"}]}},"User:2eb23a991a63":{"__typename":"User","username":"AMGAS14","id":"2eb23a991a63","customDomainState":null,"hasSubdomain":false},"CatalogViewerEdge:catalogId:0a856388a93a-viewerId:9a99d8ca5e92":{"__typename":"CatalogViewerEdge","followersCount":317,"id":"catalogId:0a856388a93a-viewerId:9a99d8ca5e92"},"ImageMetadata:0*i2EDU0pBgMeqcg2J":{"__typename":"ImageMetadata","id":"0*i2EDU0pBgMeqcg2J","alt":null},"Post:aeb422c04d34":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*i2EDU0pBgMeqcg2J"},"id":"aeb422c04d34"},"CatalogItemV2:{\"catalogItemId\":\"652d3b1aa31277c8e9b83c91\"}":{"__typename":"CatalogItemV2","catalogItemId":"652d3b1aa31277c8e9b83c91","entity":{"__ref":"Post:aeb422c04d34"}},"ImageMetadata:1*pt3GvRsCd12vtlolavMSZg.png":{"__typename":"ImageMetadata","id":"1*pt3GvRsCd12vtlolavMSZg.png","alt":null},"Post:ce78fdae4ef6":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*pt3GvRsCd12vtlolavMSZg.png"},"id":"ce78fdae4ef6"},"CatalogItemV2:{\"catalogItemId\":\"652d3b1208f3e778048d3e21\"}":{"__typename":"CatalogItemV2","catalogItemId":"652d3b1208f3e778048d3e21","entity":{"__ref":"Post:ce78fdae4ef6"}},"ImageMetadata:1*0xySt8rSH3BQ2yIwvyGMqA.png":{"__typename":"ImageMetadata","id":"1*0xySt8rSH3BQ2yIwvyGMqA.png","alt":null},"Post:74e59201b51f":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*0xySt8rSH3BQ2yIwvyGMqA.png"},"id":"74e59201b51f"},"CatalogItemV2:{\"catalogItemId\":\"652d3af3e1ab78ebb6cd7181\"}":{"__typename":"CatalogItemV2","catalogItemId":"652d3af3e1ab78ebb6cd7181","entity":{"__ref":"Post:74e59201b51f"}},"ImageMetadata:1*COnL6jdY0zZ_pX9y6YufDQ.png":{"__typename":"ImageMetadata","id":"1*COnL6jdY0zZ_pX9y6YufDQ.png","alt":null},"Post:6a39d6fe6fc9":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*COnL6jdY0zZ_pX9y6YufDQ.png"},"id":"6a39d6fe6fc9"},"CatalogItemV2:{\"catalogItemId\":\"652bf63deec715ce075ca3d7\"}":{"__typename":"CatalogItemV2","catalogItemId":"652bf63deec715ce075ca3d7","entity":{"__ref":"Post:6a39d6fe6fc9"}},"ImageMetadata:0*jXExEP37iakc0oDz":{"__typename":"ImageMetadata","id":"0*jXExEP37iakc0oDz","alt":null},"Post:672d06094e6e":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*jXExEP37iakc0oDz"},"id":"672d06094e6e"},"CatalogItemV2:{\"catalogItemId\":\"652bf618d5fa5dc62bb2bd72\"}":{"__typename":"CatalogItemV2","catalogItemId":"652bf618d5fa5dc62bb2bd72","entity":{"__ref":"Post:672d06094e6e"}},"Catalog:0a856388a93a":{"__typename":"Catalog","id":"0a856388a93a","name":"Natural Language Processing","postItemsCount":709,"predefined":null,"creator":{"__ref":"User:2eb23a991a63"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:0a856388a93a-viewerId:9a99d8ca5e92"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652d3b1aa31277c8e9b83c91\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652d3b1208f3e778048d3e21\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652d3af3e1ab78ebb6cd7181\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652bf63deec715ce075ca3d7\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"652bf618d5fa5dc62bb2bd72\"}"}]}},"User:d00bc5bb7954":{"__typename":"User","username":"tomsmith585","id":"d00bc5bb7954","customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"tomsmith585.medium.com"}},"hasSubdomain":true},"CatalogViewerEdge:catalogId:508b0743c247-viewerId:9a99d8ca5e92":{"__typename":"CatalogViewerEdge","followersCount":309,"id":"catalogId:508b0743c247-viewerId:9a99d8ca5e92"},"ImageMetadata:0*M8Jq6btD0YsgaRM1":{"__typename":"ImageMetadata","id":"0*M8Jq6btD0YsgaRM1","alt":null},"Post:d8f870949d79":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*M8Jq6btD0YsgaRM1"},"id":"d8f870949d79"},"CatalogItemV2:{\"catalogItemId\":\"649bad76f425ab969cdced5f\"}":{"__typename":"CatalogItemV2","catalogItemId":"649bad76f425ab969cdced5f","entity":{"__ref":"Post:d8f870949d79"}},"ImageMetadata:1*rsp22rKwFDjiwwCcUly56Q.jpeg":{"__typename":"ImageMetadata","id":"1*rsp22rKwFDjiwwCcUly56Q.jpeg","alt":null},"Post:4f2d34bd8736":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*rsp22rKwFDjiwwCcUly56Q.jpeg"},"id":"4f2d34bd8736"},"CatalogItemV2:{\"catalogItemId\":\"645d4448e451b6e22adc9e59\"}":{"__typename":"CatalogItemV2","catalogItemId":"645d4448e451b6e22adc9e59","entity":{"__ref":"Post:4f2d34bd8736"}},"ImageMetadata:1*PNVLDmurJ5LoCjB9Ovdnpw.png":{"__typename":"ImageMetadata","id":"1*PNVLDmurJ5LoCjB9Ovdnpw.png","alt":null},"Post:5dc899af1e90":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*PNVLDmurJ5LoCjB9Ovdnpw.png"},"id":"5dc899af1e90"},"CatalogItemV2:{\"catalogItemId\":\"643ffc88b5ce3ab74352d66e\"}":{"__typename":"CatalogItemV2","catalogItemId":"643ffc88b5ce3ab74352d66e","entity":{"__ref":"Post:5dc899af1e90"}},"ImageMetadata:1*v58AFpnX92CA8PbttV239A.png":{"__typename":"ImageMetadata","id":"1*v58AFpnX92CA8PbttV239A.png","alt":null},"Post:79845f1eff86":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*v58AFpnX92CA8PbttV239A.png"},"id":"79845f1eff86"},"CatalogItemV2:{\"catalogItemId\":\"643fec6368357cf68769d8b5\"}":{"__typename":"CatalogItemV2","catalogItemId":"643fec6368357cf68769d8b5","entity":{"__ref":"Post:79845f1eff86"}},"ImageMetadata:1*WCViofSHByd3XWuuaZEJqw.jpeg":{"__typename":"ImageMetadata","id":"1*WCViofSHByd3XWuuaZEJqw.jpeg","alt":"A towering blocky structure rising into the clouds in front of a startled onlooker"},"Post:47b1e0a324a7":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*WCViofSHByd3XWuuaZEJqw.jpeg"},"id":"47b1e0a324a7"},"CatalogItemV2:{\"catalogItemId\":\"643bfe6526058bef2a38d4d4\"}":{"__typename":"CatalogItemV2","catalogItemId":"643bfe6526058bef2a38d4d4","entity":{"__ref":"Post:47b1e0a324a7"}},"Catalog:508b0743c247":{"__typename":"Catalog","id":"508b0743c247","name":"Generative AI Recommended Reading","postItemsCount":52,"predefined":null,"creator":{"__ref":"User:d00bc5bb7954"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:508b0743c247-viewerId:9a99d8ca5e92"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"649bad76f425ab969cdced5f\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"645d4448e451b6e22adc9e59\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"643ffc88b5ce3ab74352d66e\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"643fec6368357cf68769d8b5\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"643bfe6526058bef2a38d4d4\"}"}]}},"User:795f94e67359":{"__typename":"User","username":"MediumForTeams","id":"795f94e67359","customDomainState":null,"hasSubdomain":false},"CatalogViewerEdge:catalogId:7a5756752f49-viewerId:9a99d8ca5e92":{"__typename":"CatalogViewerEdge","followersCount":193,"id":"catalogId:7a5756752f49-viewerId:9a99d8ca5e92"},"ImageMetadata:0*_eYHSSUS0abUxmDU":{"__typename":"ImageMetadata","id":"0*_eYHSSUS0abUxmDU","alt":null},"Post:7ec25c57ca94":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*_eYHSSUS0abUxmDU"},"id":"7ec25c57ca94"},"CatalogItemV2:{\"catalogItemId\":\"63c7ec663b2cbf4a9d8f6a09\"}":{"__typename":"CatalogItemV2","catalogItemId":"63c7ec663b2cbf4a9d8f6a09","entity":{"__ref":"Post:7ec25c57ca94"}},"ImageMetadata:1*wXgeNtz5OJ5O9T3c3mQRRw.png":{"__typename":"ImageMetadata","id":"1*wXgeNtz5OJ5O9T3c3mQRRw.png","alt":null},"Post:aa824ad89623":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*wXgeNtz5OJ5O9T3c3mQRRw.png"},"id":"aa824ad89623"},"CatalogItemV2:{\"catalogItemId\":\"63c7ecb03b2cbf4a9d8f6a13\"}":{"__typename":"CatalogItemV2","catalogItemId":"63c7ecb03b2cbf4a9d8f6a13","entity":{"__ref":"Post:aa824ad89623"}},"ImageMetadata:0*tIipcmrInD5UMpQI.png":{"__typename":"ImageMetadata","id":"0*tIipcmrInD5UMpQI.png","alt":null},"Post:a25fa9f54442":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:0*tIipcmrInD5UMpQI.png"},"id":"a25fa9f54442"},"CatalogItemV2:{\"catalogItemId\":\"63c7ed6e945adc0d2f12ee29\"}":{"__typename":"CatalogItemV2","catalogItemId":"63c7ed6e945adc0d2f12ee29","entity":{"__ref":"Post:a25fa9f54442"}},"ImageMetadata:1*uPt_ccZmvCMnQahIs43ahg.png":{"__typename":"ImageMetadata","id":"1*uPt_ccZmvCMnQahIs43ahg.png","alt":null},"Post:8342dd70d2bd":{"__typename":"Post","previewImage":{"__ref":"ImageMetadata:1*uPt_ccZmvCMnQahIs43ahg.png"},"id":"8342dd70d2bd"},"CatalogItemV2:{\"catalogItemId\":\"63c7ed5c1322ec40550cfe21\"}":{"__typename":"CatalogItemV2","catalogItemId":"63c7ed5c1322ec40550cfe21","entity":{"__ref":"Post:8342dd70d2bd"}},"CatalogItemV2:{\"catalogItemId\":\"63c7ed5515e8da9c2c688cbd\"}":{"__typename":"CatalogItemV2","catalogItemId":"63c7ed5515e8da9c2c688cbd","entity":{"__ref":"Post:3bc2644d4507"}},"Catalog:7a5756752f49":{"__typename":"Catalog","id":"7a5756752f49","name":"What is ChatGPT?","postItemsCount":9,"predefined":null,"creator":{"__ref":"User:795f94e67359"},"viewerEdge":{"__ref":"CatalogViewerEdge:catalogId:7a5756752f49-viewerId:9a99d8ca5e92"},"itemsConnection:(limit:5)":{"__typename":"CatalogItemsConnection","items":[{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63c7ec663b2cbf4a9d8f6a09\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63c7ecb03b2cbf4a9d8f6a13\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63c7ed6e945adc0d2f12ee29\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63c7ed5c1322ec40550cfe21\"}"},{"__ref":"CatalogItemV2:{\"catalogItemId\":\"63c7ed5515e8da9c2c688cbd\"}"}]}}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.e2d9b19a.js"></script><script src="https://cdn-client.medium.com/lite/static/js/1008.9e2c8ae5.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.bb4f9bf0.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.7cdafcd5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.2021fe63.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3130.d84e5554.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6733.c6c17f3e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4711.eb865124.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8695.4a6127a5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4341.39719d69.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3154.8be4a205.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5203.972fb599.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1957.6de9754c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9599.0edb614e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1711.6127e5e0.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5268.340f7f3b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9114.0acbd6c8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5459.cfc2e69b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6804.2f4a4354.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9174.ca12091f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4129.5e8e8e93.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8580.2dd0c5ae.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1802.266129dd.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2295.dfe54a2b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4078.9fb8a750.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8883.b0b4b29f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2550.2780af8f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9408.2907bde4.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1743.8ee80896.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4667.5c0c8d6f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9150.e244f1b8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5005.4ccc91b2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2804.9c761555.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1006.97cfd7bf.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.50762a1a.chunk.js"></script><script>window.main();</script><script defer src="https://static.cloudflareinsights.com/beacon.min.js/v8b253dfea2ab4077af8c6f58422dfbfd1689876627854" integrity="sha512-bjgnUKX4azu3dLTVtie9u6TKqgx29RBwfj3QXYt5EKfWM/9hPSAI/4qcV5NACjwAo8UtTeWefx6Zq5PHcMm7Tg==" data-cf-beacon='{"rayId":"81776be04a069211","token":"0b5f665943484354a59c39c6833f7078","version":"2023.8.0","si":100}' crossorigin="anonymous"></script>
</body></html>