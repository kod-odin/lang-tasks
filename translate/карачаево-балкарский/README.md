# Комментарий от автора

Обучил модель nllb-200 на карачаево-балкарском языке. Очень помогли наработки Давида, за что тебе огромное спасибо. Оставлю свой код, может кому-то нужен будет как ещё один источник.

**Колаб:**
- [Подготовка токензатора](https://colab.research.google.com/drive/10-0ve5dWWgWxuaYVIxwQMXQgomEc0ADm?usp=sharing)
- [Обучение](https://colab.research.google.com/drive/1tqEyZkGwFE7C3VFemk4S8HI3wiWskOAj?usp=sharing)

**Гитхаб (папки 3.Python или 4.Jupyter):** [GitHub репозиторий](https://github.com/TBSj/Qarachay_Malqar_translator)

Есть несколько моментов для тех, кто будет тренировать, которые я заметил и хочу поделиться:
1) "Познакомьте" модель со всеми вашими данными, но не больше, основное - уроните loss до 2 или немножко меньше
2) Основное обучение проводите только на предложениях, а словари и мелкие фразы не подойдут, они мало влияют на снижение ошибки (только время и деньги потратите).
3) Когда loss достигнет единицы или около неё, то сохраняйте лучшую модель исходя из наименьшей ошибки на валидационных данных (так модель не переобучится).
4) Можете дальше попробовать обучить модель на абзацах (в моему случае она переобучилась и показывает хуже результат на неизвестных ей данный, но может я запорол что-то).

Если вы собираете параллельные корпуса, то основной упор делайте на предложения, а не на словари. Объём корпуса большой дают, но для модели значимость их небольшая.

По крайней мере, в нашем случае именно так и сработало всё
